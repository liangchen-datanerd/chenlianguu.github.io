<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Kubernetes单节点部署</title>
    <link href="/2020/04/03/Kubernetes%E5%8D%95%E8%8A%82%E7%82%B9%E9%83%A8%E7%BD%B2/"/>
    <url>/2020/04/03/Kubernetes%E5%8D%95%E8%8A%82%E7%82%B9%E9%83%A8%E7%BD%B2/</url>
    
    <content type="html"><![CDATA[<h2 id="准备环境"><a href="#准备环境" class="headerlink" title="准备环境"></a>准备环境</h2><pre><code class="shell">关闭防火墙：$ systemctl stop firewalld$ systemctl disable firewalld关闭selinux：$ sed -i &#39;s/enforcing/disabled/&#39; /etc/selinux/config $ setenforce 0关闭swap：$ swapoff -a $ 临时$ vim /etc/fstab $ 永久添加主机名与IP对应关系（记得设置主机名）：$ cat /etc/hosts192.168.31.61 k8s-master192.168.31.62 k8s-node1192.168.31.63 k8s-node2将桥接的IPv4流量传递到iptables的链：$ cat &gt; /etc/sysctl.d/k8s.conf &lt;&lt; EOFnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOF$ sysctl --system卸载docker因为k8s默认会安装docker，所以如果系统安装过就需要将其卸载，不然可能出现版本不兼容安装上的情况，卸载命令如下。$ yum remove -y &#39;rpm -qa |grep docker&#39;$ rm -rf /var/lib/docker配置docker源镜像vim /etc/docker/daemin.json{  &quot;registry-mirrors&quot;: [        &quot;https://dockerhub.azk8s.cn&quot;,        &quot;https://b3sst9pc.mirror.aliyuncs.com&quot;,        &quot;https://hub-mirror.c.163.com&quot;]}systemctl daemon-reloadsystemctl restart docker</code></pre><h2 id="安装etcd-kuberetes"><a href="#安装etcd-kuberetes" class="headerlink" title="安装etcd kuberetes"></a>安装etcd kuberetes</h2><pre><code class="shell"># 配置yum源$ cat &gt; /etc/yum.repos.d/kubernetes.repo &lt;&lt; EOF[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=0repo_gpgcheck=0gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF# 安装yum install -y etcd kubernetes</code></pre><h2 id="修改配置"><a href="#修改配置" class="headerlink" title="修改配置"></a>修改配置</h2><pre><code># 将KUBE_ADMISSION_CONTROL选项中的ServiceAccount删除掉vim /etc/kubernetes/apiserver# KUBE_ADMISSION_CONTROL=&quot;--admission-control=NamespaceLifecycle,NamespaceExists,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota&quot;KUBE_ADMISSION_CONTROL=&quot;--admission_control=NamespaceLifecycle,NamespaceExists,LimitRanger,SecurityContextDeny,ResourceQuota&quot;</code></pre><h2 id="启动服务"><a href="#启动服务" class="headerlink" title="启动服务"></a>启动服务</h2><pre><code class="shell">systemctl start etcdsystemctl start dockersystemctl start kube-apiserversystemctl start kube-controller-managersystemctl start kube-schedulersystemctl start kubeletsystemctl start kube-proxy</code></pre><h2 id="部署dashboard"><a href="#部署dashboard" class="headerlink" title="部署dashboard"></a>部署dashboard</h2><pre><code class="yaml">kind: DeploymentapiVersion: extensions/v1beta1metadata:  labels:    app: kubernetes-dashboard    version: v1.10.1    name: kubernetes-dashboard  namespace: kube-systemspec:  replicas: 1  selector:    matchLabels:      app: kubernetes-dashboard  template:    metadata:      labels:        app: kubernetes-dashboard    spec:      containers:      - name: kubernetes-dashboard        image: lizhenliang/kubernetes-dashboard-amd64:v1.10.1 #已修改为国内镜像        imagePullPolicy: Always        ports:        - containerPort: 9090          protocol: TCP        livenessProbe:          httpGet:            path: /            port: 9090          initialDelaySeconds: 30          timeoutSeconds: 30---kind: ServiceapiVersion: v1metadata:  labels:    app: kubernetes-dashboard  name: kubernetes-dashboard  namespace: kube-system  type: NodePort  ports:  - port: 80    targetPort: 9090  selector:    app: kubernetes-dashboard</code></pre><p>创建服务</p><pre><code class="shell">kubectl create -f kubernetes-dashboard.yamlkubectl get pods --all-namespaces=trueNAMESPACE     NAME                                    READY     STATUS              RESTARTS   AGEkube-system   kubernetes-dashboard-1745970253-90ppp   0/1       ContainerCreating   0          24m</code></pre><p>完成后可通过浏览器访问 <a href="http://yourip:8080/ui" target="_blank" rel="noopener">http://yourip:8080/ui</a></p>]]></content>
    
    
    <categories>
      
      <category>kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>部署</tag>
      
      <tag>kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes部署Presto</title>
    <link href="/2020/03/28/Kubernetes%E9%83%A8%E7%BD%B2Presto/"/>
    <url>/2020/03/28/Kubernetes%E9%83%A8%E7%BD%B2Presto/</url>
    
    <content type="html"><![CDATA[<h2 id="一、介绍"><a href="#一、介绍" class="headerlink" title="一、介绍"></a>一、介绍</h2><blockquote><p>Presto</p></blockquote><p>Presto是一个分布式SQL查询引擎，用于查询分布在一个或多个不同数据源中的大数据集。完整安装包括一个Coordinator和多个Worker。 由客户端提交查询，从Presto命令行CLI提交到Coordinator。 Coordinator进行解析，分析并执行查询计划，然后分发处理队列到Worker。</p><p>Presto是完全基于内存的分布式大数据查询引擎，所有查询和计算都在内存中执行。</p><p>Presto的输入是SQL语句；输出是具体的SQL执行结果。</p><p>Presto可以对接不同的数据源，例如MySQL、Hive等。</p><p>Presto可以对SQL的查询过程进行优化，包括SQL本身的执行计划优化，以及用分布式查询提高并发等。</p><p>Presto不是数据库，并不能处理在线事务。</p><blockquote><p>ceph </p></blockquote><p>Ceph是当前非常流行的开源分布式存储系统，具有高扩展性、高性能、高可靠性等优点，同时提供块存储服务(rbd)、对象存储服务(rgw)以及文件系统存储服务(cephfs)，Ceph在存储的时候充分利用存储节点的计算能力，在存储每一个数据时都会通过计算得出该数据的位置，尽量的分布均衡。</p><blockquote><p>rook</p></blockquote><p>Rook是一个开放源码的云本机存储协调器，提供平台、框架和对各种存储解决方案的支持，以便与云本机环境进行本机集成。</p><p>Rook将存储软件转变为自我管理、自我扩展和自我修复的存储服务。它通过自动化部署、引导、配置、供应、扩展、升级、迁移、灾难恢复、监视和资源管理来实现这一点。Rook使用底层云本地容器管理、调度和协调平台提供的设施来执行其职责。</p><p>Rook利用扩展点深入集成到云本机环境中，为调度、生命周期管理、资源管理、安全、监控和用户体验提供无缝体验。</p><blockquote><p>基于kubernetes部署presto</p></blockquote><p>将presto数据持久化到ceph集群中，保证presto的数据高可用</p><h2 id="二、软件版本"><a href="#二、软件版本" class="headerlink" title="二、软件版本"></a>二、软件版本</h2><ul><li>Kubernetes v1.15（<code>单节点环境亦可</code>）</li><li>Rook v1.0.2</li><li>CentOS 7 </li><li>presto 332（prestosql版本）</li><li>jdk-8</li></ul><h2 id="三、部署rook及ceph"><a href="#三、部署rook及ceph" class="headerlink" title="三、部署rook及ceph"></a>三、部署rook及ceph</h2><p>rook的部署可以使用kubernetes的包管理工具<a href="https://helm.sh/" target="_blank" rel="noopener">helm</a>安装，由于rook及kubernetes版本众多以及涉及到相关源等问题，本文没有采用helm安装方式部署rook</p><pre><code class="shell">$ git clone https://github.com/rook/rook.git $ cd rook $ git checkout v1.0.2$ cd cluster/examples/kubernetes/ceph/</code></pre><p><code>cluster/examples/kubernetes/ceph/</code>目录包含部署rook相关文件</p><p>安装Rook Common Objects &amp; Operator</p><pre><code class="shell">kubectl create -f common.yamlkubectl create -f operator.yaml</code></pre><p>检查<code>rook-ceph-operator,rook-ceph-agent</code>, rook-discover`安装成功</p><pre><code class="shell">$ kubectl get pods -n rook-ceph NAME                                   READY       STATUS        RESTARTS    AGE rook-ceph-agent-chj4l                  1/1         Running       0           84s rook-ceph-operator-548b56f995-v4mvp    1/1         Running       0           4m5s rook-discover-vkkvl                    1/1         Running       0    </code></pre><h3 id="3-1-部署Rook-Ceph-Cluster"><a href="#3-1-部署Rook-Ceph-Cluster" class="headerlink" title="3.1 部署Rook Ceph Cluster"></a>3.1 部署Rook Ceph Cluster</h3><pre><code class="shell">kubectl apply -f cluster.yml</code></pre><p>检查是否部署成功</p><pre><code class="shell">kubectl get pods -n rook-cephNAME                                  READY   STATUS      RESTARTS   AGErook-ceph-agent-chl7b                 1/1     Running     0          4h14mrook-ceph-agent-tbx4l                 1/1     Running     2          4h14mrook-ceph-mgr-a-bf88cfdc4-wvhjb       1/1     Running     0          3h52mrook-ceph-mon-a-df88f8cbf-dklvj       1/1     Running     0          3h53mrook-ceph-operator-548b56f995-q47jb   1/1     Running     5          4h14mrook-ceph-osd-0-655957b867-p8tf7      1/1     Running     0          3h51mrook-ceph-osd-1-7578dc7874-f9q64      1/1     Running     0          3h50mrook-ceph-osd-prepare-node-2-nvr7d    0/2     Completed   0          3h45mrook-ceph-osd-prepare-node-3-2mm2p    0/2     Completed   0          3h45mrook-discover-44789                   1/1     Running     3          4h14mrook-discover-c4r7d                   1/1     Running     0          4h14m</code></pre><h3 id="3-2-测试ceph-cluster"><a href="#3-2-测试ceph-cluster" class="headerlink" title="3.2 测试ceph cluster"></a>3.2 测试ceph cluster</h3><p>创建StorageClass以便ceph cluster动态创建PV、</p><pre><code class="shell">kubectl create -f storageclass-test.yaml# 查找Rook Operator$ kubectl get pod -n rook-ceph --selector=app=rook-ceph-operator NAME                                  READY   STATUS    RESTARTS   AGErook-ceph-operator-548b56f995-q47jb   1/1     Running   5          4h23m# 检查ceph cluster健康状态kubectl exec -n rook-ceph -it rook-ceph-operator-548b56f995-q47jb -- ceph status  cluster:    id:     bed40efd-b3e8-4b18-b9a0-e8723f51e747    health: HEALTH_OK  services:    mon: 1 daemons, quorum a (age 4h)    mgr: a(active, since 3h)    osd: 2 osds: 2 up (since 3h), 2 in (since 3h)  data:    pools:   1 pools, 100 pgs    objects: 0 objects, 0 B    usage:   13 GiB used, 22 GiB / 35 GiB avail    pgs:     100 active+clean# 查看ceph磁盘信息kubectl exec -n rook-ceph -it rook-ceph-operator-548b56f995-q47jb -- ceph dfRAW STORAGE:    CLASS     SIZE       AVAIL      USED       RAW USED     %RAW USED    hdd       35 GiB     22 GiB     13 GiB       13 GiB         37.40    TOTAL     35 GiB     22 GiB     13 GiB       13 GiB         37.40POOLS:    POOL            ID     STORED     OBJECTS     USED     %USED     MAX AVAIL    replicapool      1        0 B           0      0 B         0        19 GiB</code></pre><h3 id="3-3-配置ceph-dashborad"><a href="#3-3-配置ceph-dashborad" class="headerlink" title="3.3 配置ceph dashborad"></a>3.3 配置ceph dashborad</h3><p>在cluster.yaml文件中默认已经启用了ceph dashboard，查看dashboard的service：</p><pre><code class="shell">[root@node-1 presto-kubernetes]# kubectl get svc -n rook-cephNAME                                     TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)             AGErook-ceph-mgr                            ClusterIP   10.1.73.175    &lt;none&gt;        9283/TCP            6h29mrook-ceph-mgr-dashboard                  ClusterIP   10.1.228.105   &lt;none&gt;        8443/TCP            6h29mrook-ceph-mgr-dashboard-external-https   NodePort    10.1.38.28     &lt;none&gt;        8443:30102/TCP      21mrook-ceph-mon-a                          ClusterIP   10.1.130.153   &lt;none&gt;        6789/TCP,3300/TCP   6h31m</code></pre><p>rook-ceph-mgr-dashboard监听的端口是8443，创建nodeport类型的service以便集群外部访问。</p><pre><code class="shell">kubectl apply -f rook/cluster/examples/kubernetes/ceph/dashboard-external-https.yaml</code></pre><p>查看一下nodeport暴露的端口，这里是30102端口：</p><pre><code>[root@node-1 presto-kubernetes]# kubectl get svc -n rook-cephNAME                                     TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)             AGErook-ceph-mgr                            ClusterIP   10.1.73.175    &lt;none&gt;        9283/TCP            6h29mrook-ceph-mgr-dashboard                  ClusterIP   10.1.228.105   &lt;none&gt;        8443/TCP            6h29mrook-ceph-mgr-dashboard-external-https   NodePort    10.1.38.28     &lt;none&gt;        8443:30102/TCP      21mrook-ceph-mon-a                          ClusterIP   10.1.130.153   &lt;none&gt;        6789/TCP,3300/TCP   6h31m</code></pre><p>获取Dashboard的登陆账号和密码</p><pre><code class="shell">[root@node-1 ~]# MGR_POD=`kubectl get pod -n rook-ceph | grep mgr | awk &#39;{print $1}&#39;`[root@node-1 ~]# kubectl -n rook-ceph logs $MGR_POD | grep passworddebug 2020-02-25 23:54:04.280 7ffb08d89700  0 log_channel(audit) log [DBG] : from=&#39;client.4198 -&#39; entity=&#39;client.admin&#39; cmd=[{&quot;username&quot;: &quot;admin&quot;, &quot;prefix&quot;: &quot;dashboard set-login-credentials&quot;, &quot;password&quot;: &quot;Wo9nf64nDs&quot;, &quot;target&quot;: [&quot;mgr&quot;, &quot;&quot;], &quot;format&quot;: &quot;json&quot;}]: dispatchdebug 2020-02-25 23:58:11.177 7f8606e0e700  0 log_channel(audit) log [DBG] : from=&#39;client.4373 -&#39; entity=&#39;client.admin&#39; cmd=[{&quot;username&quot;: &quot;admin&quot;, &quot;prefix&quot;: &quot;dashboard set-login-credentials&quot;, &quot;password&quot;: &quot;Wo9nf64nDs&quot;, &quot;target&quot;: [&quot;mgr&quot;, &quot;&quot;], &quot;format&quot;: &quot;json&quot;}]: dispatch</code></pre><p>找到username和password字段，我这里是admin，Wo9nf64nDs<br>打开浏览器输入任意一个Node的IP+nodeport端口，这里使用master节点 ip访问：<a href="https://localhost:30102/#/dashboard" target="_blank" rel="noopener">https://localhost:30102/#/dashboard</a></p><h2 id="四、部署presto"><a href="#四、部署presto" class="headerlink" title="四、部署presto"></a>四、部署presto</h2><h3 id="4-1-部署步骤"><a href="#4-1-部署步骤" class="headerlink" title="4.1 部署步骤"></a>4.1 部署步骤</h3><ul><li><p>构建镜像presto-server端机presto-cli端的镜像，hdfs-site及core-site配置文件可以构建到镜像中，方便后续配置hive connector。通过脚本<code>build_image.sh</code>构建</p><blockquote><p><code>presto-serve端Dockerfile</code></p></blockquote><pre><code class="dockerfile">FROM centos:centos7.5.1804RUN mkdir -p /etc/hadoop/confADD jdk-11.0.6_linux-x64_bin.tar.gz /optADD presto-server-332.tar.gz /optADD core-site.xml /etc/hadoop/confADD hdfs-site.xml /etc/hadoop/confADD hudi-presto-bundle-0.5.2-incubating-sources.jar /opt/presto-server-332/plugin/hive-hadoop2ADD hudi-presto-bundle-0.5.2-incubating.jar /opt/presto-server-332/plugin/hive-hadoop2ADD original-hudi-presto-bundle-0.5.2-incubating-sources.jar /opt/presto-server-332/plugin/hive-hadoop2ADD original-hudi-presto-bundle-0.5.2-incubating.jar /opt/presto-server-332/plugin/hive-hadoop2ENV PRESTO_HOME /opt/presto-server-332ENV JAVA_HOME /opt/jdk-11.0.6ENV PATH $JAVA_HOME/bin:$PATH</code></pre><blockquote><p><code>presto-client端的Dockerfile</code></p></blockquote><pre><code class="dockerfile">FROM openjdk:8-slimADD presto-cli-332-executable.jar /optRUN  mv /opt/presto-cli-332-executable.jar /opt/presto-cli  &amp;&amp; chmod +x /opt/presto-cli</code></pre></li></ul><ul><li><p>配置server端相关配置 <code>presto-config-cm.yaml</code></p><p>server端的配置<strong>注意</strong>两个地方</p><ul><li>每个presto节点的node.id需要不一样</li><li>jvm参数需要加上<code>-DHADOOP_USER_NAME=hdfs</code>及<code>-Dpresto-temporarily-allow-java8=true</code>确保presto以HDFS用户访问hdfs文件及解决presto安装jdk8报错的问题</li></ul><pre><code class="yaml">apiVersion: v1kind: ConfigMapmetadata:  name: presto-config-cm  namespace: presto  labels:    app: presto-coordinatordata:  bootstrap.sh: |-    #!/bin/bash    cd /root/bootstrap    mkdir -p $PRESTO_HOME/etc/catalog    cat ./node.properties &gt; $PRESTO_HOME/etc/node.properties    cat ./jvm.config &gt; $PRESTO_HOME/etc/jvm.config    cat ./config.properties &gt; $PRESTO_HOME/etc/config.properties    cat ./log.properties &gt; $PRESTO_HOME/etc/log.properties    echo &quot;&quot; &gt;&gt; $PRESTO_HOME/etc/node.properties    echo &quot;node.id=$HOSTNAME&quot; &gt;&gt; $PRESTO_HOME/etc/node.properties    sed -i &#39;s/${COORDINATOR_NODE}/&#39;$COORDINATOR_NODE&#39;/g&#39; $PRESTO_HOME/etc/config.properties    if ${COORDINATOR_NODE};     then      echo coordinator    else       sed -i &#39;7d&#39; $PRESTO_HOME/etc/config.properties      echo worker    fi    for cfg in ../catalog/*; do      cat $cfg &gt; $PRESTO_HOME/etc/catalog/${cfg##*/}    done    $PRESTO_HOME/bin/launcher run --verbose  node.properties: |-    node.environment=production    node.data-dir=/var/presto/data  jvm.config: |-    -server    -Xmx16G    -XX:-UseBiasedLocking    -XX:+UseG1GC    -XX:G1HeapRegionSize=32M    -XX:+ExplicitGCInvokesConcurrent    -XX:+ExitOnOutOfMemoryError    -XX:+UseGCOverheadLimit    -XX:+HeapDumpOnOutOfMemoryError    -XX:ReservedCodeCacheSize=512M    -Djdk.attach.allowAttachSelf=true    -Djdk.nio.maxCachedBufferSize=2000000    -Dpresto-temporarily-allow-java8=true    -DHADOOP_USER_NAME=hdfs  config.properties: |-    coordinator=${COORDINATOR_NODE}    node-scheduler.include-coordinator=true    http-server.http.port=8080    query.max-memory=10GB    query.max-memory-per-node=1GB    query.max-total-memory-per-node=2GB    discovery-server.enabled=true    discovery.uri=http://presto:8080  log.properties: |-    io.prestosql=DEBUG</code></pre></li><li><p>catalog相关配置<code>presto-catalog-config-cm.yaml</code></p><pre><code class="yaml">apiVersion: v1kind: ConfigMapmetadata:  name: presto-catalog-config-cm  namespace: presto  labels:    app: presto-coordinatordata:  hive.properties: |-    connector.name=hive-hadoop2    hive.metastore.uri=thrift://ip:9083    hive.config.resources=/etc/hadoop/conf/core-site.xml,/etc/hadoop/conf/hdfs-site.xml  mysql.properties: |-    connector.name=mysql    connection-url=jdbc:mysql://ip:30306    connection-user=root    connection-password=Qloud@dev?123</code></pre></li><li><p>presto-svc、worker、coordinator、presto-cli的部署<code>deployment.yaml</code></p><pre><code class="yaml">apiVersion: v1kind: Servicemetadata:  name: presto  namespace: prestospec:  ports:  - port: 8080  selector:    app: presto-coordinator  type: NodePort---apiVersion: apps/v1kind: Deploymentmetadata:    name: presto-coordinator    namespace: prestospec:    replicas: 1    revisionHistoryLimit: 10    selector:    matchLabels:        app: presto-coordinator    template:    metadata:        labels:        app: presto-coordinator    spec:        containers:        - name: presto-coordinator            image: reg.chebai.org/presto/presto-server:332            command: [&quot;bash&quot;, &quot;-c&quot;, &quot;sh /root/bootstrap/bootstrap.sh&quot;]            ports:            - containerPort: 8080            env:            - name: COORDINATOR_NODE                value: &quot;true&quot;            volumeMounts:            - name: presto-config-volume                mountPath: /root/bootstrap            - name: presto-catalog-config-volume                mountPath: /root/catalog            - name: presto-data-volume                mountPath: /var/presto/data        volumes:        - name: presto-config-volume            configMap:            name: presto-config-cm        - name: presto-catalog-config-volume            configMap:            name: presto-catalog-config-cm        - name: presto-data-volume            emptyDir: {}---apiVersion: apps/v1kind: Deploymentmetadata:    name: presto-worker    namespace: prestospec:    replicas: 2    revisionHistoryLimit: 10    selector:    matchLabels:        app: presto-worker    template:    metadata:        labels:        app: presto-worker    spec:        containers:        - name: presto-worker            image: reg.chebai.org/presto/presto-server:332            command: [&quot;bash&quot;, &quot;-c&quot;, &quot;sh /root/bootstrap/bootstrap.sh&quot;]            ports:            - containerPort: 8080            env:            - name: COORDINATOR_NODE                value: &quot;false&quot;            volumeMounts:            - name: presto-config-volume                mountPath: /root/bootstrap            - name: presto-catalog-config-volume                mountPath: /root/catalog            - name: presto-data-volume                mountPath: /var/presto/data        volumes:        - name: presto-config-volume            configMap:            name: presto-config-cm        - name: presto-catalog-config-volume            configMap:            name: presto-catalog-config-cm        - name: presto-data-volume            emptyDir: {}---apiVersion: v1kind: Podmetadata:    name: presto-cli    namespace: prestospec:    containers:    - name: presto-cli    image: reg.chebai.org/presto/presto-cli:332    command: [&quot;tail&quot;, &quot;-f&quot;, &quot;/dev/null&quot;]    imagePullPolicy: Always    restartPolicy: Always</code></pre></li><li><p>启动presto</p><pre><code class="shell">kubectl create -f presto-config-cm.yamlkubectl create -f presto-catalog-config-cm.yamlkubectl create -f deployment.yaml </code></pre></li><li><p>使用presto，找出外部地址</p><pre><code class="shell">[root@node-1 ~]# kubectl get svc -n prestoNAME     TYPE       CLUSTER-IP    EXTERNAL-IP   PORT(S)          AGEpresto   NodePort   10.1.27.143   &lt;none&gt;        8080:32151/TCP   27h</code></pre></li><li><p>使用presto-cli客户端连接不同connector</p><pre><code class="shell">kubectl exec -it presto-cli -n presto /opt/presto-cli -- --server presto:8080 --catalog hive --schema default</code></pre></li></ul><h3 id="4-2-挂载ceph卷到presto"><a href="#4-2-挂载ceph卷到presto" class="headerlink" title="4.2 挂载ceph卷到presto"></a>4.2 挂载ceph卷到presto</h3><blockquote><p>由于github开源版本，没有提供数据卷挂载，因此需要相关yaml文件进行修改，配置rook提供RBD服务</p></blockquote><p>rook可以提供以下3类型的存储：<br> Block: Create block storage to be consumed by a pod<br> Object: Create an object store that is accessible inside or outside the Kubernetes cluster<br> Shared File System: Create a file system to be shared across multiple pods<br>在提供（Provisioning）块存储之前，需要先创建StorageClass和存储池。K8S需要这两类资源，才能和Rook交互，进而分配持久卷（PV）。<br>在kubernetes集群里，要提供rbd块设备服务，需要有如下步骤：</p><ol><li>创建rbd-provisioner pod</li><li>创建rbd对应的storageclass</li><li>创建pvc，使用rbd对应的storageclass</li><li>创建pod使用rbd pvc</li></ol><p>通过rook创建Ceph Cluster之后，rook自身提供了rbd-provisioner服务，所以我们不需要再部署其provisioner。</p><pre><code class="shell"># 配置storageclasskubectl apply -f rook/cluster/examples/kubernetes/ceph/storageclass.yaml# 检查storageclasskubectl get storageclassNAME              PROVISIONER          AGErook-ceph-block   ceph.rook.io/block   171m</code></pre><blockquote><p>修改preso-kubernetes文件夹中的worker相关配置</p></blockquote><pre><code class="shell">vim deployment.yamlapiVersion: apps/v1kind: Deploymentmetadata:  name: presto-workerspec:  replicas: 2  revisionHistoryLimit: 10  selector:    matchLabels:      app: presto-worker  template:    metadata:      labels:        app: presto-worker    spec:      initContainers:        - name: wait-coordinator          image:  chenlianguu/presto-server:dm-0.208          command: [&quot;bash&quot;, &quot;-c&quot;, &quot;until curl -sf http://presto-coordinator-service:8080/ui/; do echo &#39;waiting for coordinator started...&#39;; sleep 2; done;&quot;]      containers:        - name: presto-worker          image: chenlianguu/presto-server:dm-0.208          command: [&quot;bash&quot;, &quot;-c&quot;, &quot;sh /root/bootstrap/bootstrap.sh&quot;]          ports:            - name: http-coord              containerPort: 8080              protocol: TCP          env:            - name: COORDINATOR_NODE              value: &quot;false&quot;          volumeMounts:            - name: presto-config-volume              mountPath: /root/bootstrap            - name: presto-catalog-config-volume              mountPath: /root/catalog            - name: presto-data-volume              mountPath: /var/presto/data          readinessProbe:            initialDelaySeconds: 10            periodSeconds: 5            exec:              command: [&quot;bash&quot;, &quot;-c&quot;, &quot;curl -s http://presto-coordinator-service:8080/v1/node | tr &#39;,&#39; &#39;\n&#39; | grep -s $(hostname -i)&quot;]      volumes:        - name: presto-config-volume          configMap:            name: presto-config-cm        - name: presto-catalog-config-volume          configMap:            name: presto-catalog-config-cm        - name: presto-data-volume          persistentVolumeClaim:            claimName: presto-data-claim</code></pre><p>重新apply yaml文件</p><pre><code class="shell">kubectl aplly -f worker-deployment.yaml</code></pre><p>检查是否配置成功</p><pre><code class="shell">[root@node-1 ~]# kubectl get pvNAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                       STORAGECLASS      REASON   AGEpvc-1ff7bb97-238e-4ebd-bfc3-fb9db1ae5656   10Gi       RWO            Delete           Bound    default/presto-data-claim   rook-ceph-block            100m[root@node-1 ~]# kubectl get pvcNAME                STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGEpresto-data-claim   Bound    pvc-1ff7bb97-238e-4ebd-bfc3-fb9db1ae5656   10Gi       RWO            rook-ceph-block   100m</code></pre><p>注意：这里的pv会自动创建，当提交了包含 StorageClass 字段的 PVC 之后，Kubernetes 就会根据这个 StorageClass 创建出对应的 PV，这是用到的是Dynamic Provisioning机制来动态创建pv，PV 支持 Static 静态请求，和动态创建两种方式。</p><pre><code class="shell"># ceph集群端检查[root@node-1 ~]# kubectl exec -n rook-ceph -it rook-ceph-operator-548b56f995-q47jb -- rbd info -p replicapool pvc-1ff7bb97-238e-4ebd-bfc3-fb9db1ae5656rbd image &#39;pvc-1ff7bb97-238e-4ebd-bfc3-fb9db1ae5656&#39;:        size 10 GiB in 2560 objects        order 22 (4 MiB objects)        snapshot_count: 0        id: 1dd4352e100d        block_name_prefix: rbd_data.1dd4352e100d        format: 2        features: layering        op_features:        flags:        create_timestamp: Wed Feb 26 05:23:09 2020        access_timestamp: Wed Feb 26 05:23:09 2020        modify_timestamp: Wed Feb 26 05:23:09 2020</code></pre><p>登陆pod检查rbd设备</p><pre><code class="shell">[root@node-1 presto-kubernetes]# kubectl exec -it worker-7b66b5cb5-d2vzt bashroot@worker-7b66b5cb5-d2vzt:/usr/lib/presto-server-0.167-t.0.3/etc# df -hFilesystem                                                                                        Size  Used Avail Use% Mounted on/dev/mapper/docker-253:0-203738-8a1bc94ee31c6ca3e406a6c740b84cd8bd2c681c568dc8a4007f273208bbd9fd   10G  1.4G  8.7G  14% /tmpfs                                                                                              64M     0   64M   0% /devtmpfs                                                                                             1.9G     0  1.9G   0% /sys/fs/cgroup/dev/mapper/centos-root                                                                            18G  6.3G   12G  36% /etc/hostsshm                                                                                                64M     0   64M   0% /dev/shm/dev/rbd0                                                                                          10G   33M   10G   1% /var/presto/datatmpfs                                                                                             1.9G   12K  1.9G   1% /run/secrets/kubernetes.io/serviceaccounttmpfs                                                                                             1.9G     0  1.9G   0% /proc/acpitmpfs                                                                                             1.9G     0  1.9G   0% /proc/scsitmpfs                                                                                             1.9G     0  1.9G   0% /sys/firmware</code></pre><h3 id="4-3-presto多源异构查询"><a href="#4-3-presto多源异构查询" class="headerlink" title="4.3 presto多源异构查询"></a>4.3 presto多源异构查询</h3><blockquote><p>相关connector采用configmap方式进行灵活配置，不同connector的配置参考<a href="https://prestosql.io/docs/current/connector.html" target="_blank" rel="noopener">官网Doc</a></p></blockquote><pre><code class="yaml"># 创建presto-catalog-config-cm.yamlcat &lt;&lt; EOF &gt; ～/presto-kubernetes/presto-catalog-config-cm.yamlapiVersion: v1kind: ConfigMapmetadata:  name: presto-catalog-config-cm  labels:    app: presto-coordinatordata:  # hive相关配置  hive.properties: |-    connector.name=hive-hadoop2    hive.metastore.uri=thrift://hive-metastore-ip:9083  # mysql相关配置  mysql.properties: |-    connector.name=mysql    connection-url=jdbc:mysql://example.net:3306    connection-user=root    connection-password=secret  cassandra.properties: |-    connector.name=cassandra    cassandra.contact-points=host1,host2EOF</code></pre><p>配置好之后需要生效，重新apply <code>presto-catalog-config-cm.yaml</code> 并<strong>删除coordinator及word相关的pod</strong>，删除之后会重新生成新的pod，新的pod会载入新的connector配置</p><pre><code class="shell">kubectl apply -f presto-catalog-config-cm.yamlkubectl apply -f coordinator-deployment.yamlkubectl apply -f worker-deployment.yamlkubectl get podsNAME                           READY   STATUS    RESTARTS   AGEcoordinator-8549f46c58-q8ngm   1/1     Running   0          9m29sworker-d64744f4d-8j2vg         1/1     Running   0          9m10skubectl delete pod coordinator-8549f46c58-q8ngm worker-d64744f4d-8j2vg</code></pre><h3 id="4-4-presto相关connect的测试"><a href="#4-4-presto相关connect的测试" class="headerlink" title="4.4 presto相关connect的测试"></a>4.4 presto相关connect的测试</h3><h4 id="4-4-1-presto查询hive中数据"><a href="#4-4-1-presto查询hive中数据" class="headerlink" title="4.4.1 presto查询hive中数据"></a>4.4.1 presto查询hive中数据</h4><p>首先准备hive环境，这里直接利用docker启动hive环境，简单方便高效快捷，参考github项目<a href="https://github.com/big-data-europe/docker-hive" target="_blank" rel="noopener">docker-hive</a></p><pre><code class="shell">git clone https://github.com/big-data-europe/docker-hive.gitcd docker-hive # 启动docker-hivedocker-compose up -d# 查看启动状态                 Name                                Command                  State                          Ports--------------------------------------------------------------------------------------------------------------------------------------docker-hive_datanode_1                    /entrypoint.sh /run.sh           Up (healthy)   0.0.0.0:50075-&gt;50075/tcpdocker-hive_hive-metastore-postgresql_1   /docker-entrypoint.sh postgres   Up             5432/tcp`docker-hive_hive-metastore_1              entrypoint.sh /opt/hive/bi ...   Up             10000/tcp, 10002/tcp, 0.0.0.0:9083-&gt;9083/tcp`docker-hive_hive-server_1                 entrypoint.sh /bin/sh -c s ...   Up             0.0.0.0:10000-&gt;10000/tcp, 10002/tcpdocker-hive_namenode_1                    /entrypoint.sh /run.sh           Up (healthy)   0.0.0.0:50070-&gt;50070/tcpdocker-hive_presto-coordinator_1          ./bin/launcher run               Up             0.0.0.0:8080-&gt;8080/tcp# 往hive里边加载数据docker-compose exec hive-server bash# /opt/hive/bin/beeline -u jdbc:hive2://localhost:10000  &gt; CREATE TABLE pokes (foo INT, bar STRING);  &gt; LOAD DATA LOCAL INPATH &#39;/opt/hive/examples/files/kv1.txt&#39; OVERWRITE INTO TABLE pokes;</code></pre><p>hive-metastore的连接端口为9083，确认该端口处于监听状态 <code>presto连接hive进行测试</code></p><pre><code class="shell">kubectl exec -it presto-cli -n presto /opt/presto-cli -- --server presto:8080 --catalog hive --schema defaultpresto:default&gt; show schemas;       Schema-------------------- default information_schema(2 rows)Query 20200409_121447_00002_3jete, FINISHED, 2 nodesSplits: 18 total, 18 done (100.00%)0:00 [2 rows, 35B] [16 rows/s, 290B/s]presto:default&gt; show tables; Table------- `pokes`(1 row)Query 20200409_121459_00003_3jete, FINISHED, 2 nodesSplits: 18 total, 18 done (100.00%)0:00 [1 rows, 22B] [9 rows/s, 201B/s]presto:default&gt; select * from pokes;Query 20200409_121548_00004_3jete, FAILED, 1 nodeSplits: 16 total, 0 done (0.00%)0:01 [0 rows, 0B] [0 rows/s, 0B/s]Query 20200409_121548_00004_3jete failed: java.net.UnknownHostException: namenode</code></pre><p>可以找到hive中表，但是查询不成功，查询不成功是因为使用<code>docker-compose</code>方式搭建的hive环境，其中namenode节点和kubernetes中的presto不在一套网络环境中，无法解析namenode地址，有条件可以用hive集群测试，本次测试环境都基于单机。</p><h4 id="4-4-2-presto查询mysql中数据"><a href="#4-4-2-presto查询mysql中数据" class="headerlink" title="4.4.2 presto查询mysql中数据"></a>4.4.2 presto查询mysql中数据</h4><p>首先准备mysql环境，这里直接利用docker启动mysql环境</p><pre><code class="shell">docker run --name mysql -d -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 mysql:5.6.36docker exec -it mysql /bin/bash# 造数据mysql -uroot -p123456mysql&gt; create database test;mysql&gt; use test;Database changedmysql&gt; create table user(id int not null, username varchar(32) not null, password varchar(32) not null);mysql&gt; insert into user values(1,&#39;user1&#39;,&#39;password1&#39;);mysql&gt; insert into user values(2,&#39;user2&#39;,&#39;password2&#39;);mysql&gt; insert into user values(3,&#39;user3&#39;,&#39;password3&#39;);mysql&gt; select * from user;+----+----------+-----------+| id | username | password  |+----+----------+-----------+|  1 | user1    | password1 ||  2 | user2    | password2 ||  3 | user3    | password3 |+----+----------+-----------+3 rows in set (0.00 sec)</code></pre><p>mysql的连接端口为3306，确认该端口处于监听状态 <code>presto连接mysql进行测试</code></p><pre><code class="shell">kubectl exec -it presto-cli -n presto /opt/presto-cli -- --server presto:8080 --catalog mysql --schema testpresto:test&gt; show tables; Table------- user(1 row)Query 20200409_122956_00004_6mm78, FINISHED, 2 nodesSplits: 18 total, 18 done (100.00%)0:00 [1 rows, 18B] [10 rows/s, 187B/s]presto:test&gt; select * from user; id | username | password----+----------+-----------  1 | user1    | password1  2 | user2    | password2  3 | user3    | password3(3 rows)Query 20200409_123010_00005_6mm78, FINISHED, 1 nodeSplits: 17 total, 17 done (100.00%)0:00 [3 rows, 0B] [10 rows/s, 0B/s]</code></pre><p><code>presto</code>可以查到mysql中的表及表数据</p><h4 id="4-4-3-presto查询cassandra中的数据"><a href="#4-4-3-presto查询cassandra中的数据" class="headerlink" title="4.4.3 presto查询cassandra中的数据"></a><strong>4.4.3 presto查询cassandra中的数据</strong></h4><p>首先准备mysql环境，这里直接利用docker启动cassandra环境</p><pre><code class="shell">docker run --name cassandra -p 9042:9042 -d cassandra:3.0</code></pre><p>连接数据库</p><img src="https://i.loli.net/2020/04/10/mQgKFSxtYNh8HGT.png" srcset="/img/loading.gif" style="zoom:50%;"><p>准备数据</p><pre><code class="sql">CREATE KEYSPACE IF NOT EXISTS pimin_netWITH REPLICATION = {&#39;class&#39;: &#39;SimpleStrategy&#39;,&#39;replication_factor&#39;:1};USE pimin_net;CREATE TABLE users (id int,user_name varchar,PRIMARY KEY (id) );INSERT INTO users (id,user_name) VALUES (1,&#39;china&#39;);INSERT INTO users (id,user_name) VALUES (2,&#39;taiwan&#39;);select * from users;</code></pre><img src="https://i.loli.net/2020/04/13/FmflGnxRWAsLIgB.jpg" srcset="/img/loading.gif" style="zoom:50%;"><p>查询数据</p><pre><code class="shell">kubectl exec -it presto-cli -n presto /opt/presto-cli -- --server presto:8080 --catalog cassandra --schema defaultpresto:pimin_net&gt; show tables; Table------- users(1 row)Query 20200410_024828_00002_4ekb9, FINISHED, 2 nodesSplits: 18 total, 18 done (100.00%)0:00 [1 rows, 24B] [5 rows/s, 133B/s]presto:pimin_net&gt; select * from users; id | user_name----+-----------  1 | china  2 | taiwan(2 rows)Query 20200410_024834_00003_4ekb9, FINISHED, 1 nodeSplits: 273 total, 273 done (100.00%)0:02 [2 rows, 2B] [1 rows/s, 1B/s]presto:pimin_net&gt;</code></pre><p>可以查询到cassandra中的数据</p><h3 id="4-5-弹性伸缩"><a href="#4-5-弹性伸缩" class="headerlink" title="4.5 弹性伸缩"></a>4.5 弹性伸缩</h3><pre><code class="shell">kubectl get deployment -n prestoNAME                  DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGEpresto-coordinator    1         1         1            1           21mpresto-worker         2         2         2            2           21mkubectl scale deployment presto-worker --replicas=3 -n prestodeployment &quot;presto-worker&quot; scaledkubectl get deploymentNAME                  DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGEpresto-coordinator    1         1         1            1           23mpresto-worker         3         3         3            3           23m</code></pre><h2 id="五、Trouble-Shooting"><a href="#五、Trouble-Shooting" class="headerlink" title="五、Trouble Shooting"></a>五、Trouble Shooting</h2><ul><li><p>Docker拉取镜像源time out或者拉取不上，增加docker镜像源，把163，阿里，Azure的docker加速器最好都加上</p><pre><code class="shell">vim /etc/docker/daemon.json{  &quot;registry-mirrors&quot;: [        &quot;https://dockerhub.azk8s.cn&quot;,        &quot;https://b3sst9pc.mirror.aliyuncs.com&quot;,        &quot;https://hub-mirror.c.163.com&quot;]}systemctl daemon-reloadsystemctl restart docker</code></pre></li><li><p>presto查询hive数据出现io.prestosql.spi.PrestoException: Could not obtain block: BP-1548201263错误，<code>show tables</code> <code>desc table</code>正常显示，通过在presto的pod容器内部使用hdfs 命令ls可以查看目录，但是cat hdfs上面的文件报相同的错误，说明无法联通datanode默认的50010端口，使用telnet命令可查看远程服务器是否开放次端口，通过开放端口或者解决防火墙方式解决，能够telnet成功该端口即可解决该问题 参考<a href="https://groups.google.com/forum/#!topic/presto-users/4yWpzR-zrds" target="_blank" rel="noopener">presto hive connector error reading from HDFS</a></p></li></ul><h2 id="六、注意事项"><a href="#六、注意事项" class="headerlink" title="六、注意事项"></a>六、注意事项</h2><ul><li><strong>当更新configmap的时候，由于不支持热更新，需要销毁掉presto的work及coordinatoe相关pod，销毁之后新启动的pod会载入最新的相关配置</strong></li><li>prestosql-332版本要求jdk11，但hadoop对jdk11不兼容，需要使用jdk8，并在presto的jvm参数上面加上-Dpresto-temporarily-allow-java8=true</li></ul><h2 id="七、Ref"><a href="#七、Ref" class="headerlink" title="七、Ref"></a>七、Ref</h2><ul><li><a href="https://info.crunchydata.com/blog/crunchy-postgresql-operator-with-rook-ceph-storage" target="_blank" rel="noopener">Using the PostgreSQL Operator with Rook Ceph Storage</a></li><li><a href="https://github.com/dharmeshkakadia/presto-kubernetes" target="_blank" rel="noopener">presto-kubernetes</a></li><li><a href="https://github.com/big-data-europe/docker-hive" target="_blank" rel="noopener">docker-hive</a></li><li><a href="https://github.com/joshuarobinson/presto-on-k8s" target="_blank" rel="noopener">presto-on-k8s</a></li><li><a href="https://blog.csdn.net/networken/article/details/85772418" target="_blank" rel="noopener">kubernetes部署rook+ceph存储系统</a></li><li><a href="https://blog.csdn.net/ygqygq2/article/details/103014350" target="_blank" rel="noopener">kubernetes上部署rook-ceph存储系统</a></li><li><a href="https://blog.csdn.net/chenleiking/article/details/82493798" target="_blank" rel="noopener">在Kubernetes上部署Presto</a></li><li><a href="https://docker_practice.gitee.io/install/mirror.html" target="_blank" rel="noopener">Docker镜像加速器</a></li><li><a href="https://www.jianshu.com/p/ba730747cc8c" target="_blank" rel="noopener">Presto连接MySQL</a></li><li><a href="https://medium.com/@uprush/presto-with-kubernetes-and-s3-deployment-4e262849244a" target="_blank" rel="noopener">Presto with Kubernetes and S3 — Deployment</a></li><li>[Presto-Powered S3 Data Warehouse on Kubernetes](</li></ul>]]></content>
    
    
    <categories>
      
      <category>kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>部署</tag>
      
      <tag>kubernetes</tag>
      
      <tag>presto</tag>
      
      <tag>rook</tag>
      
      <tag>ceph</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Docker大数据环境搭建指南</title>
    <link href="/2020/03/03/Docker%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%E6%8C%87%E5%8D%97/"/>
    <url>/2020/03/03/Docker%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%E6%8C%87%E5%8D%97/</url>
    
    <content type="html"><![CDATA[<h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>将大数据平台资源docker化，主要基于以下考虑</p><ul><li>开箱即用，不常驻后台，需要的时候启动集群即可，不用的时候关闭集群释放机器资源</li><li>避免了直接安装端口占用问题，大数据平台所需端口较多</li><li>spark资源docker化，便于后期k8s进行资源管理调度</li></ul><p>上述好处主要是基于测试环境下，基于生产环境的大数据平台化要考虑的点很多，例如后期扩容、运维、安全等因素，大数据平台整个是否docker化后期有待考证。</p><h2 id="镜像制作方案"><a href="#镜像制作方案" class="headerlink" title="镜像制作方案"></a>镜像制作方案</h2><p>使用Docker来搭建hadoop,spark及mysql的集群，首先使用Dockerfile制作镜像，把相关的软件拷贝到约定好的目录 下，把配置文件在外面先配置好，再拷贝移动到hadoop,spark的配置目录，为了能使得mysql能从其它节点被访问到，要配置mysql的访问权限。</p><h2 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h2><p>一共3个节点，即启动3个容器。hadoop-master,hadoop-node1,hadoop-node2这三个容器里面安装hadoop和spark集群。<br><img src="https://i.loli.net/2020/03/03/Ap2vwPqN9ijY84K.png" srcset="/img/loading.gif" alt="architect"></p><h2 id="集群部署"><a href="#集群部署" class="headerlink" title="集群部署"></a>集群部署</h2><h3 id="集群网络规划及子网配置"><a href="#集群网络规划及子网配置" class="headerlink" title="集群网络规划及子网配置"></a>集群网络规划及子网配置</h3><p>既然是做集群，网络的规划是少不了的,至于网络，可以通过Docker中的DockerNetworking的支持配置。首先设置网络，docker中设置 子网可以通过docker network create 方法，这里我们通过命令设置如下的子网。–subnet指定子网络的网段，并为这个子网命名一个名字叫spark</p><pre><code class="shell"># 创建子网docker network create --subnet=172.16.0.0/16 spark# 查看网络docker network lsNETWORK ID          NAME                       DRIVER              SCOPEfab2dd51d1cf        spark                      bridge              local</code></pre><p> 接下来就在我们创建的子网落spark中规划集群中每个容器的ip地址。网络ip分配如下:</p><p>hadoop-master 172.16.0.2</p><p>hadoop-node1 172.16.0.3</p><p>hadoop-node2 172.16.0.4</p><h3 id="软件版本"><a href="#软件版本" class="headerlink" title="软件版本"></a>软件版本</h3><p>网络规划好了，首先Spark我们使用最新的2.4.4版本，Hadoop采用比较稳定的hadoop-2.7.3版本，scala采用scala-2.11.8，JDK采用jdk-8u101-linux-x64。</p><h3 id="SSH无密钥登录规则配置"><a href="#SSH无密钥登录规则配置" class="headerlink" title="SSH无密钥登录规则配置"></a>SSH无密钥登录规则配置</h3><p>注意这里不使用ssh-keygen -t rsa -P ‘’这种方式生成id_rsa.pub，然后集群节点互拷贝id_rsa.pub到authorized_keys文件这种方式，而 是通过在.ssh目录下配置ssh_conf文件的方式，ssh_conf中可以配置SSH的通信规则，例如以正则表达式的方式指定hostname为XXX的 机器之间实现互联互通，而不进行额外的密钥验证。为了编写这个正则表达式，我们5个节点的hostname都以hadoop-*的方式作为开 头，这就是采用这种命名规则的原因。下面来看下ssh_conf配置的内容:</p><pre><code class="shell">Host localhost    StrictHostKeyChecking noHost 0.0.0.0     StrictHostKeyChecking noHost hadoop-*     StrictHostKeyChecking no</code></pre><p>注意上面的最后一行，Host hadoop-* 指定了它的严格的Host验证StrictHostKeyChecking 为no，这样既可以是这5个hostname以 hadoop-*开头的容器之间实现互联互通，而不需要二外的验证。</p><h3 id="构建镜像"><a href="#构建镜像" class="headerlink" title="构建镜像"></a>构建镜像</h3><p>Dockerfile编写完成，接下来写一个build.sh脚本，内容如下:</p><pre><code class="shell"> echo build hadoop images docker build -t=&quot;spark&quot; . </code></pre><p>表示构建一个名叫spark的镜像，.表示Dockerfile的路径，因为在当前路径下，所有用.,若在其他地方则用绝对路径指定Dockerfile的路径 即可。</p><p>运行sh build.sh，就会开始制作镜像了。</p><h2 id="集群运行"><a href="#集群运行" class="headerlink" title="集群运行"></a>集群运行</h2><h3 id="启动容器-start-container-sh"><a href="#启动容器-start-container-sh" class="headerlink" title="启动容器 start_container.sh"></a>启动容器 start_container.sh</h3><p>使用这个镜像可完成容器的启动，因为使用了基于DockerNetworking的网络机制，因此可以在启动容器的时候为容器在子网172.16.0.0/16 spark中分贝172.16.0.1 172.16.0.255以外的IP地址，容器内部容器的通信是基于hostname，因此 需要指定hostname，为了方便容器的管理，需要为启动的每个容器指定一个名字。为了方便外网访问，需要通过-p命令指定容器到宿主机的端口映射。还要为每个容器增加host列表。</p><pre><code class="shell"># hadoop-masterdocker run -itd --restart=always \    --net spark \    --ip 172.16.0.2 \    --privileged \    -p 18032:8032 \    -p 28080:18080 \    -p 29888:19888 \    -p 17077:7077 \    -p 51070:50070 \    -p 18888:8888 \    -p 19000:9000 \    -p 11100:11000 \    -p 51030:50030 \    -p 18050:8050 \    -p 18081:8081 \    -p 18900:8900 \    --name hadoop-master \    --hostname hadoop-master \    --add-host hadoop-node1:172.16.0.3 \    --add-host hadoop-node2:172.16.0.4 \    --add-host hadoop-mysql:172.16.0.6 \    spark /usr/sbin/init# hadoop-node1docker run -itd --restart=always \    --net spark \    --ip 172.16.0.3 \    --privileged \    -p 18042:8042 \    -p 51010:50010 \    -p 51020:50020 \    --name hadoop-node1 \    --hostname hadoop-node1 \    --add-host hadoop-master:172.16.0.2 \    --add-host hadoop-node2:172.16.0.4 \    spark /usr/sbin/init# hadoop-node2docker run -itd --restart=always \    --net spark \    --ip 172.16.0.4 \    --privileged \    -p 18043:8042 \    -p 51011:50011 \    -p 51021:50021 \    --name hadoop-node2 \    --hostname hadoop-node2 \    --add-host hadoop-master:172.16.0.2 \    --add-host hadoop-node1:172.16.0.3 \    spark /usr/sbin/init</code></pre><h3 id="关闭集群-stop-container-sh"><a href="#关闭集群-stop-container-sh" class="headerlink" title="关闭集群 stop_container.sh"></a>关闭集群 stop_container.sh</h3><pre><code class="shell">echo stop containersdocker stop hadoop-masterdocker stop hadoop-node1docker stop hadoop-node2echo remove containersdocker rm hadoop-masterdocker rm hadoop-node1docker rm hadoop-node2echo rm containersdocker ps</code></pre><h3 id="重启集群-restart-container-sh"><a href="#重启集群-restart-container-sh" class="headerlink" title="重启集群 restart_container.sh"></a>重启集群 restart_container.sh</h3><pre><code class="shell">echo stop containersdocker stop hadoop-masterdocker stop hadoop-node1docker stop hadoop-node2echo restart containersdocker start hadoop-masterdocker start hadoop-node1docker start hadoop-node2echo start sshddocker exec -it hadoop-master systemctl start sshddocker exec -it hadoop-node1 systemctl start sshddocker exec -it hadoop-node2 systemctl start sshddocker exec -it hadoop-master ~/restart-hadoop.shecho  containers starteddocker ps</code></pre><h2 id="Trouble-Shooting"><a href="#Trouble-Shooting" class="headerlink" title="Trouble Shooting"></a>Trouble Shooting</h2><ul><li><p>docker里面执行systemctl报错</p><p>解决方案：启动的时候用/usr/sbin/init</p></li><li><p>docker登录harbor报错<br>window直接修改docker设置，添加ip:8093到docker insecure registry<br>linux修改方法：<a href="https://blog.csdn.net/u010397369/article/details/42422243" target="_blank" rel="noopener">https://blog.csdn.net/u010397369/article/details/42422243</a>  </p></li></ul><h2 id="Demo"><a href="#Demo" class="headerlink" title="Demo"></a>Demo</h2><p>进入hadoop-master容器内部，执行spark-shell</p><pre><code class="shell">root@node-2 docker-spark]# docker exec -it hadoop-master /bin/bash[root@hadoop-master ~]# sparkspark-class   spark-shell   spark-sql     spark-submit  sparkR[root@hadoop-master ~]# spark-shell19/12/03 04:51:25 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableSetting default log level to &quot;WARN&quot;.To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).19/12/03 04:51:43 WARN util.Utils: spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.Spark context Web UI available at http://hadoop-master:4040Spark context available as &#39;sc&#39; (master = spark://hadoop-master:7077, app id = app-20191203045141-0000).Spark session available as &#39;spark&#39;.Welcome to      ____              __     / __/__  ___ _____/ /__    _\ \/ _ \/ _ `/ __/  &#39;_/   /___/ .__/\_,_/_/ /_/\_\   version 2.4.4      /_/Using Scala version 2.11.12 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_101)Type in expressions to have them evaluated.Type :help for more information.scala&gt;</code></pre><h2 id="本地部署"><a href="#本地部署" class="headerlink" title="本地部署"></a>本地部署</h2><ul><li><p>想要在自己的笔记本环境使用</p><p>首先笔记本环境下需要有docker环境</p><ul><li>拉取gitlab上面的项目：git clone <a href="mailto:git@161.189.27.8">git@161.189.27.8</a>:chenliang/docker-spark.git</li><li>拉去harbor上面的镜像：docker pull 52.83.79.244:8093/wuhan/spark:v1（也可以自己构建镜像，Dockerfile文件在gitlab项目里边）<br>  前提：机器docker环境登录harbor，账号密码：admin 1qaz!QAZ<br>docker login 52.83.79.244:8093<br>登录报错：Error response from daemon: Get <a href="https://52.83.79.244:8093/v2/" target="_blank" rel="noopener">https://52.83.79.244:8093/v2/</a>: http: server gave HTTP response to HTTPS client<br>解决：参见trouble shooting</li><li>使用相关脚本执行启动、停止、重启集群</li></ul></li></ul><ul><li><p>如何自己的spark程序如何在docker环境下执行  </p><p>启动spark的集群之后，使用docker cp等命令将打好的jar包打进容器内，使用spark脚本执行</p></li></ul>]]></content>
    
    
    <categories>
      
      <category>运维</category>
      
    </categories>
    
    
    <tags>
      
      <tag>部署</tag>
      
      <tag>docker</tag>
      
      <tag>big data</tag>
      
      <tag>证书</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>frp内网穿透使用指南</title>
    <link href="/2020/02/27/frp%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/"/>
    <url>/2020/02/27/frp%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>frp 是一个可用于内网穿透的高性能的反向代理应用，支持 tcp, udp 协议，为 http 和 https 应用协议提供了额外的能力，且尝试性支持了点对点穿透。</p><p>假如部署的k8s集群，需要暴露给外网使用。花生壳儿、向日葵等这些共可以解决，但是这些工具都为收费工具，使用不够灵活，这时frp就派上了用场，使用起来非常简单便捷。</p><p> 👉 <a href="https://github.com/fatedier/frp/blob/master/README_zh.md" target="_blank" rel="noopener">官方github地址</a></p><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><p>frp是标准的c/s架构，配置分为server端还有client端，首先需要在github页release端下载对应的版本</p><h3 id="server端配置"><a href="#server端配置" class="headerlink" title="server端配置"></a>server端配置</h3><p>server端需要部署在有公网ip地址的实例上，这里以centos为例</p><pre><code class="shell"># 下载server端wget https://github.com/fatedier/frp/releases/download/v0.31.1/frp_0.31.1_linux_arm64.tar.gz# 解压tar -zxcf frp_0.31.1_linux_arm64.tar.gzcd frp_0.31.1_linux_arm64</code></pre><p>frps.ini为server端的配置文件</p><pre><code class="shell">[common]# 绑定端口bind_port = 7000vhost_http_port = 8076# web ui访问端口dashboard_port = 7500# web ui用户名及密码dashboard_user = admindashboard_pwd =123456# 最大连接数max_pool_count = 10authentication_timeout = 900[ssh]listen_port = 22auth_token =abcdefg</code></pre><p>后台启动frp server端服务</p><pre><code class="shell">nohup ./frps -c ./frps.ini &amp;</code></pre><p>访问web ui  <a href="http://your" target="_blank" rel="noopener">http://your</a> ip:7500</p><p><img src="https://i.loli.net/2020/01/16/6s4YhmldxGnV15a.png" srcset="/img/loading.gif" alt="微信截图_20200116165729.png"></p><h3 id="client端配置"><a href="#client端配置" class="headerlink" title="client端配置"></a>client端配置</h3><p>假定kubernetes dashboard需要暴露外网访问，这里需要在提供服务的节点，开启frp服务，这里以centos为例</p><pre><code class="shell"># 下载server端wget https://github.com/fatedier/frp/releases/download/v0.31.1/frp_0.31.1_linux_arm64.tar.gz# 解压tar -zxcf frp_0.31.1_linux_arm64.tar.gzcd frp_0.31.1_linux_arm64</code></pre><p>配置客户端</p><pre><code class="ini">[common]server_addr = 161.0.0.0 #填写server端地址server_port = 7000auth_token=abcdefgpool_count=1[wu-k8s]type = tcplocal_ip = 127.0.0.1local_port = 31628  # dashboad访问端口remote_port = 31628</code></pre><p>后台启动frp client端服务</p><pre><code class="shell">nohup ./frpc -c ./frpc.ini &amp;</code></pre><p>启动之后查看frp web ui，确定是否配置成功</p><p><img src="https://i.loli.net/2020/01/16/4dKhBWfYv9VGywP.png" srcset="/img/loading.gif" alt="微信截图_20200116172139.png"></p><p>最后通过访问外网ip及映射端口访问对应服务即可，实现了内网穿透。其他组件需要暴露外网直接修改frpc.ini文件重新启动client端即可</p><p>frp除了实现内网穿透功能，还可以绑定自定义域名等，详细见官方文档。</p>]]></content>
    
    
    <categories>
      
      <category>运维</category>
      
    </categories>
    
    
    <tags>
      
      <tag>部署</tag>
      
      <tag>内网穿透</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ansible使用指南</title>
    <link href="/2020/02/22/Ansible%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/"/>
    <url>/2020/02/22/Ansible%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/</url>
    
    <content type="html"><![CDATA[<h2 id="1-Ansible介绍"><a href="#1-Ansible介绍" class="headerlink" title="1 Ansible介绍"></a>1 Ansible介绍</h2><p>Ansible是一种IT自动化工具。它可以配置系统，部署软件以及协调更高级的IT任务，例如持续部署，滚动更新。Ansible适用于管理企业IT基础设施，从具有少数主机的小规模到数千个实例的企业环境。Ansible也是一种简单的自动化语言，可以完美地描述IT应用程序基础结构。</p><p>具备以下三个特点：</p><ul><li>简单：减少学习成本   </li><li>强大：协调应用程序生命周期 </li><li>无代理：可预测，可靠和安全</li></ul><p>使用文档： <a href="https://docs.ansible.com/" target="_blank" rel="noopener">https://docs.ansible.com/</a> </p><p>安装Ansible：yum install ansible -y</p><p><img src="https://k8s-1252881505.cos.ap-beijing.myqcloud.com/k8s-2/ansible.png" srcset="/img/loading.gif" alt></p><ul><li>Inventory：Ansible管理的主机信息，包括IP地址、SSH端口、账号、密码等</li><li>Modules：任务均有模块完成，也可以自定义模块，例如经常用的脚本。</li><li>Plugins：使用插件增加Ansible核心功能，自身提供了很多插件，也可以自定义插件。例如connection插件，用于连接目标主机。</li><li>Playbooks：“剧本”，模块化定义一系列任务，供外部统一调用。Ansible核心功能。</li></ul><h2 id="1-2-主机清单"><a href="#1-2-主机清单" class="headerlink" title="1.2 主机清单"></a>1.2 主机清单</h2><pre><code>[webservers]alpha.example.orgbeta.example.org192.168.1.100www[001:006].example.com[dbservers]db01.intranet.mydomain.netdb02.intranet.mydomain.net10.25.1.5610.25.1.57db-[99:101]-node.example.com</code></pre><h2 id="1-3-命令行使用"><a href="#1-3-命令行使用" class="headerlink" title="1.3 命令行使用"></a>1.3 命令行使用</h2><p>ad-hoc命令可以输入内容，快速执行某个操作，但不希望留存记录。</p><p>ad-hoc命令是理解Ansible和在学习playbooks之前需要掌握的基础知识。</p><p>一般来说，Ansible的真正能力在于剧本。</p><h3 id="1、连接远程主机认证"><a href="#1、连接远程主机认证" class="headerlink" title="1、连接远程主机认证"></a>1、连接远程主机认证</h3><p>SSH密码认证：</p><pre><code>[webservers]192.168.1.100:22 ansible_ssh_user=root ansible_ssh_pass=’123456’192.168.1.101:22 ansible_ssh_user=root ansible_ssh_pass=’123456’</code></pre><p>SSH密钥对认证：</p><pre><code>[webservers]10.206.240.111:22 ansible_ssh_user=root ansible_ssh_key=/root/.ssh/id_rsa 10.206.240.112:22 ansible_ssh_user=root也可以ansible.cfg在配置文件中指定：[defaults]private_key_file = /root/.ssh/id_rsa  # 默认路径</code></pre><h3 id="2、常用选项"><a href="#2、常用选项" class="headerlink" title="2、常用选项"></a>2、常用选项</h3><table><thead><tr><th>选项</th><th>描述</th></tr></thead><tbody><tr><td>-C, –check</td><td>运行检查，不执行任何操作</td></tr><tr><td>-e EXTRA_VARS,–extra-vars=EXTRA_VARS</td><td>设置附加变量 key=value</td></tr><tr><td>-u REMOTE_USER, –user=REMOTE_USER</td><td>SSH连接用户，默认None</td></tr><tr><td>-k, –ask-pass</td><td>SSH连接用户密码</td></tr><tr><td>-b, –become</td><td>提权，默认root</td></tr><tr><td>-K, –ask-become-pass</td><td>提权密码</td></tr></tbody></table><h3 id="3、命令行使用"><a href="#3、命令行使用" class="headerlink" title="3、命令行使用"></a>3、命令行使用</h3><pre><code class="ansible">ansible all -m pingansible all -m shell -a &quot;ls /root&quot; -u root -k ansible webservers -m copy –a &quot;src=/etc/hosts dest=/tmp/hosts&quot;</code></pre><h2 id="1-4-常用模块"><a href="#1-4-常用模块" class="headerlink" title="1.4 常用模块"></a>1.4 常用模块</h2><p>ansible-doc –l 查看所有模块</p><p>ansible-doc –s copy 查看模块文档</p><p> 模块文档：<a href="https://docs.ansible.com/ansible/latest/modules/modules_by_category.html" target="_blank" rel="noopener">https://docs.ansible.com/ansible/latest/modules/modules_by_category.html</a> </p><h3 id="1、shell"><a href="#1、shell" class="headerlink" title="1、shell"></a>1、shell</h3><p>在目标主机执行shell命令。</p><pre><code>- name: 将命令结果输出到指定文件  shell: somescript.sh &gt;&gt; somelog.txt- name: 切换目录执行命令  shell:    cmd: ls -l | grep log    chdir: somedir/- name: 编写脚本  shell: |      if [ 0 -eq 0 ]; then         echo yes &gt; /tmp/result      else         echo no &gt; /tmp/result      fi  args:    executable: /bin/bash</code></pre><h3 id="2、copy"><a href="#2、copy" class="headerlink" title="2、copy"></a>2、copy</h3><p>将文件复制到远程主机。</p><pre><code>- name: 拷贝文件  copy:    src: /srv/myfiles/foo.conf    dest: /etc/foo.conf    owner: foo    group: foo    mode: u=rw,g=r,o=r    # mode: u+rw,g-wx,o-rwx    # mode: &#39;0644&#39;    backup: yes</code></pre><h3 id="3、file"><a href="#3、file" class="headerlink" title="3、file"></a>3、file</h3><p>管理文件和文件属性。</p><pre><code>- name: 创建目录  file:    path: /etc/some_directory    state: directory    mode: &#39;0755&#39;- name: 删除文件  file:    path: /etc/foo.txt    state: absent- name: 递归删除目录  file:    path: /etc/foo    state: absent</code></pre><p>present，latest：表示安装</p><p>absent：表示卸载</p><h3 id="4、yum"><a href="#4、yum" class="headerlink" title="4、yum"></a>4、yum</h3><p>软件包管理。</p><pre><code>- name: 安装最新版apache  yum:    name: httpd    state: latest- name: 安装列表中所有包  yum:    name:      - nginx      - postgresql      - postgresql-server    state: present- name: 卸载apache包  yum:    name: httpd    state: absent - name: 更新所有包  yum:    name: &#39;*&#39;    state: latest- name: 安装nginx来自远程repo  yum:    name: http://nginx.org/packages/rhel/7/x86_64/RPMS/nginx-1.14.0-1.el7_4.ngx.x86_64.rpm    # name: /usr/local/src/nginx-release-centos-6-0.el6.ngx.noarch.rpm    state: present</code></pre><h3 id="5、service-systemd"><a href="#5、service-systemd" class="headerlink" title="5、service/systemd"></a>5、service/systemd</h3><p>管理服务。</p><pre><code>- name: 服务管理  service:    name: etcd    state: started    #state: stopped    #state: restarted    #state: reloaded- name: 设置开机启动  service:    name: httpd    enabled: yes</code></pre><pre><code>- name: 服务管理    systemd:     name=etcd     state=restarted     enabled=yes     daemon_reload=yes</code></pre><h3 id="6、unarchive"><a href="#6、unarchive" class="headerlink" title="6、unarchive"></a>6、unarchive</h3><pre><code>- name: 解压  unarchive:     src=test.tar.gz     dest=/tmp</code></pre><h3 id="7、debug"><a href="#7、debug" class="headerlink" title="7、debug"></a>7、debug</h3><p>执行过程中打印语句。</p><pre><code>- debug:    msg: System {{ inventory_hostname }} has uuid {{ ansible_product_uuid }}- name: 显示主机已知的所有变量  debug:    var: hostvars[inventory_hostname]    verbosity: 4</code></pre><h2 id="1-5-Playbook"><a href="#1-5-Playbook" class="headerlink" title="1.5 Playbook"></a>1.5 Playbook</h2><p>Playbooks是Ansible的配置，部署和编排语言。他们可以描述您希望在远程机器做哪些事或者描述IT流程中一系列步骤。使用易读的YAML格式组织Playbook文件。</p><p>如果Ansible模块是您工作中的工具，那么Playbook就是您的使用说明书，而您的主机资产文件就是您的原材料。</p><p>与adhoc任务执行模式相比，Playbooks使用ansible是一种完全不同的方式，并且功能特别强大。</p><p><a href="https://docs.ansible.com/ansible/latest/user_guide/playbooks.html" target="_blank" rel="noopener">https://docs.ansible.com/ansible/latest/user_guide/playbooks.html</a></p><pre><code>---- hosts: webservers  vars:    http_port: 80    server_name: www.ctnrs.com  remote_user: root  gather_facts: false  tasks:  - name: 安装nginx最新版    yum: pkg=nginx state=latest  - name: 写入nginx配置文件    template: src=/srv/httpd.j2 dest=/etc/nginx/nginx.conf    notify:    - restart nginx  - name: 确保nginx正在运行    service: name=httpd state=started  handlers:    - name: restart nginx      service: name=nginx state=reloaded</code></pre><h3 id="1、主机和用户"><a href="#1、主机和用户" class="headerlink" title="1、主机和用户"></a>1、主机和用户</h3><pre><code>- hosts: webservers  remote_user: lizhenliang  become: yes  become_user: root</code></pre><p>ansible-playbook nginx.yaml -u lizhenliang -k -b -K </p><h3 id="2、定义变量"><a href="#2、定义变量" class="headerlink" title="2、定义变量"></a>2、定义变量</h3><p>变量是应用于多个主机的便捷方式； 实际在主机执行之前，变量会对每个主机添加，然后在执行中引用。</p><ul><li><p><strong>命令行传递</strong></p><pre><code>-e VAR=VALUE</code></pre></li><li><p><strong>主机变量与组变量</strong></p></li></ul><p>在Inventory中定义变量。</p><pre><code>[webservers]192.168.1.100 ansible_ssh_user=root hostname=web1192.168.1.100 ansible_ssh_user=root hostname=web2[webservers:vars]ansible_ssh_user=root hostname=web1</code></pre><ul><li><strong>单文件存储</strong></li></ul><p>Ansible中的首选做法是不将变量存储在Inventory中。</p><p>除了将变量直接存储在Inventory文件之外，主机和组变量还可以存储在相对于Inventory文件的单个文件中。</p><p>组变量：</p><p>group_vars 存放的是组变量</p><p>group_vars/all.yml  表示所有主机有效，等同于[all:vars]</p><p>grous_vars/etcd.yml 表示etcd组主机有效，等同于[etcd:vars]</p><pre><code># vi /etc/ansible/group_vars/all.ymlwork_dir: /data# vi /etc/ansible/host_vars/webservers.ymlnginx_port: 80</code></pre><ul><li><strong>在Playbook中定义</strong></li></ul><pre><code>- hosts: webservers  vars:    http_port: 80    server_name: www.ctnrs.com</code></pre><ul><li><strong>Register变量</strong></li></ul><pre><code>- shell: /usr/bin/uptime  register: result- debug:    var: result</code></pre><h3 id="3、任务列表"><a href="#3、任务列表" class="headerlink" title="3、任务列表"></a>3、任务列表</h3><p>每个play包含一系列任务。这些任务按照顺序执行，在play中，所有主机都会执行相同的任务指令。play目的是将选择的主机映射到任务。</p><pre><code>  tasks:  - name: 安装nginx最新版    yum: pkg=nginx state=latest</code></pre><h3 id="4、语法检查与调试"><a href="#4、语法检查与调试" class="headerlink" title="4、语法检查与调试"></a>4、语法检查与调试</h3><p>语法检查：ansible-playbook  –check  /path/to/playbook.yaml</p><p>测试运行，不实际操作：ansible-playbook -C /path/to/playbook.yaml</p><p>debug模块在执行期间打印语句，对于调试变量或表达式，而不必停止play。与’when：’指令一起调试更佳。</p><pre><code>  - debug: msg={{group_names}}  - name: 主机名    debug:      msg: &quot;{{inventory_hostname}}&quot;</code></pre><h3 id="5、任务控制"><a href="#5、任务控制" class="headerlink" title="5、任务控制"></a>5、任务控制</h3><p>如果你有一个大的剧本，那么能够在不运行整个剧本的情况下运行特定部分可能会很有用。</p><pre><code>  tasks:  - name: 安装nginx最新版    yum: pkg=nginx state=latest    tags: install  - name: 写入nginx配置文件    template: src=/srv/httpd.j2 dest=/etc/nginx/nginx.conf    tags: config</code></pre><p>使用：</p><pre><code>ansible-playbook example.yml --tags &quot;install&quot;ansible-playbook example.yml --tags &quot;install,config&quot;ansible-playbook example.yml --skip-tags &quot;install&quot;</code></pre><h3 id="6、流程控制"><a href="#6、流程控制" class="headerlink" title="6、流程控制"></a>6、流程控制</h3><p>条件：</p><pre><code>tasks:- name: 只在192.168.1.100运行任务  debug: msg=&quot;{{ansible_default_ipv4.address}}&quot;  when: ansible_default_ipv4.address == &#39;192.168.1.100&#39;</code></pre><p>循环：</p><pre><code>tasks:- name： 批量创建用户  user: name={{ item }} state=present groups=wheel  with_items:     - testuser1     - testuser2</code></pre><pre><code>- name: 解压  copy: src={{ item }} dest=/tmp  with_fileglob:    - &quot;*.txt&quot;</code></pre><p>常用循环语句：</p><table><thead><tr><th>语句</th><th>描述</th></tr></thead><tbody><tr><td>with_items</td><td>标准循环</td></tr><tr><td>with_fileglob</td><td>遍历目录文件</td></tr><tr><td>with_dict</td><td>遍历字典</td></tr></tbody></table><h3 id="7、模板"><a href="#7、模板" class="headerlink" title="7、模板"></a>7、模板</h3><pre><code> vars:    domain: &quot;www.ctnrs.com&quot; tasks:  - name: 写入nginx配置文件    template: src=/srv/server.j2 dest=/etc/nginx/conf.d/server.conf</code></pre><pre><code># server.j2{% set domain_name = domain %}server {   listen 80;   server_name {{ domain_name }};   location / {        root /usr/share/html;   }}</code></pre><p><strong>定义变量</strong>：</p><pre><code>{% set local_ip = inventory_hostname %}</code></pre><p><strong>条件和循环</strong>：</p><pre><code>{% set list=['one', 'two', 'three'] %}{% for i in list %}{% if i == 'two' %}-> two{% elif loop.index == 3 %}-> 3{% else %}{{i}}{% endif %}{% endfor %}</code></pre><p>例如：生成连接etcd字符串</p><pre><code>{% for host in groups['etcd'] %}https://{{ hostvars[host].inventory_hostname }}:2379{% if not loop.last %},{% endif %}{% endfor %} </code></pre><p>里面也可以用ansible的变量。</p><h2 id="1-6-Roles"><a href="#1-6-Roles" class="headerlink" title="1.6 Roles"></a>1.6 Roles</h2><p>Roles是基于已知文件结构自动加载某些变量文件，任务和处理程序的方法。按角色对内容进行分组，适合构建复杂的部署环境。</p><h3 id="1、定义Roles"><a href="#1、定义Roles" class="headerlink" title="1、定义Roles"></a>1、定义Roles</h3><p>Roles目录结构：</p><pre><code>site.ymlwebservers.ymlfooservers.ymlroles/   common/     tasks/     handlers/     files/     templates/     vars/     defaults/     meta/   webservers/     tasks/     defaults/     meta/</code></pre><ul><li><code>tasks</code> -包含角色要执行的任务的主要列表。</li><li><code>handlers</code> -包含处理程序，此角色甚至在此角色之外的任何地方都可以使用这些处理程序。</li><li><code>defaults</code>-角色的默认变量</li><li><code>vars</code>-角色的其他变量</li><li><code>files</code> -包含可以通过此角色部署的文件。</li><li><code>templates</code> -包含可以通过此角色部署的模板。</li><li><code>meta</code>-为此角色定义一些元数据。请参阅下面的更多细节。</li></ul><p>通常的做法是从<code>tasks/main.yml</code>文件中包含特定于平台的任务：</p><pre><code># roles/webservers/tasks/main.yml- name: added in 2.4, previously you used &#39;include&#39;  import_tasks: redhat.yml  when: ansible_facts[&#39;os_family&#39;]|lower == &#39;redhat&#39;- import_tasks: debian.yml  when: ansible_facts[&#39;os_family&#39;]|lower == &#39;debian&#39;# roles/webservers/tasks/redhat.yml- yum:    name: &quot;httpd&quot;    state: present# roles/webservers/tasks/debian.yml- apt:    name: &quot;apache2&quot;    state: present</code></pre><h3 id="2、使用角色"><a href="#2、使用角色" class="headerlink" title="2、使用角色"></a>2、使用角色</h3><pre><code># site.yml- hosts: webservers  roles:    - common    - webservers定义多个：- name: 0  gather_facts: false  hosts: all   roles:    - common- name: 1  gather_facts: false  hosts: all   roles:    - webservers</code></pre><h3 id="3、角色控制"><a href="#3、角色控制" class="headerlink" title="3、角色控制"></a>3、角色控制</h3><pre><code>- name: 0.系统初始化  gather_facts: false  hosts: all   roles:    - common  tags: common </code></pre>]]></content>
    
    
    <categories>
      
      <category>运维</category>
      
    </categories>
    
    
    <tags>
      
      <tag>部署</tag>
      
      <tag>自动化</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Jenkins使用指南</title>
    <link href="/2020/01/09/Jenkins%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/"/>
    <url>/2020/01/09/Jenkins%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/</url>
    
    <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Jenkins是一个广泛用于持续构建的可视化web工具，持续构建说得更直白点，就是各种项目的”自动化”编译、打包、分发部署。jenkins可以很好的支持各种语言（比如：java, c#, php等）的项目构建，也完全兼容ant、maven、gradle等多种第三方构建工具，同时跟svn、git能无缝集成，也支持直接与知名源代码托管网站，比如github、bitbucket直接集成。<br>简单点说，Jenkins其实就是大的框架集，可以整个任何你想整合的内容，实现公司的整个持续集成体系！</p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>基于docker安装：</p><pre><code class="shell">docker run -it \  --name jenkins \  --restart always \  --user root \  -p 10002:8080 \  -p 50000:50000 \  -v /data/jenkins_home:/var/jenkins_home \  -v /var/run/docker.sock:/var/run/docker.sock \  -v /opt/jdk1.8.0_25:/opt/jdk1.8.0_25 \  -v /bin/docker:/bin/docker \  -v /data/repository:/data/repository \  -v $(which docker):/usr/bin/docker \  jenkins/jenkins:lts</code></pre><p>注意：需要使用jenkins/jenkins:lts镜像，jenkins:latest镜像官方已不提供支持，版本过低</p><p>其中将外部docker映射到了内部docker，这样在jenkins容器内部也可以使用docker命令了</p><p>注意启动之后会有个随机的密码：<br>例：<br>ff9c1128af2840d990798418bd3c92f2</p><p>如果采用以-it的形式启动，可以在命令窗口中看到。</p><p><a href="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/password.png" target="_blank" rel="noopener"><img src="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/scaled-840-0/password.png" srcset="/img/loading.gif" alt="password.png"></a></p><p>当然也可以进入容器，在/var/jenkins_home/secrets/initialAdminPassword中找到。</p><p>注意！映射在容器中的/var/jenkins_home 目录到具有名字 jenkins-data 的volume。 如果这个卷不存在，那么这个 docker run 命令会自动为你创建卷。 如果您希望每次重新启动Jenkins（通过此 docker run … 命令）时保持Jenkins状态，则此选项是必需的，jenkins数据会在该卷进行持久化 。 否则，那么在每次重新启动后，Jenkins将有效地重置为新的实例。</p><p>（可选 /var/run/docker.sock 表示Docker守护程序通过其监听的基于Unix的套接字。 该映射允许 jenkinsci/blueocean 容器与Docker守护进程通信， 如果 jenkinsci/blueocean 容器需要实例化其他Docker容器，则该守护进程是必需的。 如果运行声明式管道，其语法包含agent部分用 docker.</p><p>进入 ip:10002 jenkins安装界面</p><p><a href="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/install.png" target="_blank" rel="noopener"><img src="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/scaled-840-0/install.png" srcset="/img/loading.gif" alt="install.png"></a></p><p>安装对应插件</p><h2 id="Demo"><a href="#Demo" class="headerlink" title="Demo"></a>Demo</h2><pre><code>本Demo实现的场景是push到项目master分支，自动触发打包发布到docker镜像仓库harbor，然后在拉取镜像在docker中运行</code></pre><p>项目Demo地址：<a href="http://161.189.27.8:8090/chenliang/jenkinsdemo" target="_blank" rel="noopener">http://161.189.27.8:8090/chenliang/jenkinsdemo</a></p><p>jenkins项目地址：<a href="http://52.83.79.244:10002/job/jenkins-demo" target="_blank" rel="noopener">http://52.83.79.244:10002/job/jenkins-demo</a></p><p>大家可以拉取项目Demo代码，修改代码push提交master，自动触发打包执行，在jenkins的console output查看执行信息</p><h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><ul><li><p>step1 gitlab新建项目 比如jenkins-demo</p></li><li><p>step2 </p><p>jenkins添加gitlab账户及密码</p><p>凭据—&gt;系统—&gt;全局凭据—&gt;add credentials</p><p><img src="/.com//assets%5Cpass.png" srcset="/img/loading.gif" alt="1572856908321"></p></li><li><p>step3  jenkins新建项目，选择maven项目—source code managment</p><p>填写项目的地址，选择step生成的credentials，选择代码分支</p></li></ul><p><a href="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/maven.png" target="_blank" rel="noopener"><img src="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/scaled-840-0/maven.png" srcset="/img/loading.gif" alt="maven.png"></a></p><p><a href="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/demo02.png" target="_blank" rel="noopener"><img src="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/scaled-840-0/demo02.png" srcset="/img/loading.gif" alt="demo02.png"></a></p><ul><li><p>step4  选择Build Trigger—&gt;勾选gitlab触发选项—&gt;点击generate生成scretkey并记住—&gt;复制gitlab webhook的url</p><p>url及secretkey在gitlab设置中需要用</p></li></ul><p><a href="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/setting.png" target="_blank" rel="noopener"><img src="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/scaled-840-0/setting.png" srcset="/img/loading.gif" alt="setting.png"></a></p><ul><li>step5 回到gitlab项目的setting的integration页面中，填写step4中的url及secretkey，取消勾选ssl</li></ul><p><a href="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/step5.png" target="_blank" rel="noopener"><img src="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/scaled-840-0/step5.png" srcset="/img/loading.gif" alt="step5.png"></a></p><ul><li>step6 build中填写构建maven命令及构建后需要执行的程序，本demo执行的是打包然后java命令执行jar包</li></ul><p><a href="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/build.png" target="_blank" rel="noopener"><img src="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/scaled-840-0/build.png" srcset="/img/loading.gif" alt="build.png"></a></p><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>以gitlab项目jenkinsdemo为例：<a href="http://161.189.27.8:8090/chenliang/jenkinsdemo" target="_blank" rel="noopener">http://161.189.27.8:8090/chenliang/jenkinsdemo</a> 当修改代码push到master中的时候会自动出发打包及执行jar包，在jenkins项目的console output中查看打包及执行日志</p><p><a href="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/output.png" target="_blank" rel="noopener"><img src="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/scaled-840-0/output.png" srcset="/img/loading.gif" alt="output.png"></a></p><h2 id="trouble-shooting"><a href="#trouble-shooting" class="headerlink" title="trouble shooting"></a>trouble shooting</h2><h3 id="安装插件报错"><a href="#安装插件报错" class="headerlink" title="安装插件报错"></a>安装插件报错</h3><p><a href="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/ceverror01.png" target="_blank" rel="noopener"><img src="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/scaled-840-0/ceverror01.png" srcset="/img/loading.gif" alt="ceverror01.png"></a></p><p>更改jenkins源—&gt;进入系统管理—&gt;管理插件—&gt;高级 </p><p>将</p><pre><code>http://updates.jenkins-ci.org/update-center.json</code></pre><p>更换为</p><pre><code>http://mirror.esuni.jp/jenkins/updates/update-center.json</code></pre><p>保存即可</p><p><a href="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/demo01.png" target="_blank" rel="noopener"><img src="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/scaled-840-0/demo01.png" srcset="/img/loading.gif" alt="demo01.png"></a></p><h3 id="jenkins内部执行docker命令报错"><a href="#jenkins内部执行docker命令报错" class="headerlink" title="jenkins内部执行docker命令报错"></a>jenkins内部执行docker命令报错</h3><p>docker内部执行docker报的错误信息：docker: error while loading shared libraries: libltdl.so.7: cannot open shared object file: No such file or directory</p><p>第一次使用的docker部署jenkins的时候，出现了两个问题：</p><p>1、因为用户权限问题挂载/home/jenkins/data到/var/jenkins_home挂载不了。后面通过修改data目录的所属用户可以解决，即在容器下查询用户id（1000）,然后把data改成同样的用户id</p><p>2、即便挂载docker命名和docker.sock,也修改了相应的权限，仍存在libltdl7没有权限读取。当然好像也不影响使用，只是在容器里面执行docker info的时候，会报无法读取libltdl.so.7的信息。</p><p>docker: error while loading shared libraries: /usr/lib/x86_64-linux-gnu/libltdl.so.7: cannot read file data: Error 21</p><p>于是查找资料在jenkins/jenkins基础上再建一个Jenkins镜像。</p><p>编辑Dockerfile</p><pre><code class="shell">FROM jenkins/jenkins:ltsUSER root#清除了基础镜像设置的源，切换成阿里云的jessie源RUN echo &#39;&#39; &gt; /etc/apt/sources.list.d/jessie-backports.list \  &amp;&amp; echo &quot;deb http://mirrors.aliyun.com/debian jessie main contrib non-free&quot; &gt; /etc/apt/sources.list \  &amp;&amp; echo &quot;deb http://mirrors.aliyun.com/debian jessie-updates main contrib non-free&quot; &gt;&gt; /etc/apt/sources.list \  &amp;&amp; echo &quot;deb http://mirrors.aliyun.com/debian-security jessie/updates main contrib non-free&quot; &gt;&gt; /etc/apt/sources.list#更新源并安装缺少的包RUN apt-get update &amp;&amp; apt-get install -y libltdl7ARG dockerGid=999RUN echo &quot;docker:x:${dockerGid}:jenkins&quot; &gt;&gt; /etc/group USER jenkins# 安装 docker-compose  --- 挂载宿主机上的就可以了# RUN curl -L https://github.com/docker/compose/releases/download/1.17.1/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose# RUN chmod +x /usr/local/bin/docker-compose</code></pre><p>build镜像</p><pre><code class="shell">docker build . -t myjenkins:v1</code></pre>]]></content>
    
    
    <categories>
      
      <category>CI/CD</category>
      
    </categories>
    
    
    <tags>
      
      <tag>docker</tag>
      
      <tag>jenkins</tag>
      
      <tag>CI/CD</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Apache Airflow使用指南</title>
    <link href="/2020/01/09/Apache-Airflow%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/"/>
    <url>/2020/01/09/Apache-Airflow%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>airflow 是 apache下孵化项目，是纯 Python 编写的一款非常优雅的开源调度平台。github 上有 8971 个星，是非常受欢迎的调度工具。airflow 使用 DAG (有向无环图) 来定义工作流，配置作业依赖关系非常方便，豪不夸张地说：方便程度简直甩其他任务调度工具一条街。<br> airflow 有着以下天然优势：</p><ol><li>灵活易用，airflow 本身是 Python 编写的，且工作流的定义也是 Python 编写，有了 Python 胶水的特性，没有什么任务是调度不了的，有了开源的代码，没有什么问题是无法解决的，你完全可以修改源码来满足个性化的需求，而且更重要的是代码都是 <strong>–human-readable</strong> 。</li><li>功能强大，自带的 Operators 都有15+，也就是说本身已经支持 15+ 不同类型的作业，而且还是可自定义 Operators，什么 shell 脚本，python，mysql，oracle，hive等等，无论不传统数据库平台还是大数据平台，统统不在话下，对官方提供的不满足，完全可以自己编写 Operators。</li><li>优雅，作业的定义很简单明了, 基于 jinja 模板引擎很容易做到脚本命令参数化，web 界面更是也非常 <strong>–human-readable</strong> 。</li><li>极易扩展，提供各种基类供扩展, 还有多种执行器可供选择，其中 CeleryExcutor 使用了消息队列来编排多个工作节点(worker), 可分布式部署多个 worker ，airflow 可以做到无限扩展。</li><li>丰富的命令工具，你甚至都不用打开浏览器，直接在终端敲命令就能完成测试，部署，运行，清理，重跑，追数等任务。</li></ol><p>airflow 是免费的，可以将一些常做的巡检任务，定时脚本（如 crontab ），ETL处理，监控等任务放在 airflow 上集中管理，甚至都不用再写监控脚本，作业出错会自动发送日志到指定人员邮箱，低成本高效率地解决生产问题。</p><h2 id="组成部分"><a href="#组成部分" class="headerlink" title="组成部分"></a>组成部分</h2><p>从一个使用者的角度来看，调度工作都有以下功能：</p><ol><li>系统配置（$AIRFLOW_HOME/airflow.cfg）</li><li>作业管理（$AIRFLOW_HOME/dags/xxxx.py）</li><li>运行监控（webserver)</li><li>报警（邮件或短信）</li><li>日志查看（webserver 或 $AIRFLOW_HOME/logs/***)</li><li>跑批耗时分析（webserver)</li><li>后台调度服务（scheduler)</li></ol><p>除了短信需要自己实现，其他功能 airflow 都有，而且在 airflow 的 webserver 上我们可以直接配置数据库连接来写 sql 查询，做更加灵活的统计分析。</p><h2 id="重要概念"><a href="#重要概念" class="headerlink" title="重要概念"></a>重要概念</h2><h3 id="DAG"><a href="#DAG" class="headerlink" title="DAG"></a>DAG</h3><p>Linux 的 crontab 和 windows 的任务计划，他们可以配置定时任务或间隔任务，但不能配置作业之前的依赖关系。airflow 中 DAG 就是管理作业依赖关系的。DAG 的英文 directed acyclic graphs 即有向无环图，下图 1 便是一个简单的 DAG<br><img src="https://upload-images.jianshu.io/upload_images/12989993-96bcaf5716827e99?imageMogr2/auto-orient/strip%7CimageView2/2/w/657/format/webp" srcset="/img/loading.gif" alt="img"></p><p>在 airflow 中这种 DAG 是通过编写 Python 代码来实现的，DAG 的编写非常简单，官方提供了很多的例子，在安装完成后，启动 webserver 即可看到 DAG 样例的源码（其实定义了 DAG 对象的 python 程序），稍做修改即可成为自己的 DAG 。上图 1 中 DAG 中的依赖关系通过下述三行代码即可完成：<br><img src="https://upload-images.jianshu.io/upload_images/12989993-755318fef8cd6dd4?imageMogr2/auto-orient/strip%7CimageView2/2/w/766/format/webp" srcset="/img/loading.gif" alt="img"></p><h3 id="Operators-操作符"><a href="#Operators-操作符" class="headerlink" title="Operators-操作符"></a>Operators-操作符</h3><p>DAG 定义一个作业流，Operators 则定义了实际需要执行的作业。airflow 提供了许多 Operators 来指定我们需要执行的作业：</p><ol><li>BashOperator - 执行 bash 命令或脚本。</li><li>SSHOperator - 执行远程 bash 命令或脚本（原理同 paramiko 模块）。</li><li>PythonOperator - 执行 Python 函数。</li><li>EmailOperator - 发送 Email。</li><li>HTTPOperator - 发送一个 HTTP 请求。</li><li>MySqlOperator, SqliteOperator, PostgresOperator, MsSqlOperator, OracleOperator, JdbcOperator, 等. - 执行 SQL 任务。</li><li>DockerOperator, HiveOperator, S3FileTransferOperator, PrestoToMysqlOperator, SlackOperator 等。</li></ol><p>除了以上这些 Operators 还可以方便的自定义 Operators 满足个性化的任务需求。<br>后续会介绍如何使用这些 Operators。</p><h3 id="Timezone-时区"><a href="#Timezone-时区" class="headerlink" title="Timezone-时区"></a>Timezone-时区</h3><p>airflow 1.9 之前的版本使用本地时区来定义任务开始日期，scheduler_interval 中 crontab 表达式中的定时也是依据本地时区为准，但 airflow 1.9 及后续新版本将默认使用 UTC 时区来确保 airflow 调度的独立性，以避免不同机器使用不同时区导致运行错乱。如果调度的任务集中在一个时区上，或不同机器，但使用同一时区时，需要对任务的开始时间及 cron 表达式进行时区转换，或直接使用本地时区。目前 1.9 的稳定版本还不支持时区配置，后续版本会加入时区配置，以满足使用本地时区的需求。</p><h3 id="Webserver-Web服务器"><a href="#Webserver-Web服务器" class="headerlink" title="Webserver-Web服务器"></a>Webserver-Web服务器</h3><p>webserver 是 airflow 的界面展示，可显示 DAG 视图，控制作业的启停，清除作业状态重跑，数据统计，查看日志，管理用户及数据连接等。不运行 webserver 并不影响 airflow 作业的调度。</p><h3 id="Schduler-调度器"><a href="#Schduler-调度器" class="headerlink" title="Schduler-调度器"></a>Schduler-调度器</h3><p>调度器 schduler 负责读取 DAG 文件，计算其调度时间，当满足触发条件时则开启一个执行器的实例来运行相应的作业，必须持续运行，不运行则作业不会跑批。</p><h3 id="Worker-工作节点"><a href="#Worker-工作节点" class="headerlink" title="Worker-工作节点"></a>Worker-工作节点</h3><p>当执行器为 CeleryExecutor 时，需要开启一个 worker。</p><h3 id="Executor-执行器"><a href="#Executor-执行器" class="headerlink" title="Executor-执行器"></a>Executor-执行器</h3><p>执行器有 SequentialExecutor, LocalExecutor, CeleryExecutor</p><ol><li>SequentialExecutor 为顺序执行器，默认使用 sqlite 作为知识库，由于 sqlite 数据库的原因，任务之间不支持并发执行，常用于测试环境，无需要额外配置。</li><li>LocalExecutor 为本执行器，不能使用 sqlite 作为知识库，可以使用 mysql,postgress,db2,oracle 等各种主流数据库，任务之间支持并发执行，常用于生产环境，需要配置数据库连接 url。</li><li>CeleryExecutor 为 Celery 执行器，需要安装 Celery ,Celery 是基于消息队列的分布式异步任务调度工具。需要额外启动工作节点-worker。使用 CeleryExecutor 可将作业运行在远程节点上。</li></ol><hr><h2 id="基于Docker搭建"><a href="#基于Docker搭建" class="headerlink" title="基于Docker搭建"></a>基于Docker搭建</h2><p>基于第三方docker镜像进行安装，github-repo：<a href="https://github.com/puckel/docker-airflow" target="_blank" rel="noopener">https://github.com/puckel/docker-airflow</a></p><p>前提：机器已经装上docker及docker-compose命令，本页面基于Docker搭建，部署airflow的CeleryExecutor模式</p><ul><li><p>Step1 拉取代码</p><pre><code class="shell">git clone https://github.com/puckel/docker-airflow.git</code></pre></li><li><p>Step2 修改 docker-compose-CeleryExecutor.yml及Dockerfile</p><pre><code>cd ~/docker-airflowvim docker-compose-CeleryExecutor.yml</code></pre><p>默认的compose是抓原来版本的镜像包，改成latest标签，这样才会使用用Dockerfile构建的镜像</p><p>修改web暴露端口，8080可能被占用，下图修改为8096端口</p></li></ul><ul><li><p>Step3 Airflow Config设置</p><pre><code>cd configvim airflow.cfg</code></pre></li></ul><ul><li><p>构建镜像</p><pre><code class="shell">docker build --rm -t puckel/docker-airflow:latest .docker-compose -f docker-compose-CeleryExecutor.yml up -d</code></pre></li></ul><p>当看到下图，就代表已经启动，可通过该端口进行访问</p>]]></content>
    
    
    <categories>
      
      <category>etl</category>
      
    </categories>
    
    
    <tags>
      
      <tag>apache airflow</tag>
      
      <tag>docker</tag>
      
      <tag>etl</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Apache Nifi使用指南</title>
    <link href="/2020/01/09/Apache-Nifi%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/"/>
    <url>/2020/01/09/Apache-Nifi%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/</url>
    
    <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Apache NiFi是什么？NiFi官网给出如下解释：“一个易用、强大、可靠的数据处理与分发系统”。通俗的来说，即Apache NiFi 是一个易于使用、功能强大而且可靠的数据处理和分发系统，其为数据流设计，它支持高度可配置的指示图的数据路由、转换和系统中介逻辑。</p><h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><p>单节点架构</p><p><img src="https://nifi.apache.org/docs/nifi-docs/html/images/zero-master-node.png" srcset="/img/loading.gif" alt="NiFi Architecture Diagram"></p><p>集群架构图</p><p><img src="https://nifi.apache.org/docs/nifi-docs/html/images/zero-master-cluster.png" srcset="/img/loading.gif" alt="NiFi Cluster Architecture Diagram"></p><p><strong>web-sever</strong></p><p>其目的在于提供基于HTTP的命令和控制API。</p><p><strong>Flow Controller</strong></p><p>这是操作的核心，以Processor为处理单元，提供了用于运行的扩展线程，并管理扩展接收资源时的调度。</p><p><strong>Extensions</strong></p><p>在其他文档中描述了各种类型的NiFi扩展，Extensions的关键在于扩展在JVM中操作和执行。</p><p><strong>FlowFile Repository</strong></p><p>FlowFile库的作用是NiFi跟踪记录当前在流中处于活动状态的给定流文件的状态，其实现是可插拔的，默认的方法是位于指定磁盘分区上的一个持久的写前日志。FlowFile库的作用是NiFi跟踪记录当前在流中处于活动状态的给定流文件的状态，其实现是可插拔的，默认的方法是位于指定磁盘分区上的一个持久的写前日志。</p><p><strong>Content Repository</strong></p><p>Content库的作用是给定流文件的实际内容字节所在的位置，其实现也是可插拔的。默认的方法是一种相对简单的机制，即在文件系统中存储数据块。</p><p><strong>Provenance Repository</strong></p><p>Provenance库是所有源数据存储的地方，支持可插拔。默认实现是使用一个或多个物理磁盘卷，在每个位置事件数据都是索引和可搜索的。</p><h2 id="docker部署"><a href="#docker部署" class="headerlink" title="docker部署"></a>docker部署</h2><p>nifi需要通过集群实现多租户，在standalone模式下，可以通过docker启动多个实例来实现多用户通过不同端口访问各自的nifi</p><pre><code class="shell">docker run --name nifi01 \  --restart=always \  -p 8090:8080 \  -p 10000:10000 \  -v /data/nifi:/data/nifi/ \  -d \  apache/nifi:latest  docker run --name nifi02 \  --restart=always \  -p 8094:8080 \  -p 10001:10000 \  -v /data/nifi02:/data/nifi02/ \  -d \  apache/nifi:latest</code></pre><p>启动nifi，停止nifi</p><pre><code class="shell">docker start nifidocker stop nifi</code></pre><h2 id="NiFi-Processor"><a href="#NiFi-Processor" class="headerlink" title="NiFi Processor"></a>NiFi Processor</h2><p>Flow Controller是NiFi的核心，Flow Controller扮演者文件交流的处理器角色，维持着多个处理器的连接并管理各个Processer，Processor则是实际处理单元。</p><p><img src="/.com//processor.png" srcset="/img/loading.gif" alt="1572501106736"></p><p>Processor包含各种类型的组件，如amazon、attributes、hadoop等，可通过前缀进行轻易辨识，如Get、Fetch开头代表获取，如getFile、getFTP、FetchHDFS，execute代表执行，如ExecuteSQL、ExecuteProcess、ExecuteFlumeSink等均可较容易知其简单用途。右边选择hadoop则会显示所有hadoop相关的processor，如图所示</p><p><img src="/.com//hadoop.png" srcset="/img/loading.gif" alt="1572501279158"></p><p>与hadoop相关的processor有读写HDFS文件，写Hbase，读写parquet文件等</p><h2 id="Nifi实战Demo"><a href="#Nifi实战Demo" class="headerlink" title="Nifi实战Demo"></a>Nifi实战Demo</h2><h3 id="通过Nifi实现把指定文件夹中的文件移动到另一个文件夹"><a href="#通过Nifi实现把指定文件夹中的文件移动到另一个文件夹" class="headerlink" title="通过Nifi实现把指定文件夹中的文件移动到另一个文件夹"></a>通过Nifi实现把指定文件夹中的文件移动到另一个文件夹</h3><ul><li><p>step 1选取processor</p><p>选取getfile及putfile的process，并连接</p><p><a href="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/geifile.png" target="_blank" rel="noopener"><img src="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/scaled-840-0/geifile.png" srcset="/img/loading.gif" alt="geifile.png"></a></p></li></ul><p><a href="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/flow.png" target="_blank" rel="noopener"><img src="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/scaled-840-0/flow.png" srcset="/img/loading.gif" alt="flow.png"></a></p><ul><li><p>step 2 填写getfile及putfile相关属性</p><p>双击getfile或者putfile的processor，property中填写源数据文件夹及目标文件夹，其他参数按需进行配置，此demo按照默认参数填写，对于putfile processor中的setting栏，设置勾选自动终止策略，当putfile失败或者成功的时候就停止此processor</p><p><a href="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/properties.png" target="_blank" rel="noopener"><img src="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/scaled-840-0/properties.png" srcset="/img/loading.gif" alt="properties.png"></a></p></li></ul><p><a href="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/putfile.png" target="_blank" rel="noopener"><img src="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/scaled-840-0/putfile.png" srcset="/img/loading.gif" alt="putfile.png"></a></p><p><a href="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/putfile_config.png" target="_blank" rel="noopener"><img src="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/scaled-840-0/putfile_config.png" srcset="/img/loading.gif" alt="putfile_config.png"></a></p><ul><li><p>step 3 启动</p><p>空白处右键选择start，状态栏显示绿色代表processor在执行，假如/data/nifi/input有文件此时开启工作流后/data/nifi/input对应也会有该文件</p></li></ul><p><a href="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/start.png" target="_blank" rel="noopener"><img src="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/scaled-840-0/start.png" srcset="/img/loading.gif" alt="start.png"></a></p><h3 id="数据库表和表同步"><a href="#数据库表和表同步" class="headerlink" title="数据库表和表同步"></a>数据库表和表同步</h3><p>表到表的同步,NIFI默认sql查询出来的数据为Avro格式,所以需要先将Avro格式转化为json格式,再将json转换为sql语句,最后使用PUTSQL处理器将数据存入数据库。需要用到ExecuteorSQL、ConvertAvroToJSON、ConvertJSONToSQL、PUTSQL四种处理器。<br><a href="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/table2table.png" target="_blank" rel="noopener"><img src="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/scaled-840-0/table2table.png" srcset="/img/loading.gif" alt="table2table.png"></a> </p><ul><li>step 1 使用ExecuteSQL配置数据源<br>从Components ToolBar上将processor拖拽到画布上，选择ExecuteSQL处理器，右键或双击该处理器编辑properties。该处理器原生提供三种数据库连接池，提供了大多数数据库驱动，另外还可以自定义连接池的名称。<br><a href="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/ExecuteSQL.png" target="_blank" rel="noopener"><img src="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/scaled-840-0/ExecuteSQL.png" srcset="/img/loading.gif" alt="ExecuteSQL.png"></a><ul><li>Database Connection Pooling Service 提供数据库连接池服务  </li><li>postgresql驱动程序<a href="https://jdbc.postgresql.org/download.html" target="_blank" rel="noopener">下载</a><br><a href="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/driverConfig.png" target="_blank" rel="noopener"><img src="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/scaled-840-0/driverConfig.png" srcset="/img/loading.gif" alt="driverConfig.png"></a></li><li>驱动配置完成后点击⚡,使能后查看是否报错  </li></ul></li><li>step 2 使用ConvertAvroToJSON将Avro格式转换为json格式<br><a href="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/ConvertAvroToJSON.png" target="_blank" rel="noopener"><img src="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/scaled-840-0/ConvertAvroToJSON.png" srcset="/img/loading.gif" alt="ConvertAvroToJSON.png"></a></li><li>step 3 使用ConvertJSONToSQL将json数据转换为SQL语句<br><a href="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/ConvertJSONToSQL.png" target="_blank" rel="noopener"><img src="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/scaled-840-0/ConvertJSONToSQL.png" srcset="/img/loading.gif" alt="ConvertJSONToSQL.png"></a> </li><li>step 4 使用PUTSQL将数据存入到数据库，配置完成后数据流启动后可以看到数据流动过程，再到数据库中验证结果。<br><a href="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/PUTSQL.png" target="_blank" rel="noopener"><img src="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/scaled-840-0/PUTSQL.png" srcset="/img/loading.gif" alt="PUTSQL.png"></a></li></ul><p><strong>Tips:基本案例跑通后可以在Operate栏createTemplate,可以在web页面直接导出xml格式的数据处理流程重复使用减少配置时间。</strong>  </p><h2 id="Trouble-Shooting"><a href="#Trouble-Shooting" class="headerlink" title="Trouble Shooting"></a>Trouble Shooting</h2><ul><li>原始文件夹或者目标文件夹没有权限，process右上角会显示报错信息，此时应该将文件夹设置对应权限，将拥有者改为nifi用户，此命令应该用root权限执行，若nifi在docker中，应该到使用root用户登录到对应container执行</li></ul><pre><code>linux版本sudo chown -R nifi:nifi /data/nifi/inputsudo chown -R nifi:nifi /data/nifi/outputdocker版本docker exec -it -u root nifi /bin/bashchown -R nifi:nifi /data/nifi/inputchown -R nifi:nifi /data/nifi/output</code></pre><p>报错信息：</p><p><a href="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/error.png" target="_blank" rel="noopener"><img src="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/scaled-840-0/error.png" srcset="/img/loading.gif" alt="error.png"></a></p><ul><li>putfile或者getfile的properties填写错误的时候，对应processor上面会显示感叹号，按照提示修改即可，如getfile中源文件夹为/data/nifi/input01，系统中没有此文件夹，则会显示对应提示信息</li></ul><p><a href="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/error01.png" target="_blank" rel="noopener"><img src="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/scaled-840-0/error01.png" srcset="/img/loading.gif" alt="error01.png"></a></p>]]></content>
    
    
    <categories>
      
      <category>etl</category>
      
    </categories>
    
    
    <tags>
      
      <tag>docker</tag>
      
      <tag>etl</tag>
      
      <tag>apache nifi</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Spark run on K8s</title>
    <link href="/2020/01/09/Spark-run-on-K8s/"/>
    <url>/2020/01/09/Spark-run-on-K8s/</url>
    
    <content type="html"><![CDATA[<h2 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h2><p>首先查看了下spark的官方文档，了解了spark怎么在k8s上面跑的，实际上不需要搭建spark集群，提交作业到k8s的api server即可。看似简单但是还是不知道怎么动手实践。于是youtube上面搜了下相关视频，按照视频很快就实践了一把spark run on k8s，具体步骤如下：</p><ul><li>step 1 到k8-master机器下载二进制spark最新二进制安装包，并解压</li></ul><pre><code class="shell">cd /optwget http://mirrors.tuna.tsinghua.edu.cn/apache/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgztar -zxvf http://mirrors.tuna.tsinghua.edu.cn/apache/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz</code></pre><ul><li>step 2 制作spark的docker镜像</li></ul><pre><code class="shell">cd spark-2.4.4-bin-hadoop2.7./bin/docker-image-tool.sh -r chenlianguu -t v2.4.4 build  # 制作spark进行./bin/docker-image-tool.sh -r chenlianguu -t v2.4.4 push  #将spark镜像推送到docker hubdocker images[root@k8s-master opt]# docker imagesREPOSITORY                                                        TAG                 IMAGE ID            CREATED             SIZEchenliang/spark-r                                                 v2.4.4              6479a523e3f7        20 hours ago        759MB</code></pre><ul><li>step 3 提交作业到k8s</li></ul><p>在提交spark的作业的机器上，把api server的proxy打开</p><pre><code class="shell">kubectl proxy</code></pre><pre><code class="shell">./bin/spark-submit \--master k8s://http://127.0.0.1:8001 \--name spark-pi \--deploy-mode cluster \--class org.apache.spark.examples.SparkPi \--conf spark.executor.instances=3 \--conf spark.kubernetes.container.image=chenlianguu/spark-r:v2.4.4 \/opt/spark-2.4.4-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.4.4.jar</code></pre><h2 id="运行说明"><a href="#运行说明" class="headerlink" title="运行说明"></a>运行说明</h2><p><a href="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/k8s-cluster-mode.png" target="_blank" rel="noopener"><img src="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/scaled-840-0/k8s-cluster-mode.png" srcset="/img/loading.gif" alt="k8s-cluster-mode.png"></a><br>集群模式下，通过spark-submit提交程序到k8s集群，具体一下步骤：  </p><ul><li>通过k8s创建spark driver端的pod</li><li>driver端在k8s其他节点创建executor端的pod并保持通信，executor具体执行代码，这里设计到权限问题，假如没有对应的权限创建pods，执行spark会报错，具体见trouble  shooting第二点</li><li>当程序跑完了，executor端pod会终止并清理，driver端的pod会保持complete状态并持久化log信息，最后会由k8s api server进行driver端pod的垃圾回收工作</li></ul><h2 id="trouble-shooting"><a href="#trouble-shooting" class="headerlink" title="trouble shooting"></a>trouble shooting</h2><p>跑spark on k8s的pi example碰到的一些坑及解决方案</p><ul><li>找不到jar问题<br><img src="https://i.loli.net/2020/01/09/xzqUmBIuCdP6HlW.jpg" srcset="/img/loading.gif" alt="1"><br><img src="https://i.loli.net/2020/01/09/U6uIFOrgcjQAkn4.jpg" srcset="/img/loading.gif" alt="2"><br>之前提交的脚本是这样的  </li></ul><pre><code class="shell">./bin/spark-submit \--master k8s://http://127.0.0.1:8001 \--name spark-pi \--deploy-mode cluster \--class org.apache.spark.examples.SparkPi \--conf spark.executor.instances=3 \--conf spark.kubernetes.container.image=chenlianguu/spark-r:v2.4.4 \/opt/spark-2.4.4-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.4.4.jar</code></pre><p>其中jar包制定的地址是local，路径填写应该加上协议local:///opt/spark-2.4.4-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.4.4.jar，加上之后还是报同样的错，google后发现jar包实际上是在jar包在docker镜像里面的地址，因此改为local:///opt/spark/examples/jars/spark-examples_2.11-2.4.4.jar   </p><ul><li>jar问题解决了，又开始报错，报错信息如下<br><img src="https://i.loli.net/2020/01/09/BgsviowCp5EzARG.jpg" srcset="/img/loading.gif" alt="3"><br>第一感觉就是权限问题，网上找到了解决方案，ref：<a href="https://github.com/GoogleCloudPlatform/continuous-deployment-on-kubernetes/issues/113" target="_blank" rel="noopener">解决办法</a>，按照这个思路，在k8s-master节点，输入   <pre><code>kubectl create clusterrolebinding default --clusterrole cluster-admin --serviceaccount=default:default</code></pre></li></ul><p>然后继续执行spark-submit脚本，可以顺利启动driver端，但是有2个executor端还是报错，说是资源不够，有一个executor端执行成功，整个job还是顺利的跑下来了，查看driver端日志。kubectl logs podsname </p><h2 id><a href="#" class="headerlink" title></a><img src="https://i.loli.net/2020/01/09/5bWGKh34DqVQfYU.jpg" srcset="/img/loading.gif" alt="5"></h2><p>参考资料：<br><a href="https://spark.apache.org/docs/latest/running-on-kubernetes.html" target="_blank" rel="noopener">spark官方doc</a><br><a href="https://www.youtube.com/watch?v=l7UoE97Z24I&list=LL-3gJZTnF4DbSyb7KID7P7g&index=2&t=0s" target="_blank" rel="noopener">youtube动手视频</a></p>]]></content>
    
    
    <categories>
      
      <category>kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>部署</tag>
      
      <tag>大数据</tag>
      
      <tag>kubernetes</tag>
      
      <tag>spark</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes集群部署-admin</title>
    <link href="/2020/01/09/Kubernetes%E9%83%A8%E7%BD%B2/"/>
    <url>/2020/01/09/Kubernetes%E9%83%A8%E7%BD%B2/</url>
    
    <content type="html"><![CDATA[<p>k8s的搭建主要有三种方式：kubeadmin安装、docker安装及二进制安装，其中二进制安装方式最为复杂需要部署人员了解网络https，ca证书等方面的知识，kubeadmin安装方式大大简化了部署操作，但是对K8s的HA支持处于测试阶段，刚入门建议尝试kubeadmin方式安装。docker安装我并不觉得合适，本身k8s作为容器编排系统部署在docker里面，网络及相关端口配置较为复杂，为了避免埋下太多的坑，本人没有尝试这种方式进行安装。</p><p>具体安装部署我就不写在这里了，分享一套自己看的k8s入门视频，很简短能够快速了解k8s，当时k8s学习曲线是比较陡峭的。</p><p>👉🏻<a href="https://pan.baidu.com/s/1ieZmpZdsae73PacD9FZDYA" target="_blank" rel="noopener">百度云盘</a> 密码:amlo</p><p>👉🏻<a href="https://alevelhome-my.sharepoint.com/:w:/p/chenliang/EbQBPVmKj7VBpKZGYo02h4YBGbutlFV-28jq86GsR76HzQ?e=SgE4o5" target="_blank" rel="noopener">安装部署文档</a>  </p><p>按照此文档部署基本没啥坑，注意的是部分镜像在国外服务器，需要替换成文档作者国内的镜像即可。</p><hr><p>最近看到新的工具rancher，企业级管理运维kubernetes集群的好工具的。后续继续更新</p>]]></content>
    
    
    <categories>
      
      <category>kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>部署</tag>
      
      <tag>kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Docker部署伪分布式大数据环境</title>
    <link href="/2020/01/09/Docker%E9%83%A8%E7%BD%B2%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%8E%AF%E5%A2%83/"/>
    <url>/2020/01/09/Docker%E9%83%A8%E7%BD%B2%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%8E%AF%E5%A2%83/</url>
    
    <content type="html"><![CDATA[<h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>通过docker搭建大数据环境  </p><ul><li>开箱即用，不常驻后台，需要的时候启动集群即可，不用的时候关闭集群释放机器资源</li><li>避免了直接安装端口占用问题，大数据平台所需端口较多</li><li>spark资源docker化，便于后期k8s进行资源管理调度</li></ul><p>上述好处主要是基于测试环境下，基于生产环境的大数据平台化要考虑的点很多，例如后期扩容、运维、安全等因素，大数据平台整个是否docker化后期有待考证。</p><h2 id="镜像制作方案"><a href="#镜像制作方案" class="headerlink" title="镜像制作方案"></a>镜像制作方案</h2><p>使用Docker来搭建hadoop,spark及mysql的集群，首先使用Dockerfile制作镜像，把相关的软件拷贝到约定好的目录 下，把配置文件在外面先配置好，再拷贝移动到hadoop,spark的配置目录，为了能使得mysql能从其它节点被访问到，要配置mysql的访问权限。</p><h2 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h2><p>一共3个节点，即启动3个容器。hadoop-master,hadoop-node1,hadoop-node2这三个容器里面安装hadoop和spark集群。<br><img src="https://i.loli.net/2020/01/09/HIAsFMrPekj4fZw.png" srcset="/img/loading.gif" alt="architect.png"></p><h2 id="集群部署"><a href="#集群部署" class="headerlink" title="集群部署"></a>集群部署</h2><h3 id="集群网络规划及子网配置"><a href="#集群网络规划及子网配置" class="headerlink" title="集群网络规划及子网配置"></a>集群网络规划及子网配置</h3><p>既然是做集群，网络的规划是少不了的,至于网络，可以通过Docker中的DockerNetworking的支持配置。首先设置网络，docker中设置 子网可以通过docker network create 方法，这里我们通过命令设置如下的子网。–subnet指定子网络的网段，并为这个子网命名一个名字叫spark</p><pre><code class="shell"># 创建子网docker network create --subnet=172.16.0.0/16 spark# 查看网络docker network lsNETWORK ID          NAME                       DRIVER              SCOPEfab2dd51d1cf        spark                      bridge              local</code></pre><p> 接下来就在我们创建的子网落spark中规划集群中每个容器的ip地址。网络ip分配如下:</p><p>hadoop-master 172.16.0.2</p><p>hadoop-node1 172.16.0.3</p><p>hadoop-node2 172.16.0.4</p><h3 id="软件版本"><a href="#软件版本" class="headerlink" title="软件版本"></a>软件版本</h3><p>网络规划好了，首先Spark我们使用最新的2.4.4版本，Hadoop采用比较稳定的hadoop-2.7.3版本，scala采用scala-2.11.8，JDK采用jdk-8u101-linux-x64。</p><h3 id="SSH无密钥登录规则配置"><a href="#SSH无密钥登录规则配置" class="headerlink" title="SSH无密钥登录规则配置"></a>SSH无密钥登录规则配置</h3><p>注意这里不使用ssh-keygen -t rsa -P ‘’这种方式生成id_rsa.pub，然后集群节点互拷贝id_rsa.pub到authorized_keys文件这种方式，而 是通过在.ssh目录下配置ssh_conf文件的方式，ssh_conf中可以配置SSH的通信规则，例如以正则表达式的方式指定hostname为XXX的 机器之间实现互联互通，而不进行额外的密钥验证。为了编写这个正则表达式，我们5个节点的hostname都以hadoop-*的方式作为开 头，这就是采用这种命名规则的原因。下面来看下ssh_conf配置的内容:</p><pre><code class="shell">Host localhost    StrictHostKeyChecking noHost 0.0.0.0     StrictHostKeyChecking noHost hadoop-*     StrictHostKeyChecking no</code></pre><p>注意上面的最后一行，Host hadoop-* 指定了它的严格的Host验证StrictHostKeyChecking 为no，这样既可以是这5个hostname以 hadoop-*开头的容器之间实现互联互通，而不需要二外的验证。</p><h3 id="构建镜像"><a href="#构建镜像" class="headerlink" title="构建镜像"></a>构建镜像</h3><p>Dockerfile编写完成，接下来写一个build.sh脚本，内容如下:</p><pre><code class="shell"> echo build hadoop images docker build -t=&quot;spark&quot; . </code></pre><p>表示构建一个名叫spark的镜像，.表示Dockerfile的路径，因为在当前路径下，所有用.,若在其他地方则用绝对路径指定Dockerfile的路径 即可。</p><p>运行sh build.sh，就会开始制作镜像了。</p><h2 id="集群运行"><a href="#集群运行" class="headerlink" title="集群运行"></a>集群运行</h2><h3 id="启动容器-start-container-sh"><a href="#启动容器-start-container-sh" class="headerlink" title="启动容器 start_container.sh"></a>启动容器 start_container.sh</h3><p>使用这个镜像可完成容器的启动，因为使用了基于DockerNetworking的网络机制，因此可以在启动容器的时候为容器在子网172.16.0.0/16 spark中分贝172.16.0.1 172.16.0.255以外的IP地址，容器内部容器的通信是基于hostname，因此 需要指定hostname，为了方便容器的管理，需要为启动的每个容器指定一个名字。为了方便外网访问，需要通过-p命令指定容器到宿主机的端口映射。还要为每个容器增加host列表。</p><pre><code class="shell"># hadoop-masterdocker run -itd --restart=always \    --net spark \    --ip 172.16.0.2 \    --privileged \    -p 18032:8032 \    -p 28080:18080 \    -p 29888:19888 \    -p 17077:7077 \    -p 51070:50070 \    -p 18888:8888 \    -p 19000:9000 \    -p 11100:11000 \    -p 51030:50030 \    -p 18050:8050 \    -p 18081:8081 \    -p 18900:8900 \    --name hadoop-master \    --hostname hadoop-master \    --add-host hadoop-node1:172.16.0.3 \    --add-host hadoop-node2:172.16.0.4 \    --add-host hadoop-mysql:172.16.0.6 \    spark /usr/sbin/init# hadoop-node1docker run -itd --restart=always \    --net spark \    --ip 172.16.0.3 \    --privileged \    -p 18042:8042 \    -p 51010:50010 \    -p 51020:50020 \    --name hadoop-node1 \    --hostname hadoop-node1 \    --add-host hadoop-master:172.16.0.2 \    --add-host hadoop-node2:172.16.0.4 \    spark /usr/sbin/init# hadoop-node2docker run -itd --restart=always \    --net spark \    --ip 172.16.0.4 \    --privileged \    -p 18043:8042 \    -p 51011:50011 \    -p 51021:50021 \    --name hadoop-node2 \    --hostname hadoop-node2 \    --add-host hadoop-master:172.16.0.2 \    --add-host hadoop-node1:172.16.0.3 \    spark /usr/sbin/init</code></pre><h3 id="关闭集群-stop-container-sh"><a href="#关闭集群-stop-container-sh" class="headerlink" title="关闭集群 stop_container.sh"></a>关闭集群 stop_container.sh</h3><pre><code class="shell">echo stop containersdocker stop hadoop-masterdocker stop hadoop-node1docker stop hadoop-node2echo remove containersdocker rm hadoop-masterdocker rm hadoop-node1docker rm hadoop-node2echo rm containersdocker ps</code></pre><h3 id="重启集群-restart-container-sh"><a href="#重启集群-restart-container-sh" class="headerlink" title="重启集群 restart_container.sh"></a>重启集群 restart_container.sh</h3><pre><code class="shell">echo stop containersdocker stop hadoop-masterdocker stop hadoop-node1docker stop hadoop-node2echo restart containersdocker start hadoop-masterdocker start hadoop-node1docker start hadoop-node2echo start sshddocker exec -it hadoop-master systemctl start sshddocker exec -it hadoop-node1 systemctl start sshddocker exec -it hadoop-node2 systemctl start sshddocker exec -it hadoop-master ~/restart-hadoop.shecho  containers starteddocker ps</code></pre><h2 id="Trouble-Shooting"><a href="#Trouble-Shooting" class="headerlink" title="Trouble Shooting"></a>Trouble Shooting</h2><ul><li><p>docker里面执行systemctl报错</p><p>解决方案：启动的时候用/usr/sbin/init</p></li><li><p>docker登录harbor报错<br>window直接修改docker设置，添加52.83.79.244:8093到docker insecure registry<br>linux修改方法：<a href="https://blog.csdn.net/u010397369/article/details/42422243" target="_blank" rel="noopener">https://blog.csdn.net/u010397369/article/details/42422243</a>  </p></li></ul><h2 id="Demo"><a href="#Demo" class="headerlink" title="Demo"></a>Demo</h2><p>进入hadoop-master容器内部，执行spark-shell</p><pre><code class="shell">root@node-2 docker-spark]# docker exec -it hadoop-master /bin/bash[root@hadoop-master ~]# sparkspark-class   spark-shell   spark-sql     spark-submit  sparkR[root@hadoop-master ~]# spark-shell19/12/03 04:51:25 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableSetting default log level to &quot;WARN&quot;.To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).19/12/03 04:51:43 WARN util.Utils: spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.Spark context Web UI available at http://hadoop-master:4040Spark context available as &#39;sc&#39; (master = spark://hadoop-master:7077, app id = app-20191203045141-0000).Spark session available as &#39;spark&#39;.Welcome to      ____              __     / __/__  ___ _____/ /__    _\ \/ _ \/ _ `/ __/  &#39;_/   /___/ .__/\_,_/_/ /_/\_\   version 2.4.4      /_/Using Scala version 2.11.12 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_101)Type in expressions to have them evaluated.Type :help for more information.scala&gt;</code></pre><h2 id="本地部署"><a href="#本地部署" class="headerlink" title="本地部署"></a>本地部署</h2><ul><li><p>想要在自己的笔记本环境使用</p><p>首先笔记本环境下需要有docker环境</p><ul><li>拉取gitlab上面的项目：git clone <a href="mailto:git@161.189.27.8">git@161.189.27.8</a>:chenliang/docker-spark.git</li><li>拉去harbor上面的镜像：docker pull 52.83.79.244:8093/wuhan/spark:v1（也可以自己构建镜像，Dockerfile文件在gitlab项目里边）<br>  前提：机器docker环境登录harbor，账号密码：admin 1qaz!QAZ<br>docker login 52.83.79.244:8093<br>登录报错：Error response from daemon: Get <a href="https://52.83.79.244:8093/v2/" target="_blank" rel="noopener">https://52.83.79.244:8093/v2/</a>: http: server gave HTTP response to HTTPS client<br>解决：参见trouble shooting</li><li>使用相关脚本执行启动、停止、重启集群</li></ul></li></ul><ul><li><p>如何自己的spark程序如何在docker环境下执行  </p><p>启动spark的集群之后，使用docker cp等命令将打好的jar包打进容器内，使用spark脚本执行</p></li></ul>]]></content>
    
    
    <categories>
      
      <category>运维</category>
      
    </categories>
    
    
    <tags>
      
      <tag>部署</tag>
      
      <tag>docker</tag>
      
      <tag>环境</tag>
      
      <tag>大数据</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Docker部署Nextcloud</title>
    <link href="/2020/01/09/Docker%E9%83%A8%E7%BD%B2Nextcloud/"/>
    <url>/2020/01/09/Docker%E9%83%A8%E7%BD%B2Nextcloud/</url>
    
    <content type="html"><![CDATA[<h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><h3 id="docker的安装及配置"><a href="#docker的安装及配置" class="headerlink" title="docker的安装及配置"></a>docker的安装及配置</h3><pre><code class="shell"># 通过yum安装yum install -y docker-ce# 启动docker并设置开机启动systemctl start dockersystemctl enable docker# aws ec2 linux2另外一种安装方式sudo amazon-linux-extras install dockersudo service docker start</code></pre><p>由于docker镜像源默认是在国外，拉取镜像速度非常慢，修改daemon配置文件/etc/docker/daemon.json来使用加速器</p><pre><code class="shell">mkdir -p /etc/dockertouch /etc/docker/daemon.jsonvim /etc/docker/daemon.json{  &quot;registry-mirrors&quot;: [&quot;https://b3sst9pc.mirror.aliyuncs.com&quot;]}systemctl daemon-reloadsystemctl restart docker</code></pre><h3 id="docker-compose安装"><a href="#docker-compose安装" class="headerlink" title="docker-compose安装"></a>docker-compose安装</h3><p>Docker Compose是 docker 提供的一个命令行工具，用来定义和运行由多个容器组成的应用。使用 compose，我们可以通过 YAML 文件声明式的定义应用程序的各个服务，并由单个命令完成应用的创建和启动。</p><ul><li><p>docker-compose命令安装</p><pre><code class="shell"># 安装pipyum -y install epel-releaseyum -y install python-pip# 确认版本pip --version# 更新pippip install --upgrade pip# 安装docker-composepip install docker-compose # 查看版本docker-compose version</code></pre></li></ul><h2 id="nextcloud的部署"><a href="#nextcloud的部署" class="headerlink" title="nextcloud的部署"></a>nextcloud的部署</h2><p>nextcloud通过docker-compose命令进行构建，创建 docker-compose.yml文件</p><pre><code class="shell"># 创建docker-compose.yml文件touch docker-compose.yml# 粘贴以下内容version: &#39;2&#39;services:  db:    image: mariadb    restart: always    volumes:      - /data/mariadb:/var/lib/mysql    environment:      - MYSQL_ROOT_PASSWORD=root      - MYSQL_PASSWORD=nextcloud      - MYSQL_DATABASE=nextcloud      - MYSQL_USER=nextcloud  app:    image: nextcloud    restart: always    ports:      - 8091:80    links:      - db    volumes:      - /data/nextcloud/data:/var/www/html/data          - /data/nextcloud/themes:/var/www/html/themes      - /data/nextcloud/apps:/var/www/html/custom_apps</code></pre><p>docker-compose.yml文件说明</p><p>通过启动两个service服务mariadb及nextcloud服务，并通过link连接到一起，其中将/var/www/html/data、/var/www/html/themes、/var/www/html/custom_apps映射到linux服务器指定目录实现数据持久化，这样下次docker重启的时候数据不会发生丢失。</p><p>nextcloud实现预览编辑office文件需要插件onlyoffice支持，通过docker创建onlyoffice服务器，并启动nextcloud配置onlyoffice</p><pre><code class="shell">docker run -it -d -p 8061:80 onlyoffice/documentserver  -v /data/onlyoffice/logs:/var/log/onlyoffice /data/onlyoffice/data:/var/www/onlyoffice/Data /data/onlyoffice/lib:/var/lib/onlyoffice /data/onlyoffice/db:/var/lib/postgresql</code></pre><p>启动nextcloud、停止nextcloud、查看nextcloud启动状态</p><pre><code class="shell"># 启动nextclouddocker-compose up -d# 查看nextcloud状态docker-compose ps# 停止nextclouddocker-compose stop</code></pre><h2 id="nextcloud的数据迁移"><a href="#nextcloud的数据迁移" class="headerlink" title="nextcloud的数据迁移"></a>nextcloud的数据迁移</h2><p>当需要nextcloud迁移到另外一台服务的时候，需要将nextcloud持久化的数据通过scp或者其他方式复制到另外一台机器，将原来的机器mariadb的nextcloud数据库进行导出，在新的机器上面导入数据库数据。复制YAML文件到新的机器，通过docker-compose进行启动，进入容器内部修改持久化数据的权限及修改nextcloud配置文件，最后重启容器。</p><pre><code class="shell"># 复制持久化数据到新的机器上面scp -R /data/nextcloud root@192.168.12.1:/data# 导出原来机器的上面的mariadb数据docker exec -it  nextcloud_db_1【docker容器名称/ID】 mysqldump -uroot -proot【数据库密码】 nextcloud【数据库名称】 &gt; /opt/sql_bak/nextcloud.sql【导出表格路径】# 将sql文件复制到新的机器上面scp  /opt/sql_bak/nextcloud.sql root@192.168.12.1:/opt/sql_bak# 在新的机器上面执行该sql文件(需要先启动docker容器)docker cp /opt/sql_bak/nextcloud.sql 【容器名】:/root/docker exec -it 【容器名/ID】shmysql -uroot -p nextcloud【数据库名】 &lt; /root/nextcloud.sql# 通过docker-compose启动镜像，修改权限 docker exec -it -u root nextcloud_app_1【容器id/容器名】 /bin/bash root@bafc02ce112a:/var/www/html# chown -R www-data:root /var/www/html/ root@bafc02ce112a:exit# 修改nextcloud配置文件vim /data/nextcloud/config/config.php# 修改ip&#39;trusted_domains&#39; =&gt;  array (    0 =&gt; &#39;161.189.27.8:8091&#39;,  ),  &#39;datadirectory&#39; =&gt; &#39;/var/www/html/data&#39;,  &#39;dbtype&#39; =&gt; &#39;mysql&#39;,  &#39;version&#39; =&gt; &#39;16.0.4.1&#39;,  &#39;overwrite.cli.url&#39; =&gt; &#39;http://161.189.27.8:8091&#39;,# 重启docker-compose restart</code></pre><h2 id="trouble-shooting"><a href="#trouble-shooting" class="headerlink" title="trouble shooting"></a>trouble shooting</h2><ul><li><p>安装docker-compose命令执行pip install docker-compose的时候报以下错误</p><pre><code>ERROR: Cannot uninstall &#39;requests&#39;. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.</code></pre><p>解决版本，强制安装requests包</p><pre><code class="shell"> pip install --ignore-installed requests</code></pre><p>再重新执行</p><pre><code class="shell">pip install docker-compose</code></pre></li><li><p>docker-compose报错：No module named ssl_match_hostname</p><pre><code>File &quot;/usr/local/lib/python2.7/dist-packages/docker/transport/ssladapter.py&quot;, line 23, in &lt;module&gt;from backports.ssl_match_hostname import match_hostnameImportError: No module named ssl_match_hostname</code></pre><p>原因：</p><p><strong>/usr/local/lib/python2.7/distpackages/docker/transport/ssladapter.py **<br>在包路径下找不到 **backports包里的ssl_match_hostname</strong>模块</p><p>解决办法</p><pre><code class="shell">#进入backports模块路径cd /usr/lib/python2.7/site-packages#复制整个包到transport包路径下cp -r backports /usr/lib/python2.7/site-packages/docker/transport</code></pre></li></ul>]]></content>
    
    
    <categories>
      
      <category>运维</category>
      
    </categories>
    
    
    <tags>
      
      <tag>部署</tag>
      
      <tag>docker</tag>
      
      <tag>nextcloud</tag>
      
      <tag>网盘</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Docker私服Harbor使用指南</title>
    <link href="/2020/01/09/Docker%E7%A7%81%E6%9C%8DHarbor%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/"/>
    <url>/2020/01/09/Docker%E7%A7%81%E6%9C%8DHarbor%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/</url>
    
    <content type="html"><![CDATA[<h2 id="harbor搭建"><a href="#harbor搭建" class="headerlink" title="harbor搭建"></a>harbor搭建</h2><p>官方提供两种方式安装harbor，离线及在线方式。本文档描述离线方式进行安装</p><p>查看docker-compose版本，若版本低进行升级安装，</p><p>下载官方安装包</p><pre><code class="shell">cd /optwget https://storage.googleapis.com/harbor-releases/release-1.9.0/harbor-offline-installer-v1.9.2-rc1.tgz# 解压tar zxvf harbor-offline-installer-v1.9.2-rc1.tgz# 查看cd harborls[root@ip-172-31-23-16 harbor]# lltotal 623296drwxr-xr-x 3 root root        20 Nov  5 05:50 common-rw-r--r-- 1 root root      5369 Nov  5 06:07 docker-compose.yml-rw-r--r-- 1 root root 638214056 Nov  1 03:14 harbor.v1.9.2.tar.gz-rw-r--r-- 1 root root      5816 Nov  5 06:07 harbor.yml-rwxr-xr-x 1 root root      5088 Nov  1 03:13 install.sh-rw-r--r-- 1 root root     11347 Nov  1 03:13 LICENSE-rwxr-xr-x 1 root root      1748 Nov  1 03:13 prepare</code></pre><p>修改配置，主要修改两个文件</p><p>harbor.yml为系统配置文件，docker-compose.yml为docker相关配置文件</p><p>修改harbor.yml</p><pre><code class="shell">vim harbor.yml# 修改hostnamehostname: 52.83.79.244# 修改映射端口http:  port: 8093# 修改admin password（可选）harbor_admin_password: 1qaz!QAZ# 修改数据库相关参数database:  password: root  max_idle_conns: 50  max_open_conns: 100# 修改数据持久化host目录data_volume: /data/harbor# 修改log目录 location: /var/log/harbor</code></pre><p>修改docker-compose.yml文件</p><pre><code class="yaml">vim docker-compose.yml# 添加ports映射registry:    image: goharbor/registry-photon:v2.7.1-patch-2819-2553-v1.9.2    container_name: registry    restart: always    ports:      - 5000:5000</code></pre><p>启动harbor</p><pre><code>./install.sh</code></pre><p>安装之后查看</p><pre><code class="shell">docker-compose ps     Name                     Command                  State                 Ports---------------------------------------------------------------------------------------------harbor-core         /harbor/harbor_core              Up (healthy)harbor-db           /docker-entrypoint.sh            Up (healthy)   5432/tcpharbor-jobservice   /harbor/harbor_jobservice  ...   Up (healthy)harbor-log          /bin/sh -c /usr/local/bin/ ...   Up (healthy)   127.0.0.1:1514-&gt;10514/tcpharbor-portal       nginx -g daemon off;             Up (healthy)   8080/tcpnginx               nginx -g daemon off;             Up (healthy)   0.0.0.0:8093-&gt;8080/tcpredis               redis-server /etc/redis.conf     Up (healthy)   6379/tcpregistry            /entrypoint.sh /etc/regist ...   Restartingregistryctl         /harbor/start.sh                 Up (healthy)</code></pre><p>登录harbor ，账号密码为harbor.yml所设置的密码</p><p><a href="http://52.83.79.244:8093/harbor/projects" target="_blank" rel="noopener">http://52.83.79.244:8093/harbor/projects</a></p><p>登录时候还不能直接利用docker push 到服务器，这是因为 docker1.3.2 版本开始默认 docker registry 使用的是 https，我们设置 Harbor 默认 http 方式，所以当执行用 docker login、pull、push 等命令操作非 https 的 docker regsitry 的时就会报错。解决办法：</p><pre><code class="shell">vim /usr/lib/systemd/system/docker.service# ExecStart 增加 --insecure-registry=52.83.79.244【配置文件中的hostname】ExecStart=/usr/bin/dockerd $OPTIONS $DOCKER_STORAGE_OPTIONS $DOCKER_ADD_RUNTIMES --insecure-registry=52.83.79.244:8093# 重启服务systemctl daemon-reloadsystemctl restart docker</code></pre><p>harbor常用命令</p><pre><code class="shell"># 需要进入harbor文件夹执行cd /opt/harbordocker-compose up -d               ###后台启动，如果容器不存在根据镜像自动创建docker-compose down   -v           ###停止容器并删除容器docker-compose start               ###启动容器，容器不存在就无法启动，不会自动创建镜像docker-compose stop                ###停止容器</code></pre><h2 id="使用及配置"><a href="#使用及配置" class="headerlink" title="使用及配置"></a>使用及配置</h2><ul><li><p>docker 登录到harbor</p><pre><code class="shell">[root@ip-172-31-23-16 harbor]# docker login 52.83.79.244:8093Username: adminPassword:WARNING! Your password will be stored unencrypted in /root/.docker/config.json.Configure a credential helper to remove this warning. Seehttps://docs.docker.com/engine/reference/commandline/login/#credentials-storeLogin Succeeded</code></pre></li></ul><p>登录harbor后创建一个公共仓库，命名为wuhan</p><p>将镜像发布到harbor</p><pre><code class="shell"># 查看需要上传的镜像docker images# 为镜像上tagdocker tag jenkins/jenkins:lts 52.83.79.244:8093/wuhan/jenkins:lts# 上传到wuhan库docker push jenkins/jenkins:lts 52.83.79.244:8093/wuhan/jenkins:lts# 若需要上传到library库docker push jenkins/jenkins:lts 52.83.79.244:8093/library/jenkins:lts</code></pre><h2 id="配置https协议"><a href="#配置https协议" class="headerlink" title="配置https协议"></a>配置https协议</h2><pre><code class="shell"># 准备工作mkdir -p /data/harbor-certyum install -y openssl------------cd data/harbor-certopenssl genrsa -des3 -out server.key 2048 openssl req -new -key server.key -out server.csr cp server.key server.key.org openssl rsa -in server.key.org -out server.key openssl x509 -req -days 365 -in server.csr -signkey server.key -out server.crt </code></pre><p>生成证书之后，修改harbor.yaml文件<br>具体配置如下<br><a href="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/WX20191219-155232@2x.png" target="_blank" rel="noopener"><img src="http://52.83.79.244:6875/uploads/images/gallery/2019-12-Dec/scaled-840-0/WX20191219-155232@2x.png" srcset="/img/loading.gif" alt="WX20191219-155232@2x.png"></a></p><h2 id="使用及配置-1"><a href="#使用及配置-1" class="headerlink" title="使用及配置"></a>使用及配置</h2><ul><li><p>docker 登录到harbor</p><pre><code class="shell">[root@ip-172-31-23-16 harbor]# docker login 52.83.79.244:8093Username: adminPassword:WARNING! Your password will be stored unencrypted in /root/.docker/config.json.Configure a credential helper to remove this warning. Seehttps://docs.docker.com/engine/reference/commandline/login/#credentials-storeLogin Succeeded</code></pre></li></ul><p>登录harbor后创建一个公共仓库，命名为wuhan</p><p>将镜像发布到harbor</p><pre><code class="shell"># 查看需要上传的镜像docker images# 为镜像上tagdocker tag jenkins/jenkins:lts 52.83.79.244:8093/wuhan/jenkins:lts# 上传到wuhan库docker push jenkins/jenkins:lts 52.83.79.244:8093/wuhan/jenkins:lts# 若需要上传到library库docker push 52.83.79.244:8093/library/jenkins:lts</code></pre><h2 id="trouble-shooting"><a href="#trouble-shooting" class="headerlink" title="trouble shooting"></a>trouble shooting</h2><ul><li>docker登录harbor报错<br>window直接修改docker设置，添加52.83.79.244:8093到docker insecure registry<br>linux修改方法：<a href="https://blog.csdn.net/u010397369/article/details/42422243" target="_blank" rel="noopener">https://blog.csdn.net/u010397369/article/details/42422243</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>运维</category>
      
    </categories>
    
    
    <tags>
      
      <tag>部署</tag>
      
      <tag>docker</tag>
      
      <tag>证书</tag>
      
      <tag>harbor</tag>
      
      <tag>https</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>K8s监控部署方案</title>
    <link href="/2020/01/07/K8s%E7%9B%91%E6%8E%A7%E9%83%A8%E7%BD%B2%E6%96%B9%E6%A1%88/"/>
    <url>/2020/01/07/K8s%E7%9B%91%E6%8E%A7%E9%83%A8%E7%BD%B2%E6%96%B9%E6%A1%88/</url>
    
    <content type="html"><![CDATA[<h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><p><img src="https://i.loli.net/2020/01/08/DVBiRAwaNsnYbgL.png" srcset="/img/loading.gif" alt="grafana"></p><h2 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h2><p>yaml文件地址见文章底部 ⬇️</p><h3 id="1-在k8s集群中创建namespace"><a href="#1-在k8s集群中创建namespace" class="headerlink" title="1 在k8s集群中创建namespace"></a>1 在k8s集群中创建namespace</h3><pre><code class="yaml">apiVersion: v1kind: Namespacemetadata:   name: ns-monitor  labels:    name: ns-monitorkubectl apply -f namespace.yaml</code></pre><h3 id="2-安装node-exporter"><a href="#2-安装node-exporter" class="headerlink" title="2 安装node-exporter"></a>2 安装node-exporter</h3><p>在kubernetest集群中部署node-exporter，Node-exporter用于采集kubernetes集群中各个节点的物理指标，比如：Memory、CPU等。可以直接在每个物理节点是直接安装，这里我们使用DaemonSet部署到每个节点上，使用 hostNetwork: true 和 hostPID: true 使其获得Node的物理指标信息，配置tolerations使其在master节点也启动一个pod。</p><p>node-exporter.yaml</p><pre><code class="yaml">kind: DaemonSetapiVersion: apps/v1beta2metadata:   labels:    app: node-exporter  name: node-exporter  namespace: ns-monitorspec:  revisionHistoryLimit: 10  selector:    matchLabels:      app: node-exporter  template:    metadata:      labels:        app: node-exporter    spec:      containers:        - name: node-exporter          image: prom/node-exporter:v0.16.0          ports:            - containerPort: 9100              protocol: TCP              name:    http      hostNetwork: true      hostPID: true      tolerations:        - effect: NoSchedule          operator: Exists---kind: ServiceapiVersion: v1metadata:  labels:    app: node-exporter  name: node-exporter-service  namespace: ns-monitorspec:  ports:    - name:    http      port: 9100      nodePort: 31672      protocol: TCP  type: NodePort  selector:    app: node-exporter</code></pre><pre><code class="shell">kubectl apply -f node-exporter.yaml</code></pre><p><strong>*检查是否执行成功(对应pod及svc)</strong> 👉🏻 **</p><pre><code class="shell">➜  ~ kubectl get pod -n ns-monitorNAME                          READY   STATUS    RESTARTS   AGEgrafana-547699f75-lxljq       1/1     Running   0          3h37mnode-exporter-75nmc           0/1     Pending   0          20hnode-exporter-t29kx           1/1     Running   0          20hnode-exporter-z6s7x           1/1     Running   0          20hprometheus-7d7654554d-f5fvf   1/1     Running   0          3h45m➜  ~ kubectl get svc -n ns-monitorNAME                    TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGEgrafana-service         NodePort   10.1.242.56    &lt;none&gt;        3000:31026/TCP   3h38mnode-exporter-service   NodePort   10.1.130.7     &lt;none&gt;        9100:31672/TCP   20hprometheus-service      NodePort   10.1.133.130   &lt;none&gt;        9090:30753/TCP   3h45m</code></pre><p><img src="https://i.loli.net/2020/01/08/TP6tIlvFKhgzHuZ.png" srcset="/img/loading.gif" alt="image-20200103143320329"></p><h3 id="3-部署Prometheus-pod"><a href="#3-部署Prometheus-pod" class="headerlink" title="3 部署Prometheus pod"></a>3 部署Prometheus pod</h3><p>prometheus.yaml 中包含rbac认证、ConfigMap等</p><pre><code class="shell">kubectl apply -f prometheus.yaml </code></pre><p><em>检查是否执行成功(对应pod及svc)*</em> 👉🏻 </p><pre><code class="shell">➜  ~ kubectl get pod -n ns-monitorNAME                          READY   STATUS    RESTARTS   AGEgrafana-547699f75-lxljq       1/1     Running   0          3h37mnode-exporter-75nmc           0/1     Pending   0          20hnode-exporter-t29kx           1/1     Running   0          20hnode-exporter-z6s7x           1/1     Running   0          20hprometheus-7d7654554d-f5fvf   1/1     Running   0          3h45m➜  ~ kubectl get svc -n ns-monitorNAME                    TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGEgrafana-service         NodePort   10.1.242.56    &lt;none&gt;        3000:31026/TCP   3h38mnode-exporter-service   NodePort   10.1.130.7     &lt;none&gt;        9100:31672/TCP   20hprometheus-service      NodePort   10.1.133.130   &lt;none&gt;        9090:30753/TCP   3h45m</code></pre><p><img src="https://i.loli.net/2020/01/08/fdFs8EN2xhHwCM6.png" srcset="/img/loading.gif" alt="image-20200103143645494"></p><h3 id="4-在k8s中部署grafana"><a href="#4-在k8s中部署grafana" class="headerlink" title="4 在k8s中部署grafana"></a>4 在k8s中部署grafana</h3><pre><code class="shell">kubectl apply -f grafana.yaml</code></pre><p><strong>检查是否执行成功(对应pod及svc)</strong> 👉🏻 </p><pre><code class="shell">➜  ~ kubectl get pod -n ns-monitorNAME                          READY   STATUS    RESTARTS   AGEgrafana-547699f75-lxljq       1/1     Running   0          3h37mnode-exporter-75nmc           0/1     Pending   0          20hnode-exporter-t29kx           1/1     Running   0          20hnode-exporter-z6s7x           1/1     Running   0          20hprometheus-7d7654554d-f5fvf   1/1     Running   0          3h45m➜  ~ kubectl get svc -n ns-monitorNAME                    TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGEgrafana-service         NodePort   10.1.242.56    &lt;none&gt;        3000:31026/TCP   3h38mnode-exporter-service   NodePort   10.1.130.7     &lt;none&gt;        9100:31672/TCP   20hprometheus-service      NodePort   10.1.133.130   &lt;none&gt;        9090:30753/TCP   3h45m</code></pre><h3 id="5-配置grafana数据源"><a href="#5-配置grafana数据源" class="headerlink" title="5 配置grafana数据源"></a>5 配置grafana数据源</h3><p>把prometheus配置成数据源 ：<a href="http://prometheus-service.ns-monitor:9090" target="_blank" rel="noopener">http://prometheus-service.ns-monitor:9090</a></p><p><img src="https://i.loli.net/2020/01/08/fzxnrR5iguDjlHk.png" srcset="/img/loading.gif" alt="image-20200103144416930"></p><h3 id="6-倒入dashboard"><a href="#6-倒入dashboard" class="headerlink" title="6 倒入dashboard"></a>6 倒入dashboard</h3><p>把 kubernetes的Dashboard的模板导入进来，直接把JSON格式内容复制进来。</p><p><img src="https://i.loli.net/2020/01/08/6XkA5hEjN1OWioS.png" srcset="/img/loading.gif" alt="image-20200103145516630"></p><h2 id="效果图"><a href="#效果图" class="headerlink" title="效果图"></a>效果图</h2><p><img src="https://i.loli.net/2020/01/08/bqEolIi8KVSGuHk.png" srcset="/img/loading.gif" alt="image-20200103145627198"></p><h2 id="Ref"><a href="#Ref" class="headerlink" title="Ref"></a>Ref</h2><ul><li><p><a href="https://jimmysong.io/kubernetes-handbook/practice/using-prometheus-to-monitor-kuberentes-cluster.html" target="_blank" rel="noopener">使用Prometheus监控kubernetes集群</a></p></li><li><p><a href="https://www.jianshu.com/p/ac8853927528" target="_blank" rel="noopener">k8s安装Prometheus+Grafana</a></p></li><li><p><a href="https://github.com/giantswarm/kubernetes-prometheus" target="_blank" rel="noopener">github-kubernetes-promethues</a></p></li><li><p><a href="https://github.com/giantswarm/prometheus" target="_blank" rel="noopener">github-giantswarm-promethues</a></p></li></ul><h2 id="Yaml文件及Dashboard配置json文件"><a href="#Yaml文件及Dashboard配置json文件" class="headerlink" title="Yaml文件及Dashboard配置json文件"></a>Yaml文件及Dashboard配置json文件</h2><pre><code class="yaml">apiVersion: v1kind: Namespacemetadata:   name: ns-monitor  labels:    name: ns-monitor---kind: DaemonSetapiVersion: apps/v1beta2metadata:   labels:    app: node-exporter  name: node-exporter  namespace: ns-monitorspec:  revisionHistoryLimit: 10  selector:    matchLabels:      app: node-exporter  template:    metadata:      labels:        app: node-exporter    spec:      containers:        - name: node-exporter          image: prom/node-exporter:v0.16.0          ports:            - containerPort: 9100              protocol: TCP              name:    http      hostNetwork: true      hostPID: true      tolerations:        - effect: NoSchedule          operator: Exists---kind: ServiceapiVersion: v1metadata:  labels:    app: node-exporter  name: node-exporter-service  namespace: ns-monitorspec:  ports:    - name:    http      port: 9100      nodePort: 31672      protocol: TCP  type: NodePort  selector:    app: node-exporter------apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRolemetadata:  name: prometheusrules:  - apiGroups: [&quot;&quot;] # &quot;&quot; indicates the core API group    resources:      - nodes      - nodes/proxy      - services      - endpoints      - pods    verbs:      - get      - watch      - list  - apiGroups:      - extensions    resources:      - ingresses    verbs:      - get      - watch      - list  - nonResourceURLs: [&quot;/metrics&quot;]    verbs:      - get---apiVersion: v1kind: ServiceAccountmetadata:  name: prometheus  namespace: ns-monitor  labels:    app: prometheus---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata:  name: prometheussubjects:  - kind: ServiceAccount    name: prometheus    namespace: ns-monitorroleRef:  kind: ClusterRole  name: prometheus  apiGroup: rbac.authorization.k8s.io---apiVersion: v1kind: ConfigMapmetadata:  name: prometheus-conf  namespace: ns-monitor  labels:    app: prometheusdata:  prometheus.yml: |-    # my global config    global:      scrape_interval:     15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.      evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.      # scrape_timeout is set to the global default (10s).    # Alertmanager configuration    alerting:      alertmanagers:      - static_configs:        - targets:          # - alertmanager:9093    # Load rules once and periodically evaluate them according to the global &#39;evaluation_interval&#39;.    rule_files:      # - &quot;first_rules.yml&quot;      # - &quot;second_rules.yml&quot;    # A scrape configuration containing exactly one endpoint to scrape:    # Here it&#39;s Prometheus itself.    scrape_configs:      # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config.      - job_name: &#39;prometheus&#39;        # metrics_path defaults to &#39;/metrics&#39;        # scheme defaults to &#39;http&#39;.        static_configs:          - targets: [&#39;localhost:9090&#39;]      - job_name: &#39;grafana&#39;        static_configs:          - targets:              - &#39;grafana-service.ns-monitor:3000&#39;      - job_name: &#39;kubernetes-apiservers&#39;        kubernetes_sd_configs:        - role: endpoints        # Default to scraping over https. If required, just disable this or change to        # `http`.        scheme: https        # This TLS &amp; bearer token file config is used to connect to the actual scrape        # endpoints for cluster components. This is separate to discovery auth        # configuration because discovery &amp; scraping are two separate concerns in        # Prometheus. The discovery auth config is automatic if Prometheus runs inside        # the cluster. Otherwise, more config options have to be provided within the        # &lt;kubernetes_sd_config&gt;.        tls_config:          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt          # If your node certificates are self-signed or use a different CA to the          # master CA, then disable certificate verification below. Note that          # certificate verification is an integral part of a secure infrastructure          # so this should only be disabled in a controlled environment. You can          # disable certificate verification by uncommenting the line below.          #          # insecure_skip_verify: true        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token        # Keep only the default/kubernetes service endpoints for the https port. This        # will add targets for each API server which Kubernetes adds an endpoint to        # the default/kubernetes service.        relabel_configs:        - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]          action: keep          regex: default;kubernetes;https      # Scrape config for nodes (kubelet).      #      # Rather than connecting directly to the node, the scrape is proxied though the      # Kubernetes apiserver.  This means it will work if Prometheus is running out of      # cluster, or can&#39;t connect to nodes for some other reason (e.g. because of      # firewalling).      - job_name: &#39;kubernetes-nodes&#39;        # Default to scraping over https. If required, just disable this or change to        # `http`.        scheme: https        # This TLS &amp; bearer token file config is used to connect to the actual scrape        # endpoints for cluster components. This is separate to discovery auth        # configuration because discovery &amp; scraping are two separate concerns in        # Prometheus. The discovery auth config is automatic if Prometheus runs inside        # the cluster. Otherwise, more config options have to be provided within the        # &lt;kubernetes_sd_config&gt;.        tls_config:          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token        kubernetes_sd_configs:        - role: node        relabel_configs:        - action: labelmap          regex: __meta_kubernetes_node_label_(.+)        - target_label: __address__          replacement: kubernetes.default.svc:443        - source_labels: [__meta_kubernetes_node_name]          regex: (.+)          target_label: __metrics_path__          replacement: /api/v1/nodes/${1}/proxy/metrics      # Scrape config for Kubelet cAdvisor.      #      # This is required for Kubernetes 1.7.3 and later, where cAdvisor metrics      # (those whose names begin with &#39;container_&#39;) have been removed from the      # Kubelet metrics endpoint.  This job scrapes the cAdvisor endpoint to      # retrieve those metrics.      #      # In Kubernetes 1.7.0-1.7.2, these metrics are only exposed on the cAdvisor      # HTTP endpoint; use &quot;replacement: /api/v1/nodes/${1}:4194/proxy/metrics&quot;      # in that case (and ensure cAdvisor&#39;s HTTP server hasn&#39;t been disabled with      # the --cadvisor-port=0 Kubelet flag).      #      # This job is not necessary and should be removed in Kubernetes 1.6 and      # earlier versions, or it will cause the metrics to be scraped twice.      - job_name: &#39;kubernetes-cadvisor&#39;        # Default to scraping over https. If required, just disable this or change to        # `http`.        scheme: https        # This TLS &amp; bearer token file config is used to connect to the actual scrape        # endpoints for cluster components. This is separate to discovery auth        # configuration because discovery &amp; scraping are two separate concerns in        # Prometheus. The discovery auth config is automatic if Prometheus runs inside        # the cluster. Otherwise, more config options have to be provided within the        # &lt;kubernetes_sd_config&gt;.        tls_config:          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token        kubernetes_sd_configs:        - role: node        relabel_configs:        - action: labelmap          regex: __meta_kubernetes_node_label_(.+)        - target_label: __address__          replacement: kubernetes.default.svc:443        - source_labels: [__meta_kubernetes_node_name]          regex: (.+)          target_label: __metrics_path__          replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor      # Scrape config for service endpoints.      #      # The relabeling allows the actual service scrape endpoint to be configured      # via the following annotations:      #      # * `prometheus.io/scrape`: Only scrape services that have a value of `true`      # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need      # to set this to `https` &amp; most likely set the `tls_config` of the scrape config.      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.      # * `prometheus.io/port`: If the metrics are exposed on a different port to the      # service then set this appropriately.      - job_name: &#39;kubernetes-service-endpoints&#39;        kubernetes_sd_configs:        - role: endpoints        relabel_configs:        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]          action: keep          regex: true        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]          action: replace          target_label: __scheme__          regex: (https?)        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]          action: replace          target_label: __metrics_path__          regex: (.+)        - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]          action: replace          target_label: __address__          regex: ([^:]+)(?::\d+)?;(\d+)          replacement: $1:$2        - action: labelmap          regex: __meta_kubernetes_service_label_(.+)        - source_labels: [__meta_kubernetes_namespace]          action: replace          target_label: kubernetes_namespace        - source_labels: [__meta_kubernetes_service_name]          action: replace          target_label: kubernetes_name      # Example scrape config for probing services via the Blackbox Exporter.      #      # The relabeling allows the actual service scrape endpoint to be configured      # via the following annotations:      #      # * `prometheus.io/probe`: Only probe services that have a value of `true`      - job_name: &#39;kubernetes-services&#39;        metrics_path: /probe        params:          module: [http_2xx]        kubernetes_sd_configs:        - role: service        relabel_configs:        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]          action: keep          regex: true        - source_labels: [__address__]          target_label: __param_target        - target_label: __address__          replacement: blackbox-exporter.example.com:9115        - source_labels: [__param_target]          target_label: instance        - action: labelmap          regex: __meta_kubernetes_service_label_(.+)        - source_labels: [__meta_kubernetes_namespace]          target_label: kubernetes_namespace        - source_labels: [__meta_kubernetes_service_name]          target_label: kubernetes_name      # Example scrape config for probing ingresses via the Blackbox Exporter.      #      # The relabeling allows the actual ingress scrape endpoint to be configured      # via the following annotations:      #      # * `prometheus.io/probe`: Only probe services that have a value of `true`      - job_name: &#39;kubernetes-ingresses&#39;        metrics_path: /probe        params:          module: [http_2xx]        kubernetes_sd_configs:          - role: ingress        relabel_configs:          - source_labels: [__meta_kubernetes_ingress_annotation_prometheus_io_probe]            action: keep            regex: true          - source_labels: [__meta_kubernetes_ingress_scheme,__address__,__meta_kubernetes_ingress_path]            regex: (.+);(.+);(.+)            replacement: ${1}://${2}${3}            target_label: __param_target          - target_label: __address__            replacement: blackbox-exporter.example.com:9115          - source_labels: [__param_target]            target_label: instance          - action: labelmap            regex: __meta_kubernetes_ingress_label_(.+)          - source_labels: [__meta_kubernetes_namespace]            target_label: kubernetes_namespace          - source_labels: [__meta_kubernetes_ingress_name]            target_label: kubernetes_name      # Example scrape config for pods      #      # The relabeling allows the actual pod scrape endpoint to be configured via the      # following annotations:      #      # * `prometheus.io/scrape`: Only scrape pods that have a value of `true`      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.      # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the      # pod&#39;s declared ports (default is a port-free target if none are declared).      - job_name: &#39;kubernetes-pods&#39;        kubernetes_sd_configs:        - role: pod        relabel_configs:        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]          action: keep          regex: true        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]          action: replace          target_label: __metrics_path__          regex: (.+)        - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]          action: replace          regex: ([^:]+)(?::\d+)?;(\d+)          replacement: $1:$2          target_label: __address__        - action: labelmap          regex: __meta_kubernetes_pod_label_(.+)        - source_labels: [__meta_kubernetes_namespace]          action: replace          target_label: kubernetes_namespace        - source_labels: [__meta_kubernetes_pod_name]          action: replace          target_label: kubernetes_pod_name---apiVersion: v1kind: ConfigMapmetadata:  name: prometheus-rules  namespace: ns-monitor  labels:    app: prometheusdata:  cpu-usage.rule: |    groups:      - name: NodeCPUUsage        rules:          - alert: NodeCPUUsage            expr: (100 - (avg by (instance) (irate(node_cpu{name=&quot;node-exporter&quot;,mode=&quot;idle&quot;}[5m])) * 100)) &gt; 75            for: 2m            labels:              severity: &quot;page&quot;            annotations:              summary: &quot;{{$labels.instance}}: High CPU usage detected&quot;              description: &quot;{{$labels.instance}}: CPU usage is above 75% (current value is: {{ $value }})&quot;---apiVersion: v1kind: PersistentVolumemetadata:  name: &quot;prometheus-data-pv&quot;  labels:    name: prometheus-data-pv    release: stablespec:  capacity:    storage: 5Gi  accessModes:    - ReadWriteOnce  persistentVolumeReclaimPolicy: Recycle  nfs:    path: /nfs    server: 52.83.79.244---apiVersion: v1kind: PersistentVolumeClaimmetadata:  name: prometheus-data-pvc  namespace: ns-monitorspec:  accessModes:    - ReadWriteOnce  resources:    requests:      storage: 5Gi  selector:    matchLabels:      name: prometheus-data-pv      release: stable---kind: DeploymentapiVersion: apps/v1beta2metadata:  labels:    app: prometheus  name: prometheus  namespace: ns-monitorspec:  replicas: 1  revisionHistoryLimit: 10  selector:    matchLabels:      app: prometheus  template:    metadata:      labels:        app: prometheus    spec:      serviceAccountName: prometheus      securityContext:        runAsUser: 0      containers:        - name: prometheus          image: 52.83.79.244:8093/pythagoras/prometheus:latest          imagePullPolicy: IfNotPresent          volumeMounts:            - mountPath: /prometheus              name: prometheus-data-volume            - mountPath: /etc/prometheus/prometheus.yml              name: prometheus-conf-volume              subPath: prometheus.yml            - mountPath: /etc/prometheus/rules              name: prometheus-rules-volume          ports:            - containerPort: 9090              protocol: TCP      volumes:        - name: prometheus-data-volume          persistentVolumeClaim:            claimName: prometheus-data-pvc        - name: prometheus-conf-volume          configMap:            name: prometheus-conf        - name: prometheus-rules-volume          configMap:            name: prometheus-rules      tolerations:        - key: node-role.kubernetes.io/master          effect: NoSchedule---kind: ServiceapiVersion: v1metadata:  annotations:    prometheus.io/scrape: &#39;true&#39;  labels:    app: prometheus  name: prometheus-service  namespace: ns-monitorspec:  ports:    - port: 9090      targetPort: 9090  selector:    app: prometheus  type: NodePort---apiVersion: v1kind: PersistentVolumemetadata:  name: &quot;grafana-data-pv&quot;  labels:    name: grafana-data-pv    release: stablespec:  capacity:    storage: 5Gi  accessModes:    - ReadWriteOnce  persistentVolumeReclaimPolicy: Recycle  nfs:    path: /nfs/grafana    server: 52.83.79.244---apiVersion: v1kind: PersistentVolumeClaimmetadata:  name: grafana-data-pvc  namespace: ns-monitorspec:  accessModes:    - ReadWriteOnce  resources:    requests:      storage: 5Gi  selector:    matchLabels:      name: grafana-data-pv      release: stable---kind: DeploymentapiVersion: apps/v1beta2metadata:  labels:    app: grafana  name: grafana  namespace: ns-monitorspec:  replicas: 1  revisionHistoryLimit: 10  selector:    matchLabels:      app: grafana  template:    metadata:      labels:        app: grafana    spec:      securityContext:        runAsUser: 0      containers:        - name: grafana          image: 52.83.79.244:8093/pythagoras/grafana:latest          imagePullPolicy: IfNotPresent          env:            - name: GF_AUTH_BASIC_ENABLED              value: &quot;true&quot;            - name: GF_AUTH_ANONYMOUS_ENABLED              value: &quot;false&quot;          readinessProbe:            httpGet:              path: /login              port: 3000          volumeMounts:            - mountPath: /var/lib/grafana              name: grafana-data-volume          ports:            - containerPort: 3000              protocol: TCP      volumes:        - name: grafana-data-volume          persistentVolumeClaim:            claimName: grafana-data-pvc---kind: ServiceapiVersion: v1metadata:  labels:    app: grafana  name: grafana-service  namespace: ns-monitorspec:  ports:    - port: 3000      targetPort: 3000  selector:    app: grafana  type: NodePort</code></pre><blockquote><p>dashboard配置</p></blockquote><pre><code class="json">{  &quot;__inputs&quot;: [    {      &quot;name&quot;: &quot;DS_PROMETHEUS&quot;,      &quot;label&quot;: &quot;prometheus&quot;,      &quot;description&quot;: &quot;&quot;,      &quot;type&quot;: &quot;datasource&quot;,      &quot;pluginId&quot;: &quot;prometheus&quot;,      &quot;pluginName&quot;: &quot;Prometheus&quot;    }  ],  &quot;__requires&quot;: [    {      &quot;type&quot;: &quot;grafana&quot;,      &quot;id&quot;: &quot;grafana&quot;,      &quot;name&quot;: &quot;Grafana&quot;,      &quot;version&quot;: &quot;5.0.4&quot;    },    {      &quot;type&quot;: &quot;panel&quot;,      &quot;id&quot;: &quot;graph&quot;,      &quot;name&quot;: &quot;Graph&quot;,      &quot;version&quot;: &quot;5.0.0&quot;    },    {      &quot;type&quot;: &quot;datasource&quot;,      &quot;id&quot;: &quot;prometheus&quot;,      &quot;name&quot;: &quot;Prometheus&quot;,      &quot;version&quot;: &quot;5.0.0&quot;    },    {      &quot;type&quot;: &quot;panel&quot;,      &quot;id&quot;: &quot;singlestat&quot;,      &quot;name&quot;: &quot;Singlestat&quot;,      &quot;version&quot;: &quot;5.0.0&quot;    }  ],  &quot;annotations&quot;: {    &quot;list&quot;: [      {        &quot;builtIn&quot;: 1,        &quot;datasource&quot;: &quot;-- Grafana --&quot;,        &quot;enable&quot;: true,        &quot;hide&quot;: true,        &quot;iconColor&quot;: &quot;rgba(0, 211, 255, 1)&quot;,        &quot;name&quot;: &quot;Annotations &amp; Alerts&quot;,        &quot;type&quot;: &quot;dashboard&quot;      }    ]  },  &quot;description&quot;: &quot;Shows resource usage of Kubernetes pods.&quot;,  &quot;editable&quot;: true,  &quot;gnetId&quot;: 737,  &quot;graphTooltip&quot;: 0,  &quot;id&quot;: null,  &quot;iteration&quot;: 1524106098941,  &quot;links&quot;: [],  &quot;panels&quot;: [    {      &quot;collapsed&quot;: false,      &quot;gridPos&quot;: {        &quot;h&quot;: 1,        &quot;w&quot;: 24,        &quot;x&quot;: 0,        &quot;y&quot;: 0      },      &quot;id&quot;: 35,      &quot;panels&quot;: [],      &quot;title&quot;: &quot;all pods&quot;,      &quot;type&quot;: &quot;row&quot;    },    {      &quot;cacheTimeout&quot;: null,      &quot;colorBackground&quot;: false,      &quot;colorValue&quot;: true,      &quot;colors&quot;: [        &quot;rgba(50, 172, 45, 0.97)&quot;,        &quot;rgba(237, 129, 40, 0.89)&quot;,        &quot;rgba(245, 54, 54, 0.9)&quot;      ],      &quot;datasource&quot;: &quot;${DS_PROMETHEUS}&quot;,      &quot;editable&quot;: true,      &quot;error&quot;: false,      &quot;format&quot;: &quot;percent&quot;,      &quot;gauge&quot;: {        &quot;maxValue&quot;: 100,        &quot;minValue&quot;: 0,        &quot;show&quot;: true,        &quot;thresholdLabels&quot;: false,        &quot;thresholdMarkers&quot;: true      },      &quot;gridPos&quot;: {        &quot;h&quot;: 5,        &quot;w&quot;: 8,        &quot;x&quot;: 0,        &quot;y&quot;: 1      },      &quot;height&quot;: &quot;180px&quot;,      &quot;id&quot;: 4,      &quot;interval&quot;: null,      &quot;isNew&quot;: true,      &quot;links&quot;: [],      &quot;mappingType&quot;: 1,      &quot;mappingTypes&quot;: [        {          &quot;name&quot;: &quot;value to text&quot;,          &quot;value&quot;: 1        },        {          &quot;name&quot;: &quot;range to text&quot;,          &quot;value&quot;: 2        }      ],      &quot;maxDataPoints&quot;: 100,      &quot;nullPointMode&quot;: &quot;connected&quot;,      &quot;nullText&quot;: null,      &quot;postfix&quot;: &quot;&quot;,      &quot;postfixFontSize&quot;: &quot;50%&quot;,      &quot;prefix&quot;: &quot;&quot;,      &quot;prefixFontSize&quot;: &quot;50%&quot;,      &quot;rangeMaps&quot;: [        {          &quot;from&quot;: &quot;null&quot;,          &quot;text&quot;: &quot;N/A&quot;,          &quot;to&quot;: &quot;null&quot;        }      ],      &quot;sparkline&quot;: {        &quot;fillColor&quot;: &quot;rgba(31, 118, 189, 0.18)&quot;,        &quot;full&quot;: false,        &quot;lineColor&quot;: &quot;rgb(31, 120, 193)&quot;,        &quot;show&quot;: false      },      &quot;tableColumn&quot;: &quot;&quot;,      &quot;targets&quot;: [        {          &quot;expr&quot;: &quot;sum (container_memory_working_set_bytes{id=\&quot;/\&quot;,instance=~\&quot;^$instance$\&quot;}) / sum (machine_memory_bytes{instance=~\&quot;^$instance$\&quot;}) * 100&quot;,          &quot;interval&quot;: &quot;&quot;,          &quot;intervalFactor&quot;: 2,          &quot;legendFormat&quot;: &quot;&quot;,          &quot;refId&quot;: &quot;A&quot;,          &quot;step&quot;: 2        }      ],      &quot;thresholds&quot;: &quot;65, 90&quot;,      &quot;timeFrom&quot;: &quot;1m&quot;,      &quot;timeShift&quot;: null,      &quot;title&quot;: &quot;Memory Working Set&quot;,      &quot;transparent&quot;: false,      &quot;type&quot;: &quot;singlestat&quot;,      &quot;valueFontSize&quot;: &quot;80%&quot;,      &quot;valueMaps&quot;: [        {          &quot;op&quot;: &quot;=&quot;,          &quot;text&quot;: &quot;N/A&quot;,          &quot;value&quot;: &quot;null&quot;        }      ],      &quot;valueName&quot;: &quot;current&quot;    },    {      &quot;cacheTimeout&quot;: null,      &quot;colorBackground&quot;: false,      &quot;colorValue&quot;: true,      &quot;colors&quot;: [        &quot;rgba(50, 172, 45, 0.97)&quot;,        &quot;rgba(237, 129, 40, 0.89)&quot;,        &quot;rgba(245, 54, 54, 0.9)&quot;      ],      &quot;datasource&quot;: &quot;${DS_PROMETHEUS}&quot;,      &quot;decimals&quot;: 2,      &quot;editable&quot;: true,      &quot;error&quot;: false,      &quot;format&quot;: &quot;percent&quot;,      &quot;gauge&quot;: {        &quot;maxValue&quot;: 100,        &quot;minValue&quot;: 0,        &quot;show&quot;: true,        &quot;thresholdLabels&quot;: false,        &quot;thresholdMarkers&quot;: true      },      &quot;gridPos&quot;: {        &quot;h&quot;: 5,        &quot;w&quot;: 8,        &quot;x&quot;: 8,        &quot;y&quot;: 1      },      &quot;height&quot;: &quot;180px&quot;,      &quot;id&quot;: 6,      &quot;interval&quot;: null,      &quot;isNew&quot;: true,      &quot;links&quot;: [],      &quot;mappingType&quot;: 1,      &quot;mappingTypes&quot;: [        {          &quot;name&quot;: &quot;value to text&quot;,          &quot;value&quot;: 1        },        {          &quot;name&quot;: &quot;range to text&quot;,          &quot;value&quot;: 2        }      ],      &quot;maxDataPoints&quot;: 100,      &quot;nullPointMode&quot;: &quot;connected&quot;,      &quot;nullText&quot;: null,      &quot;postfix&quot;: &quot;&quot;,      &quot;postfixFontSize&quot;: &quot;50%&quot;,      &quot;prefix&quot;: &quot;&quot;,      &quot;prefixFontSize&quot;: &quot;50%&quot;,      &quot;rangeMaps&quot;: [        {          &quot;from&quot;: &quot;null&quot;,          &quot;text&quot;: &quot;N/A&quot;,          &quot;to&quot;: &quot;null&quot;        }      ],      &quot;sparkline&quot;: {        &quot;fillColor&quot;: &quot;rgba(31, 118, 189, 0.18)&quot;,        &quot;full&quot;: false,        &quot;lineColor&quot;: &quot;rgb(31, 120, 193)&quot;,        &quot;show&quot;: false      },      &quot;tableColumn&quot;: &quot;&quot;,      &quot;targets&quot;: [        {          &quot;expr&quot;: &quot;sum(rate(container_cpu_usage_seconds_total{id=\&quot;/\&quot;,instance=~\&quot;^$instance$\&quot;}[1m])) / sum (machine_cpu_cores{instance=~\&quot;^$instance$\&quot;}) * 100&quot;,          &quot;interval&quot;: &quot;10s&quot;,          &quot;intervalFactor&quot;: 1,          &quot;refId&quot;: &quot;A&quot;,          &quot;step&quot;: 10        }      ],      &quot;thresholds&quot;: &quot;65, 90&quot;,      &quot;timeFrom&quot;: &quot;1m&quot;,      &quot;timeShift&quot;: null,      &quot;title&quot;: &quot;Cpu Usage&quot;,      &quot;type&quot;: &quot;singlestat&quot;,      &quot;valueFontSize&quot;: &quot;80%&quot;,      &quot;valueMaps&quot;: [        {          &quot;op&quot;: &quot;=&quot;,          &quot;text&quot;: &quot;N/A&quot;,          &quot;value&quot;: &quot;null&quot;        }      ],      &quot;valueName&quot;: &quot;current&quot;    },    {      &quot;cacheTimeout&quot;: null,      &quot;colorBackground&quot;: false,      &quot;colorValue&quot;: true,      &quot;colors&quot;: [        &quot;rgba(50, 172, 45, 0.97)&quot;,        &quot;rgba(237, 129, 40, 0.89)&quot;,        &quot;rgba(245, 54, 54, 0.9)&quot;      ],      &quot;datasource&quot;: &quot;${DS_PROMETHEUS}&quot;,      &quot;decimals&quot;: 2,      &quot;editable&quot;: true,      &quot;error&quot;: false,      &quot;format&quot;: &quot;percent&quot;,      &quot;gauge&quot;: {        &quot;maxValue&quot;: 100,        &quot;minValue&quot;: 0,        &quot;show&quot;: true,        &quot;thresholdLabels&quot;: false,        &quot;thresholdMarkers&quot;: true      },      &quot;gridPos&quot;: {        &quot;h&quot;: 5,        &quot;w&quot;: 8,        &quot;x&quot;: 16,        &quot;y&quot;: 1      },      &quot;height&quot;: &quot;180px&quot;,      &quot;id&quot;: 7,      &quot;interval&quot;: null,      &quot;isNew&quot;: true,      &quot;links&quot;: [],      &quot;mappingType&quot;: 1,      &quot;mappingTypes&quot;: [        {          &quot;name&quot;: &quot;value to text&quot;,          &quot;value&quot;: 1        },        {          &quot;name&quot;: &quot;range to text&quot;,          &quot;value&quot;: 2        }      ],      &quot;maxDataPoints&quot;: 100,      &quot;nullPointMode&quot;: &quot;connected&quot;,      &quot;nullText&quot;: null,      &quot;postfix&quot;: &quot;&quot;,      &quot;postfixFontSize&quot;: &quot;50%&quot;,      &quot;prefix&quot;: &quot;&quot;,      &quot;prefixFontSize&quot;: &quot;50%&quot;,      &quot;rangeMaps&quot;: [        {          &quot;from&quot;: &quot;null&quot;,          &quot;text&quot;: &quot;N/A&quot;,          &quot;to&quot;: &quot;null&quot;        }      ],      &quot;sparkline&quot;: {        &quot;fillColor&quot;: &quot;rgba(31, 118, 189, 0.18)&quot;,        &quot;full&quot;: false,        &quot;lineColor&quot;: &quot;rgb(31, 120, 193)&quot;,        &quot;show&quot;: false      },      &quot;tableColumn&quot;: &quot;&quot;,      &quot;targets&quot;: [        {          &quot;expr&quot;: &quot;sum(container_fs_usage_bytes{id=\&quot;/\&quot;,instance=~\&quot;^$instance$\&quot;}) / sum(container_fs_limit_bytes{id=\&quot;/\&quot;,instance=~\&quot;^$instance$\&quot;}) * 100&quot;,          &quot;interval&quot;: &quot;10s&quot;,          &quot;intervalFactor&quot;: 1,          &quot;legendFormat&quot;: &quot;&quot;,          &quot;metric&quot;: &quot;&quot;,          &quot;refId&quot;: &quot;A&quot;,          &quot;step&quot;: 10        }      ],      &quot;thresholds&quot;: &quot;65, 90&quot;,      &quot;timeFrom&quot;: &quot;1m&quot;,      &quot;timeShift&quot;: null,      &quot;title&quot;: &quot;Filesystem Usage&quot;,      &quot;type&quot;: &quot;singlestat&quot;,      &quot;valueFontSize&quot;: &quot;80%&quot;,      &quot;valueMaps&quot;: [        {          &quot;op&quot;: &quot;=&quot;,          &quot;text&quot;: &quot;N/A&quot;,          &quot;value&quot;: &quot;null&quot;        }      ],      &quot;valueName&quot;: &quot;current&quot;    },    {      &quot;cacheTimeout&quot;: null,      &quot;colorBackground&quot;: false,      &quot;colorValue&quot;: false,      &quot;colors&quot;: [        &quot;rgba(50, 172, 45, 0.97)&quot;,        &quot;rgba(237, 129, 40, 0.89)&quot;,        &quot;rgba(245, 54, 54, 0.9)&quot;      ],      &quot;datasource&quot;: &quot;${DS_PROMETHEUS}&quot;,      &quot;decimals&quot;: 2,      &quot;editable&quot;: true,      &quot;error&quot;: false,      &quot;format&quot;: &quot;bytes&quot;,      &quot;gauge&quot;: {        &quot;maxValue&quot;: 100,        &quot;minValue&quot;: 0,        &quot;show&quot;: false,        &quot;thresholdLabels&quot;: false,        &quot;thresholdMarkers&quot;: true      },      &quot;gridPos&quot;: {        &quot;h&quot;: 3,        &quot;w&quot;: 4,        &quot;x&quot;: 0,        &quot;y&quot;: 6      },      &quot;height&quot;: &quot;1px&quot;,      &quot;hideTimeOverride&quot;: true,      &quot;id&quot;: 9,      &quot;interval&quot;: null,      &quot;isNew&quot;: true,      &quot;links&quot;: [],      &quot;mappingType&quot;: 1,      &quot;mappingTypes&quot;: [        {          &quot;name&quot;: &quot;value to text&quot;,          &quot;value&quot;: 1        },        {          &quot;name&quot;: &quot;range to text&quot;,          &quot;value&quot;: 2        }      ],      &quot;maxDataPoints&quot;: 100,      &quot;nullPointMode&quot;: &quot;connected&quot;,      &quot;nullText&quot;: null,      &quot;postfix&quot;: &quot;&quot;,      &quot;postfixFontSize&quot;: &quot;20%&quot;,      &quot;prefix&quot;: &quot;&quot;,      &quot;prefixFontSize&quot;: &quot;20%&quot;,      &quot;rangeMaps&quot;: [        {          &quot;from&quot;: &quot;null&quot;,          &quot;text&quot;: &quot;N/A&quot;,          &quot;to&quot;: &quot;null&quot;        }      ],      &quot;sparkline&quot;: {        &quot;fillColor&quot;: &quot;rgba(31, 118, 189, 0.18)&quot;,        &quot;full&quot;: false,        &quot;lineColor&quot;: &quot;rgb(31, 120, 193)&quot;,        &quot;show&quot;: false      },      &quot;tableColumn&quot;: &quot;&quot;,      &quot;targets&quot;: [        {          &quot;expr&quot;: &quot;sum(container_memory_working_set_bytes{id=\&quot;/\&quot;,instance=~\&quot;^$instance$\&quot;})&quot;,          &quot;interval&quot;: &quot;10s&quot;,          &quot;intervalFactor&quot;: 1,          &quot;refId&quot;: &quot;A&quot;,          &quot;step&quot;: 10        }      ],      &quot;thresholds&quot;: &quot;&quot;,      &quot;timeFrom&quot;: &quot;1m&quot;,      &quot;title&quot;: &quot;Used&quot;,      &quot;type&quot;: &quot;singlestat&quot;,      &quot;valueFontSize&quot;: &quot;50%&quot;,      &quot;valueMaps&quot;: [        {          &quot;op&quot;: &quot;=&quot;,          &quot;text&quot;: &quot;N/A&quot;,          &quot;value&quot;: &quot;null&quot;        }      ],      &quot;valueName&quot;: &quot;current&quot;    },    {      &quot;cacheTimeout&quot;: null,      &quot;colorBackground&quot;: false,      &quot;colorValue&quot;: false,      &quot;colors&quot;: [        &quot;rgba(50, 172, 45, 0.97)&quot;,        &quot;rgba(237, 129, 40, 0.89)&quot;,        &quot;rgba(245, 54, 54, 0.9)&quot;      ],      &quot;datasource&quot;: &quot;${DS_PROMETHEUS}&quot;,      &quot;decimals&quot;: 2,      &quot;editable&quot;: true,      &quot;error&quot;: false,      &quot;format&quot;: &quot;bytes&quot;,      &quot;gauge&quot;: {        &quot;maxValue&quot;: 100,        &quot;minValue&quot;: 0,        &quot;show&quot;: false,        &quot;thresholdLabels&quot;: false,        &quot;thresholdMarkers&quot;: true      },      &quot;gridPos&quot;: {        &quot;h&quot;: 3,        &quot;w&quot;: 4,        &quot;x&quot;: 4,        &quot;y&quot;: 6      },      &quot;height&quot;: &quot;1px&quot;,      &quot;hideTimeOverride&quot;: true,      &quot;id&quot;: 10,      &quot;interval&quot;: null,      &quot;isNew&quot;: true,      &quot;links&quot;: [],      &quot;mappingType&quot;: 1,      &quot;mappingTypes&quot;: [        {          &quot;name&quot;: &quot;value to text&quot;,          &quot;value&quot;: 1        },        {          &quot;name&quot;: &quot;range to text&quot;,          &quot;value&quot;: 2        }      ],      &quot;maxDataPoints&quot;: 100,      &quot;nullPointMode&quot;: &quot;connected&quot;,      &quot;nullText&quot;: null,      &quot;postfix&quot;: &quot;&quot;,      &quot;postfixFontSize&quot;: &quot;50%&quot;,      &quot;prefix&quot;: &quot;&quot;,      &quot;prefixFontSize&quot;: &quot;50%&quot;,      &quot;rangeMaps&quot;: [        {          &quot;from&quot;: &quot;null&quot;,          &quot;text&quot;: &quot;N/A&quot;,          &quot;to&quot;: &quot;null&quot;        }      ],      &quot;sparkline&quot;: {        &quot;fillColor&quot;: &quot;rgba(31, 118, 189, 0.18)&quot;,        &quot;full&quot;: false,        &quot;lineColor&quot;: &quot;rgb(31, 120, 193)&quot;,        &quot;show&quot;: false      },      &quot;tableColumn&quot;: &quot;&quot;,      &quot;targets&quot;: [        {          &quot;expr&quot;: &quot;sum (machine_memory_bytes{instance=~\&quot;^$instance$\&quot;})&quot;,          &quot;interval&quot;: &quot;10s&quot;,          &quot;intervalFactor&quot;: 1,          &quot;refId&quot;: &quot;A&quot;,          &quot;step&quot;: 10        }      ],      &quot;thresholds&quot;: &quot;&quot;,      &quot;timeFrom&quot;: &quot;1m&quot;,      &quot;title&quot;: &quot;Total&quot;,      &quot;type&quot;: &quot;singlestat&quot;,      &quot;valueFontSize&quot;: &quot;50%&quot;,      &quot;valueMaps&quot;: [        {          &quot;op&quot;: &quot;=&quot;,          &quot;text&quot;: &quot;N/A&quot;,          &quot;value&quot;: &quot;null&quot;        }      ],      &quot;valueName&quot;: &quot;current&quot;    },    {      &quot;cacheTimeout&quot;: null,      &quot;colorBackground&quot;: false,      &quot;colorValue&quot;: false,      &quot;colors&quot;: [        &quot;rgba(50, 172, 45, 0.97)&quot;,        &quot;rgba(237, 129, 40, 0.89)&quot;,        &quot;rgba(245, 54, 54, 0.9)&quot;      ],      &quot;datasource&quot;: &quot;${DS_PROMETHEUS}&quot;,      &quot;decimals&quot;: 2,      &quot;editable&quot;: true,      &quot;error&quot;: false,      &quot;format&quot;: &quot;none&quot;,      &quot;gauge&quot;: {        &quot;maxValue&quot;: 100,        &quot;minValue&quot;: 0,        &quot;show&quot;: false,        &quot;thresholdLabels&quot;: false,        &quot;thresholdMarkers&quot;: true      },      &quot;gridPos&quot;: {        &quot;h&quot;: 3,        &quot;w&quot;: 4,        &quot;x&quot;: 8,        &quot;y&quot;: 6      },      &quot;height&quot;: &quot;1px&quot;,      &quot;hideTimeOverride&quot;: true,      &quot;id&quot;: 11,      &quot;interval&quot;: null,      &quot;isNew&quot;: true,      &quot;links&quot;: [],      &quot;mappingType&quot;: 1,      &quot;mappingTypes&quot;: [        {          &quot;name&quot;: &quot;value to text&quot;,          &quot;value&quot;: 1        },        {          &quot;name&quot;: &quot;range to text&quot;,          &quot;value&quot;: 2        }      ],      &quot;maxDataPoints&quot;: 100,      &quot;nullPointMode&quot;: &quot;connected&quot;,      &quot;nullText&quot;: null,      &quot;postfix&quot;: &quot; cores&quot;,      &quot;postfixFontSize&quot;: &quot;30%&quot;,      &quot;prefix&quot;: &quot;&quot;,      &quot;prefixFontSize&quot;: &quot;50%&quot;,      &quot;rangeMaps&quot;: [        {          &quot;from&quot;: &quot;null&quot;,          &quot;text&quot;: &quot;N/A&quot;,          &quot;to&quot;: &quot;null&quot;        }      ],      &quot;sparkline&quot;: {        &quot;fillColor&quot;: &quot;rgba(31, 118, 189, 0.18)&quot;,        &quot;full&quot;: false,        &quot;lineColor&quot;: &quot;rgb(31, 120, 193)&quot;,        &quot;show&quot;: false      },      &quot;tableColumn&quot;: &quot;&quot;,      &quot;targets&quot;: [        {          &quot;expr&quot;: &quot;sum (rate (container_cpu_usage_seconds_total{id=\&quot;/\&quot;,instance=~\&quot;^$instance$\&quot;}[1m]))&quot;,          &quot;interval&quot;: &quot;10s&quot;,          &quot;intervalFactor&quot;: 1,          &quot;refId&quot;: &quot;A&quot;,          &quot;step&quot;: 10        }      ],      &quot;thresholds&quot;: &quot;&quot;,      &quot;timeFrom&quot;: &quot;1m&quot;,      &quot;timeShift&quot;: null,      &quot;title&quot;: &quot;Used&quot;,      &quot;type&quot;: &quot;singlestat&quot;,      &quot;valueFontSize&quot;: &quot;50%&quot;,      &quot;valueMaps&quot;: [        {          &quot;op&quot;: &quot;=&quot;,          &quot;text&quot;: &quot;N/A&quot;,          &quot;value&quot;: &quot;null&quot;        }      ],      &quot;valueName&quot;: &quot;current&quot;    },    {      &quot;cacheTimeout&quot;: null,      &quot;colorBackground&quot;: false,      &quot;colorValue&quot;: false,      &quot;colors&quot;: [        &quot;rgba(50, 172, 45, 0.97)&quot;,        &quot;rgba(237, 129, 40, 0.89)&quot;,        &quot;rgba(245, 54, 54, 0.9)&quot;      ],      &quot;datasource&quot;: &quot;${DS_PROMETHEUS}&quot;,      &quot;decimals&quot;: 2,      &quot;editable&quot;: true,      &quot;error&quot;: false,      &quot;format&quot;: &quot;none&quot;,      &quot;gauge&quot;: {        &quot;maxValue&quot;: 100,        &quot;minValue&quot;: 0,        &quot;show&quot;: false,        &quot;thresholdLabels&quot;: false,        &quot;thresholdMarkers&quot;: true      },      &quot;gridPos&quot;: {        &quot;h&quot;: 3,        &quot;w&quot;: 4,        &quot;x&quot;: 12,        &quot;y&quot;: 6      },      &quot;height&quot;: &quot;1px&quot;,      &quot;hideTimeOverride&quot;: true,      &quot;id&quot;: 12,      &quot;interval&quot;: null,      &quot;isNew&quot;: true,      &quot;links&quot;: [],      &quot;mappingType&quot;: 1,      &quot;mappingTypes&quot;: [        {          &quot;name&quot;: &quot;value to text&quot;,          &quot;value&quot;: 1        },        {          &quot;name&quot;: &quot;range to text&quot;,          &quot;value&quot;: 2        }      ],      &quot;maxDataPoints&quot;: 100,      &quot;nullPointMode&quot;: &quot;connected&quot;,      &quot;nullText&quot;: null,      &quot;postfix&quot;: &quot; cores&quot;,      &quot;postfixFontSize&quot;: &quot;30%&quot;,      &quot;prefix&quot;: &quot;&quot;,      &quot;prefixFontSize&quot;: &quot;50%&quot;,      &quot;rangeMaps&quot;: [        {          &quot;from&quot;: &quot;null&quot;,          &quot;text&quot;: &quot;N/A&quot;,          &quot;to&quot;: &quot;null&quot;        }      ],      &quot;sparkline&quot;: {        &quot;fillColor&quot;: &quot;rgba(31, 118, 189, 0.18)&quot;,        &quot;full&quot;: false,        &quot;lineColor&quot;: &quot;rgb(31, 120, 193)&quot;,        &quot;show&quot;: false      },      &quot;tableColumn&quot;: &quot;&quot;,      &quot;targets&quot;: [        {          &quot;expr&quot;: &quot;sum (machine_cpu_cores{instance=~\&quot;^$instance$\&quot;})&quot;,          &quot;interval&quot;: &quot;10s&quot;,          &quot;intervalFactor&quot;: 1,          &quot;refId&quot;: &quot;A&quot;,          &quot;step&quot;: 10        }      ],      &quot;thresholds&quot;: &quot;&quot;,      &quot;timeFrom&quot;: &quot;1m&quot;,      &quot;title&quot;: &quot;Total&quot;,      &quot;type&quot;: &quot;singlestat&quot;,      &quot;valueFontSize&quot;: &quot;50%&quot;,      &quot;valueMaps&quot;: [        {          &quot;op&quot;: &quot;=&quot;,          &quot;text&quot;: &quot;N/A&quot;,          &quot;value&quot;: &quot;null&quot;        }      ],      &quot;valueName&quot;: &quot;current&quot;    },    {      &quot;cacheTimeout&quot;: null,      &quot;colorBackground&quot;: false,      &quot;colorValue&quot;: false,      &quot;colors&quot;: [        &quot;rgba(50, 172, 45, 0.97)&quot;,        &quot;rgba(237, 129, 40, 0.89)&quot;,        &quot;rgba(245, 54, 54, 0.9)&quot;      ],      &quot;datasource&quot;: &quot;${DS_PROMETHEUS}&quot;,      &quot;decimals&quot;: 2,      &quot;editable&quot;: true,      &quot;error&quot;: false,      &quot;format&quot;: &quot;bytes&quot;,      &quot;gauge&quot;: {        &quot;maxValue&quot;: 100,        &quot;minValue&quot;: 0,        &quot;show&quot;: false,        &quot;thresholdLabels&quot;: false,        &quot;thresholdMarkers&quot;: true      },      &quot;gridPos&quot;: {        &quot;h&quot;: 3,        &quot;w&quot;: 4,        &quot;x&quot;: 16,        &quot;y&quot;: 6      },      &quot;height&quot;: &quot;1px&quot;,      &quot;hideTimeOverride&quot;: true,      &quot;id&quot;: 13,      &quot;interval&quot;: null,      &quot;isNew&quot;: true,      &quot;links&quot;: [],      &quot;mappingType&quot;: 1,      &quot;mappingTypes&quot;: [        {          &quot;name&quot;: &quot;value to text&quot;,          &quot;value&quot;: 1        },        {          &quot;name&quot;: &quot;range to text&quot;,          &quot;value&quot;: 2        }      ],      &quot;maxDataPoints&quot;: 100,      &quot;nullPointMode&quot;: &quot;connected&quot;,      &quot;nullText&quot;: null,      &quot;postfix&quot;: &quot;&quot;,      &quot;postfixFontSize&quot;: &quot;50%&quot;,      &quot;prefix&quot;: &quot;&quot;,      &quot;prefixFontSize&quot;: &quot;50%&quot;,      &quot;rangeMaps&quot;: [        {          &quot;from&quot;: &quot;null&quot;,          &quot;text&quot;: &quot;N/A&quot;,          &quot;to&quot;: &quot;null&quot;        }      ],      &quot;sparkline&quot;: {        &quot;fillColor&quot;: &quot;rgba(31, 118, 189, 0.18)&quot;,        &quot;full&quot;: false,        &quot;lineColor&quot;: &quot;rgb(31, 120, 193)&quot;,        &quot;show&quot;: false      },      &quot;tableColumn&quot;: &quot;&quot;,      &quot;targets&quot;: [        {          &quot;expr&quot;: &quot;sum(container_fs_usage_bytes{id=\&quot;/\&quot;,instance=~\&quot;^$instance$\&quot;})&quot;,          &quot;interval&quot;: &quot;10s&quot;,          &quot;intervalFactor&quot;: 1,          &quot;refId&quot;: &quot;A&quot;,          &quot;step&quot;: 10        }      ],      &quot;thresholds&quot;: &quot;&quot;,      &quot;timeFrom&quot;: &quot;1m&quot;,      &quot;title&quot;: &quot;Used&quot;,      &quot;type&quot;: &quot;singlestat&quot;,      &quot;valueFontSize&quot;: &quot;50%&quot;,      &quot;valueMaps&quot;: [        {          &quot;op&quot;: &quot;=&quot;,          &quot;text&quot;: &quot;N/A&quot;,          &quot;value&quot;: &quot;null&quot;        }      ],      &quot;valueName&quot;: &quot;current&quot;    },    {      &quot;cacheTimeout&quot;: null,      &quot;colorBackground&quot;: false,      &quot;colorValue&quot;: false,      &quot;colors&quot;: [        &quot;rgba(50, 172, 45, 0.97)&quot;,        &quot;rgba(237, 129, 40, 0.89)&quot;,        &quot;rgba(245, 54, 54, 0.9)&quot;      ],      &quot;datasource&quot;: &quot;${DS_PROMETHEUS}&quot;,      &quot;decimals&quot;: 2,      &quot;editable&quot;: true,      &quot;error&quot;: false,      &quot;format&quot;: &quot;bytes&quot;,      &quot;gauge&quot;: {        &quot;maxValue&quot;: 100,        &quot;minValue&quot;: 0,        &quot;show&quot;: false,        &quot;thresholdLabels&quot;: false,        &quot;thresholdMarkers&quot;: true      },      &quot;gridPos&quot;: {        &quot;h&quot;: 3,        &quot;w&quot;: 4,        &quot;x&quot;: 20,        &quot;y&quot;: 6      },      &quot;height&quot;: &quot;1px&quot;,      &quot;hideTimeOverride&quot;: true,      &quot;id&quot;: 14,      &quot;interval&quot;: null,      &quot;isNew&quot;: true,      &quot;links&quot;: [],      &quot;mappingType&quot;: 1,      &quot;mappingTypes&quot;: [        {          &quot;name&quot;: &quot;value to text&quot;,          &quot;value&quot;: 1        },        {          &quot;name&quot;: &quot;range to text&quot;,          &quot;value&quot;: 2        }      ],      &quot;maxDataPoints&quot;: 100,      &quot;nullPointMode&quot;: &quot;connected&quot;,      &quot;nullText&quot;: null,      &quot;postfix&quot;: &quot;&quot;,      &quot;postfixFontSize&quot;: &quot;50%&quot;,      &quot;prefix&quot;: &quot;&quot;,      &quot;prefixFontSize&quot;: &quot;50%&quot;,      &quot;rangeMaps&quot;: [        {          &quot;from&quot;: &quot;null&quot;,          &quot;text&quot;: &quot;N/A&quot;,          &quot;to&quot;: &quot;null&quot;        }      ],      &quot;sparkline&quot;: {        &quot;fillColor&quot;: &quot;rgba(31, 118, 189, 0.18)&quot;,        &quot;full&quot;: false,        &quot;lineColor&quot;: &quot;rgb(31, 120, 193)&quot;,        &quot;show&quot;: false      },      &quot;tableColumn&quot;: &quot;&quot;,      &quot;targets&quot;: [        {          &quot;expr&quot;: &quot;sum (container_fs_limit_bytes{id=\&quot;/\&quot;,instance=~\&quot;^$instance$\&quot;})&quot;,          &quot;interval&quot;: &quot;10s&quot;,          &quot;intervalFactor&quot;: 1,          &quot;refId&quot;: &quot;A&quot;,          &quot;step&quot;: 10        }      ],      &quot;thresholds&quot;: &quot;&quot;,      &quot;timeFrom&quot;: &quot;1m&quot;,      &quot;title&quot;: &quot;Total&quot;,      &quot;type&quot;: &quot;singlestat&quot;,      &quot;valueFontSize&quot;: &quot;50%&quot;,      &quot;valueMaps&quot;: [        {          &quot;op&quot;: &quot;=&quot;,          &quot;text&quot;: &quot;N/A&quot;,          &quot;value&quot;: &quot;null&quot;        }      ],      &quot;valueName&quot;: &quot;current&quot;    },    {      &quot;aliasColors&quot;: {},      &quot;bars&quot;: false,      &quot;dashLength&quot;: 10,      &quot;dashes&quot;: false,      &quot;datasource&quot;: &quot;${DS_PROMETHEUS}&quot;,      &quot;decimals&quot;: 2,      &quot;editable&quot;: true,      &quot;error&quot;: false,      &quot;fill&quot;: 1,      &quot;grid&quot;: {},      &quot;gridPos&quot;: {        &quot;h&quot;: 5,        &quot;w&quot;: 24,        &quot;x&quot;: 0,        &quot;y&quot;: 9      },      &quot;height&quot;: &quot;200px&quot;,      &quot;id&quot;: 32,      &quot;isNew&quot;: true,      &quot;legend&quot;: {        &quot;alignAsTable&quot;: true,        &quot;avg&quot;: true,        &quot;current&quot;: true,        &quot;max&quot;: false,        &quot;min&quot;: false,        &quot;rightSide&quot;: true,        &quot;show&quot;: true,        &quot;sideWidth&quot;: 200,        &quot;sort&quot;: &quot;current&quot;,        &quot;sortDesc&quot;: true,        &quot;total&quot;: false,        &quot;values&quot;: true      },      &quot;lines&quot;: true,      &quot;linewidth&quot;: 2,      &quot;links&quot;: [],      &quot;nullPointMode&quot;: &quot;connected&quot;,      &quot;percentage&quot;: false,      &quot;pointradius&quot;: 5,      &quot;points&quot;: false,      &quot;renderer&quot;: &quot;flot&quot;,      &quot;seriesOverrides&quot;: [],      &quot;spaceLength&quot;: 10,      &quot;stack&quot;: false,      &quot;steppedLine&quot;: false,      &quot;targets&quot;: [        {          &quot;expr&quot;: &quot;sum(rate(container_network_receive_bytes_total{instance=~\&quot;^$instance$\&quot;,namespace=~\&quot;^$namespace$\&quot;}[1m]))&quot;,          &quot;interval&quot;: &quot;&quot;,          &quot;intervalFactor&quot;: 2,          &quot;legendFormat&quot;: &quot;receive&quot;,          &quot;metric&quot;: &quot;network&quot;,          &quot;refId&quot;: &quot;A&quot;,          &quot;step&quot;: 240        },        {          &quot;expr&quot;: &quot;- sum(rate(container_network_transmit_bytes_total{instance=~\&quot;^$instance$\&quot;,namespace=~\&quot;^$namespace$\&quot;}[1m]))&quot;,          &quot;interval&quot;: &quot;&quot;,          &quot;intervalFactor&quot;: 2,          &quot;legendFormat&quot;: &quot;transmit&quot;,          &quot;metric&quot;: &quot;network&quot;,          &quot;refId&quot;: &quot;B&quot;,          &quot;step&quot;: 240        }      ],      &quot;thresholds&quot;: [],      &quot;timeFrom&quot;: null,      &quot;timeShift&quot;: null,      &quot;title&quot;: &quot;Network&quot;,      &quot;tooltip&quot;: {        &quot;msResolution&quot;: false,        &quot;shared&quot;: true,        &quot;sort&quot;: 0,        &quot;value_type&quot;: &quot;cumulative&quot;      },      &quot;transparent&quot;: false,      &quot;type&quot;: &quot;graph&quot;,      &quot;xaxis&quot;: {        &quot;buckets&quot;: null,        &quot;mode&quot;: &quot;time&quot;,        &quot;name&quot;: null,        &quot;show&quot;: true,        &quot;values&quot;: []      },      &quot;yaxes&quot;: [        {          &quot;format&quot;: &quot;Bps&quot;,          &quot;label&quot;: &quot;transmit / receive&quot;,          &quot;logBase&quot;: 1,          &quot;max&quot;: null,          &quot;min&quot;: null,          &quot;show&quot;: true        },        {          &quot;format&quot;: &quot;Bps&quot;,          &quot;label&quot;: null,          &quot;logBase&quot;: 1,          &quot;max&quot;: null,          &quot;min&quot;: null,          &quot;show&quot;: false        }      ]    },    {      &quot;collapsed&quot;: false,      &quot;gridPos&quot;: {        &quot;h&quot;: 1,        &quot;w&quot;: 24,        &quot;x&quot;: 0,        &quot;y&quot;: 14      },      &quot;id&quot;: 36,      &quot;panels&quot;: [],      &quot;title&quot;: &quot;each pod&quot;,      &quot;type&quot;: &quot;row&quot;    },    {      &quot;aliasColors&quot;: {},      &quot;bars&quot;: false,      &quot;dashLength&quot;: 10,      &quot;dashes&quot;: false,      &quot;datasource&quot;: &quot;${DS_PROMETHEUS}&quot;,      &quot;decimals&quot;: 3,      &quot;editable&quot;: true,      &quot;error&quot;: false,      &quot;fill&quot;: 0,      &quot;grid&quot;: {},      &quot;gridPos&quot;: {        &quot;h&quot;: 7,        &quot;w&quot;: 24,        &quot;x&quot;: 0,        &quot;y&quot;: 15      },      &quot;height&quot;: &quot;&quot;,      &quot;id&quot;: 17,      &quot;isNew&quot;: true,      &quot;legend&quot;: {        &quot;alignAsTable&quot;: true,        &quot;avg&quot;: true,        &quot;current&quot;: true,        &quot;hideEmpty&quot;: true,        &quot;hideZero&quot;: true,        &quot;max&quot;: false,        &quot;min&quot;: false,        &quot;rightSide&quot;: true,        &quot;show&quot;: true,        &quot;sideWidth&quot;: null,        &quot;sort&quot;: &quot;current&quot;,        &quot;sortDesc&quot;: true,        &quot;total&quot;: false,        &quot;values&quot;: true      },      &quot;lines&quot;: true,      &quot;linewidth&quot;: 2,      &quot;links&quot;: [],      &quot;nullPointMode&quot;: &quot;connected&quot;,      &quot;percentage&quot;: false,      &quot;pointradius&quot;: 5,      &quot;points&quot;: false,      &quot;renderer&quot;: &quot;flot&quot;,      &quot;seriesOverrides&quot;: [],      &quot;spaceLength&quot;: 10,      &quot;stack&quot;: false,      &quot;steppedLine&quot;: false,      &quot;targets&quot;: [        {          &quot;expr&quot;: &quot;sum(rate(container_cpu_usage_seconds_total{image!=\&quot;\&quot;,name=~\&quot;^k8s_.*\&quot;,instance=~\&quot;^$instance$\&quot;,namespace=~\&quot;^$namespace$\&quot;}[1m])) by (pod_name)&quot;,          &quot;interval&quot;: &quot;&quot;,          &quot;intervalFactor&quot;: 2,          &quot;legendFormat&quot;: &quot;{{ pod_name }}&quot;,          &quot;metric&quot;: &quot;container_cpu&quot;,          &quot;refId&quot;: &quot;A&quot;,          &quot;step&quot;: 240        }      ],      &quot;thresholds&quot;: [],      &quot;timeFrom&quot;: null,      &quot;timeShift&quot;: null,      &quot;title&quot;: &quot;Cpu Usage&quot;,      &quot;tooltip&quot;: {        &quot;msResolution&quot;: true,        &quot;shared&quot;: false,        &quot;sort&quot;: 2,        &quot;value_type&quot;: &quot;cumulative&quot;      },      &quot;transparent&quot;: false,      &quot;type&quot;: &quot;graph&quot;,      &quot;xaxis&quot;: {        &quot;buckets&quot;: null,        &quot;mode&quot;: &quot;time&quot;,        &quot;name&quot;: null,        &quot;show&quot;: true,        &quot;values&quot;: []      },      &quot;yaxes&quot;: [        {          &quot;format&quot;: &quot;none&quot;,          &quot;label&quot;: &quot;cores&quot;,          &quot;logBase&quot;: 1,          &quot;max&quot;: null,          &quot;min&quot;: null,          &quot;show&quot;: true        },        {          &quot;format&quot;: &quot;short&quot;,          &quot;label&quot;: null,          &quot;logBase&quot;: 1,          &quot;max&quot;: null,          &quot;min&quot;: null,          &quot;show&quot;: false        }      ]    },    {      &quot;aliasColors&quot;: {},      &quot;bars&quot;: false,      &quot;dashLength&quot;: 10,      &quot;dashes&quot;: false,      &quot;datasource&quot;: &quot;${DS_PROMETHEUS}&quot;,      &quot;decimals&quot;: 2,      &quot;editable&quot;: true,      &quot;error&quot;: false,      &quot;fill&quot;: 0,      &quot;grid&quot;: {},      &quot;gridPos&quot;: {        &quot;h&quot;: 7,        &quot;w&quot;: 24,        &quot;x&quot;: 0,        &quot;y&quot;: 22      },      &quot;id&quot;: 33,      &quot;isNew&quot;: true,      &quot;legend&quot;: {        &quot;alignAsTable&quot;: true,        &quot;avg&quot;: true,        &quot;current&quot;: true,        &quot;hideEmpty&quot;: true,        &quot;hideZero&quot;: true,        &quot;max&quot;: false,        &quot;min&quot;: false,        &quot;rightSide&quot;: true,        &quot;show&quot;: true,        &quot;sideWidth&quot;: null,        &quot;sort&quot;: &quot;current&quot;,        &quot;sortDesc&quot;: true,        &quot;total&quot;: false,        &quot;values&quot;: true      },      &quot;lines&quot;: true,      &quot;linewidth&quot;: 2,      &quot;links&quot;: [],      &quot;nullPointMode&quot;: &quot;null&quot;,      &quot;percentage&quot;: false,      &quot;pointradius&quot;: 5,      &quot;points&quot;: false,      &quot;renderer&quot;: &quot;flot&quot;,      &quot;seriesOverrides&quot;: [],      &quot;spaceLength&quot;: 10,      &quot;stack&quot;: false,      &quot;steppedLine&quot;: false,      &quot;targets&quot;: [        {          &quot;expr&quot;: &quot;sum (container_memory_working_set_bytes{image!=\&quot;\&quot;,name=~\&quot;^k8s_.*\&quot;,instance=~\&quot;^$instance$\&quot;,namespace=~\&quot;^$namespace$\&quot;}) by (pod_name)&quot;,          &quot;interval&quot;: &quot;&quot;,          &quot;intervalFactor&quot;: 2,          &quot;legendFormat&quot;: &quot;{{ pod_name }}&quot;,          &quot;metric&quot;: &quot;&quot;,          &quot;refId&quot;: &quot;A&quot;,          &quot;step&quot;: 240        }      ],      &quot;thresholds&quot;: [],      &quot;timeFrom&quot;: null,      &quot;timeShift&quot;: null,      &quot;title&quot;: &quot;Memory Working Set&quot;,      &quot;tooltip&quot;: {        &quot;msResolution&quot;: false,        &quot;shared&quot;: false,        &quot;sort&quot;: 2,        &quot;value_type&quot;: &quot;cumulative&quot;      },      &quot;type&quot;: &quot;graph&quot;,      &quot;xaxis&quot;: {        &quot;buckets&quot;: null,        &quot;mode&quot;: &quot;time&quot;,        &quot;name&quot;: null,        &quot;show&quot;: true,        &quot;values&quot;: []      },      &quot;yaxes&quot;: [        {          &quot;format&quot;: &quot;bytes&quot;,          &quot;label&quot;: &quot;used&quot;,          &quot;logBase&quot;: 1,          &quot;max&quot;: null,          &quot;min&quot;: null,          &quot;show&quot;: true        },        {          &quot;format&quot;: &quot;short&quot;,          &quot;label&quot;: null,          &quot;logBase&quot;: 1,          &quot;max&quot;: null,          &quot;min&quot;: null,          &quot;show&quot;: false        }      ]    },    {      &quot;aliasColors&quot;: {},      &quot;bars&quot;: false,      &quot;dashLength&quot;: 10,      &quot;dashes&quot;: false,      &quot;datasource&quot;: &quot;${DS_PROMETHEUS}&quot;,      &quot;decimals&quot;: 2,      &quot;editable&quot;: true,      &quot;error&quot;: false,      &quot;fill&quot;: 1,      &quot;grid&quot;: {},      &quot;gridPos&quot;: {        &quot;h&quot;: 7,        &quot;w&quot;: 24,        &quot;x&quot;: 0,        &quot;y&quot;: 29      },      &quot;id&quot;: 16,      &quot;isNew&quot;: true,      &quot;legend&quot;: {        &quot;alignAsTable&quot;: true,        &quot;avg&quot;: true,        &quot;current&quot;: true,        &quot;hideEmpty&quot;: true,        &quot;hideZero&quot;: true,        &quot;max&quot;: false,        &quot;min&quot;: false,        &quot;rightSide&quot;: true,        &quot;show&quot;: true,        &quot;sideWidth&quot;: 200,        &quot;sort&quot;: &quot;avg&quot;,        &quot;sortDesc&quot;: true,        &quot;total&quot;: false,        &quot;values&quot;: true      },      &quot;lines&quot;: true,      &quot;linewidth&quot;: 2,      &quot;links&quot;: [],      &quot;nullPointMode&quot;: &quot;null&quot;,      &quot;percentage&quot;: false,      &quot;pointradius&quot;: 5,      &quot;points&quot;: false,      &quot;renderer&quot;: &quot;flot&quot;,      &quot;seriesOverrides&quot;: [],      &quot;spaceLength&quot;: 10,      &quot;stack&quot;: false,      &quot;steppedLine&quot;: false,      &quot;targets&quot;: [        {          &quot;expr&quot;: &quot;sum (rate (container_network_receive_bytes_total{image!=\&quot;\&quot;,name=~\&quot;^k8s_.*\&quot;,instance=~\&quot;^$instance$\&quot;,namespace=~\&quot;^$namespace$\&quot;}[1m])) by (pod_name)&quot;,          &quot;interval&quot;: &quot;&quot;,          &quot;intervalFactor&quot;: 2,          &quot;legendFormat&quot;: &quot;{{ pod_name }} &lt; in&quot;,          &quot;metric&quot;: &quot;network&quot;,          &quot;refId&quot;: &quot;A&quot;,          &quot;step&quot;: 240        },        {          &quot;expr&quot;: &quot;- sum (rate (container_network_transmit_bytes_total{image!=\&quot;\&quot;,name=~\&quot;^k8s_.*\&quot;,instance=~\&quot;^$instance$\&quot;,namespace=~\&quot;^$namespace$\&quot;}[1m])) by (pod_name)&quot;,          &quot;interval&quot;: &quot;&quot;,          &quot;intervalFactor&quot;: 2,          &quot;legendFormat&quot;: &quot;{{ pod_name }} &gt; out&quot;,          &quot;metric&quot;: &quot;network&quot;,          &quot;refId&quot;: &quot;B&quot;,          &quot;step&quot;: 240        }      ],      &quot;thresholds&quot;: [],      &quot;timeFrom&quot;: null,      &quot;timeShift&quot;: null,      &quot;title&quot;: &quot;Network&quot;,      &quot;tooltip&quot;: {        &quot;msResolution&quot;: false,        &quot;shared&quot;: false,        &quot;sort&quot;: 2,        &quot;value_type&quot;: &quot;cumulative&quot;      },      &quot;type&quot;: &quot;graph&quot;,      &quot;xaxis&quot;: {        &quot;buckets&quot;: null,        &quot;mode&quot;: &quot;time&quot;,        &quot;name&quot;: null,        &quot;show&quot;: true,        &quot;values&quot;: []      },      &quot;yaxes&quot;: [        {          &quot;format&quot;: &quot;Bps&quot;,          &quot;label&quot;: &quot;transmit / receive&quot;,          &quot;logBase&quot;: 1,          &quot;max&quot;: null,          &quot;min&quot;: null,          &quot;show&quot;: true        },        {          &quot;format&quot;: &quot;short&quot;,          &quot;label&quot;: null,          &quot;logBase&quot;: 1,          &quot;max&quot;: null,          &quot;min&quot;: null,          &quot;show&quot;: false        }      ]    },    {      &quot;aliasColors&quot;: {},      &quot;bars&quot;: false,      &quot;dashLength&quot;: 10,      &quot;dashes&quot;: false,      &quot;datasource&quot;: &quot;${DS_PROMETHEUS}&quot;,      &quot;decimals&quot;: 2,      &quot;editable&quot;: true,      &quot;error&quot;: false,      &quot;fill&quot;: 1,      &quot;grid&quot;: {},      &quot;gridPos&quot;: {        &quot;h&quot;: 7,        &quot;w&quot;: 24,        &quot;x&quot;: 0,        &quot;y&quot;: 36      },      &quot;id&quot;: 34,      &quot;isNew&quot;: true,      &quot;legend&quot;: {        &quot;alignAsTable&quot;: true,        &quot;avg&quot;: true,        &quot;current&quot;: true,        &quot;hideEmpty&quot;: true,        &quot;hideZero&quot;: true,        &quot;max&quot;: false,        &quot;min&quot;: false,        &quot;rightSide&quot;: true,        &quot;show&quot;: true,        &quot;sideWidth&quot;: 200,        &quot;sort&quot;: &quot;current&quot;,        &quot;sortDesc&quot;: true,        &quot;total&quot;: false,        &quot;values&quot;: true      },      &quot;lines&quot;: true,      &quot;linewidth&quot;: 2,      &quot;links&quot;: [],      &quot;nullPointMode&quot;: &quot;null&quot;,      &quot;percentage&quot;: false,      &quot;pointradius&quot;: 5,      &quot;points&quot;: false,      &quot;renderer&quot;: &quot;flot&quot;,      &quot;seriesOverrides&quot;: [],      &quot;spaceLength&quot;: 10,      &quot;stack&quot;: false,      &quot;steppedLine&quot;: false,      &quot;targets&quot;: [        {          &quot;expr&quot;: &quot;sum(container_fs_usage_bytes{image!=\&quot;\&quot;,name=~\&quot;^k8s_.*\&quot;,instance=~\&quot;^$instance$\&quot;,namespace=~\&quot;^$namespace$\&quot;}) by (pod_name)&quot;,          &quot;interval&quot;: &quot;&quot;,          &quot;intervalFactor&quot;: 2,          &quot;legendFormat&quot;: &quot;{{ pod_name }}&quot;,          &quot;metric&quot;: &quot;network&quot;,          &quot;refId&quot;: &quot;A&quot;,          &quot;step&quot;: 240        }      ],      &quot;thresholds&quot;: [],      &quot;timeFrom&quot;: null,      &quot;timeShift&quot;: null,      &quot;title&quot;: &quot;Filesystem&quot;,      &quot;tooltip&quot;: {        &quot;msResolution&quot;: false,        &quot;shared&quot;: false,        &quot;sort&quot;: 2,        &quot;value_type&quot;: &quot;cumulative&quot;      },      &quot;type&quot;: &quot;graph&quot;,      &quot;xaxis&quot;: {        &quot;buckets&quot;: null,        &quot;mode&quot;: &quot;time&quot;,        &quot;name&quot;: null,        &quot;show&quot;: true,        &quot;values&quot;: []      },      &quot;yaxes&quot;: [        {          &quot;format&quot;: &quot;bytes&quot;,          &quot;label&quot;: &quot;used&quot;,          &quot;logBase&quot;: 1,          &quot;max&quot;: null,          &quot;min&quot;: null,          &quot;show&quot;: true        },        {          &quot;format&quot;: &quot;short&quot;,          &quot;label&quot;: null,          &quot;logBase&quot;: 1,          &quot;max&quot;: null,          &quot;min&quot;: null,          &quot;show&quot;: false        }      ]    }  ],  &quot;refresh&quot;: false,  &quot;schemaVersion&quot;: 16,  &quot;style&quot;: &quot;dark&quot;,  &quot;tags&quot;: [    &quot;kubernetes&quot;  ],  &quot;templating&quot;: {    &quot;list&quot;: [      {        &quot;allValue&quot;: &quot;.*&quot;,        &quot;current&quot;: {},        &quot;datasource&quot;: &quot;${DS_PROMETHEUS}&quot;,        &quot;hide&quot;: 0,        &quot;includeAll&quot;: true,        &quot;label&quot;: &quot;Instance&quot;,        &quot;multi&quot;: false,        &quot;name&quot;: &quot;instance&quot;,        &quot;options&quot;: [],        &quot;query&quot;: &quot;label_values(instance)&quot;,        &quot;refresh&quot;: 1,        &quot;regex&quot;: &quot;master|node.*&quot;,        &quot;sort&quot;: 0,        &quot;tagValuesQuery&quot;: &quot;&quot;,        &quot;tags&quot;: [],        &quot;tagsQuery&quot;: &quot;&quot;,        &quot;type&quot;: &quot;query&quot;,        &quot;useTags&quot;: false      },      {        &quot;allValue&quot;: null,        &quot;current&quot;: {},        &quot;datasource&quot;: &quot;${DS_PROMETHEUS}&quot;,        &quot;hide&quot;: 0,        &quot;includeAll&quot;: true,        &quot;label&quot;: &quot;Namespace&quot;,        &quot;multi&quot;: true,        &quot;name&quot;: &quot;namespace&quot;,        &quot;options&quot;: [],        &quot;query&quot;: &quot;label_values(namespace)&quot;,        &quot;refresh&quot;: 1,        &quot;regex&quot;: &quot;&quot;,        &quot;sort&quot;: 0,        &quot;tagValuesQuery&quot;: &quot;&quot;,        &quot;tags&quot;: [],        &quot;tagsQuery&quot;: &quot;&quot;,        &quot;type&quot;: &quot;query&quot;,        &quot;useTags&quot;: false      }    ]  },  &quot;time&quot;: {    &quot;from&quot;: &quot;now-15m&quot;,    &quot;to&quot;: &quot;now&quot;  },  &quot;timepicker&quot;: {    &quot;refresh_intervals&quot;: [      &quot;5s&quot;,      &quot;10s&quot;,      &quot;30s&quot;,      &quot;1m&quot;,      &quot;5m&quot;,      &quot;15m&quot;,      &quot;30m&quot;,      &quot;1h&quot;,      &quot;2h&quot;,      &quot;1d&quot;    ],    &quot;time_options&quot;: [      &quot;5m&quot;,      &quot;15m&quot;,      &quot;1h&quot;,      &quot;6h&quot;,      &quot;12h&quot;,      &quot;24h&quot;,      &quot;2d&quot;,      &quot;7d&quot;,      &quot;30d&quot;    ]  },  &quot;timezone&quot;: &quot;browser&quot;,  &quot;title&quot;: &quot;Kubernetes Pod Resources&quot;,  &quot;uid&quot;: &quot;Tl8II5Wmz&quot;,  &quot;version&quot;: 19}</code></pre>]]></content>
    
    
    <categories>
      
      <category>kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>部署</tag>
      
      <tag>kubernetes</tag>
      
      <tag>grafana</tag>
      
      <tag>prometheus</tag>
      
      <tag>监控</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Gitlab开启Https</title>
    <link href="/2020/01/07/gitlab%E5%BC%80%E5%90%AFhttps/"/>
    <url>/2020/01/07/gitlab%E5%BC%80%E5%90%AFhttps/</url>
    
    <content type="html"><![CDATA[<h2 id="Gitlab开启Https"><a href="#Gitlab开启Https" class="headerlink" title="Gitlab开启Https"></a>Gitlab开启Https</h2><h3 id="建立认证目录"><a href="#建立认证目录" class="headerlink" title="建立认证目录"></a>建立认证目录</h3><pre><code class="shell">mkdir -p /etc/gitlab/sslchmod 700 /etc/gitlab/ssl</code></pre><h3 id="建立证书"><a href="#建立证书" class="headerlink" title="建立证书"></a>建立证书</h3><pre><code class="shell"># step 1 创建private key （记住输入的密码（Pass phrase））openssl genrsa -des3 -out /etc/gitlab/ssl/server.key 2048# step 2 生成 Certificate Requestopenssl req -new -key /etc/gitlab/ssl/gitlab.domain.com.key -out /etc/gitlab/ssl/server.csr</code></pre><blockquote><p> Enter Country Name CN<br> Enter State or Province Full Name HB<br> Enter City Name WuHan<br> Enter Organization Name<br> Enter Company Name EV-IV<br> Enter Organizational Unit Name<br> Enter server hostname i.e. URL gitlab.domain.com<br> Enter Admin Email Address<br> Skip Challenge Password (Hit Enter)<br> Skip Optional Company Name (Hit Enter)</p></blockquote><pre><code class="shell">#在加载SSL支持的Nginx并使用上述私钥时要除去刚才设置的口令： #step3 备份csr文件及去除命令，直接覆盖了server.key了openssl rsa -inserver.key.org -out server.key#step 4最后标记证书使用上述私钥和CSR：（把csr标记后转换成了crt nginx要用key和crt文件）openssl x509 -req -days 365 -in server.csr -signkey server.key -out server.crt</code></pre><h3 id="修改gitlab配置"><a href="#修改gitlab配置" class="headerlink" title="修改gitlab配置"></a>修改gitlab配置</h3><pre><code class="shell">vim /etc/gitlab/gitlab.rbexternal_url &#39;https://161.189.27.8:8090/&#39;nginx[&#39;redirect_http_to_https&#39;]= truenginx[&#39;ssl_client_certificate&#39;] = &quot;/etc/gitlab/ssl/server.crt&quot;nginx[&#39;ssl_certificate&#39;]= &quot;/etc/gitlab/ssl/server.crt&quot;nginx[&#39;ssl_certificate_key&#39;]= &quot;/etc/gitlab/ssl/server.key&quot;</code></pre><h3 id="重启gitlab"><a href="#重启gitlab" class="headerlink" title="重启gitlab"></a>重启gitlab</h3><pre><code class="shell">gitlab-ctl reconfiguregitlab-ctl restart</code></pre><p>最后通过访问https地址进行访问测试</p><h3 id="相关问题"><a href="#相关问题" class="headerlink" title="相关问题"></a>相关问题</h3><p>Q1 由于ssl证书为自签证书 git clone报错</p><pre><code class="shell">git clone https://161.189.27.8:8090/dqdev/pythogoras.gitCloning into &#39;pythogoras&#39;...fatal: unable to access &#39;https://161.189.27.8:8090/dqdev/pythogoras.git/&#39;: SSL certificate problem: self signed certificate</code></pre><p>关闭git ssl验证</p><pre><code class="bash">git config --global http.sslVerify false 关闭git config --global http.sslVerify true  开启</code></pre>]]></content>
    
    
    <categories>
      
      <category>运维</category>
      
    </categories>
    
    
    <tags>
      
      <tag>证书</tag>
      
      <tag>https</tag>
      
      <tag>gitlab</tag>
      
      <tag>ssl</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
